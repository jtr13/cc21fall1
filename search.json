[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"ideas can add welcome page? Open issue pull request.","code":""},{"path":"community-contribution.html","id":"community-contribution","chapter":"2 Community Contribution","heading":"2 Community Contribution","text":"fairly open-ended assignment provides opportunity receive credit contributing collective learning class, perhaps beyond. reflect minimum 3 hours work. complete assignment must submit short description contribution. appropriate, attach relevant files.many ways can contribute:organize lead workshop particular topic (date may assignment due date need schedule )help students find final project partnersgive well-rehearsed 5 minute lightning talk class datavis topic (theory tool) (email set date – may assignment due date need schedule )create video tutorial (length)create cheatsheet resourcewrite tutorial tool ’s well documentedbuild viz product (ex. htmlwidget RStudio add-) class use[idea](Note: translations allowed)may draw expand existing resources. , critical cite sources.","code":""},{"path":"community-contribution.html","id":"important-logistics","chapter":"2 Community Contribution","heading":"2.1 IMPORTANT LOGISTICS","text":"","code":""},{"path":"community-contribution.html","id":"groups","chapter":"2 Community Contribution","heading":"2.1.1 Groups","text":"may work partner choosing. work alone, need join group 1, simply submit work CourseWorks solo assignment.work partner, add group CC page People tab. Ed Discussion can used find partners similar interests.","code":""},{"path":"community-contribution.html","id":"what-to-submit","chapter":"2 Community Contribution","heading":"2.1.2 What to submit","text":"cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)","code":""},{"path":"community-contribution.html","id":"submitting-your-assignment","chapter":"2 Community Contribution","heading":"2.1.3 Submitting your assignment","text":"must submit assignment twice: CourseWorks (can graded) class, details follow.CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .Class (GitHub) submission: detail provided separate assignment.Class (GitHub) submission: detail provided separate assignment.","code":""},{"path":"community-contribution.html","id":"grading","chapter":"2 Community Contribution","heading":"2.1.4 Grading","text":"graded quality work, originality, effort invested. sources used must cited.","code":""},{"path":"github-submission-instructions.html","id":"github-submission-instructions","chapter":"3 GitHub submission instructions","heading":"3 GitHub submission instructions","text":"chapter gives information need upload community contribution. Please read entire document carefully making submission. particular note fact bookdown requires different .Rmd format ’re used , must make changes beginning file described submitting.","code":""},{"path":"github-submission-instructions.html","id":"background","chapter":"3 GitHub submission instructions","heading":"3.1 Background","text":"web site makes use bookdown package render collection .Rmd files nicely formatted online book chapters subchapters. job submit slightly modified version community contribution .Rmd file GitHub repository source files web site stored. backend, admins divide chapters book sections order .community contribution different format, create short .Rmd file explains , includes links relevant files, slides, etc. can post GitHub repo (another online site.)","code":""},{"path":"github-submission-instructions.html","id":"preparing-your-.rmd-file","chapter":"3 GitHub submission instructions","heading":"3.2 Preparing your .Rmd file","text":"submit ONE Rmd file.completing modifications, .Rmd look like sample .Rmd.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.second line blank, followed name(s):\n# Base R vs. ggplot2\n\nAaron Burr Alexander Hamilton\n\ncontent starts . second line blank, followed name(s):project requires data, please use built-dataset read directly URL, :\ndf <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.project requires data, please use built-dataset read directly URL, :df <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:\n{r, include=FALSE}\ninstead :\n{r setup, include=FALSE}included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:instead :project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:\n\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must installed sourceIf project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:developed .Rmd file moving library() statements rest file content, highly recommended knit review document . may change namespace available section code development, causing function work exhibit unexpected behavior.file contain getwd() / setwd() calls (never use scripts anyway!) write statements.Want get fancy? See optional tweaks section .","code":"# Base R vs. ggplot2\n\nAaron Burr and Alexander Hamilton\n\nYour content starts here. {r, include=FALSE}{r setup, include=FALSE}\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must be installed from source"},{"path":"github-submission-instructions.html","id":"submission-steps","chapter":"3 GitHub submission instructions","heading":"3.3 Submission steps","text":"submit work, following “Workflow #4” – submitting pull request someone else’s repository write access. Instructions available lecture slides topic well tutorial. repeated abbreviated form, specific instructions naming conventions, content information, important details.Fork cc21fall1 repo (repo) GitHub account.Fork cc21fall1 repo (repo) GitHub account.Clone/download forked repo local computer.Clone/download forked repo local computer.Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:\n![Test Photo](resources/sample_project/pumpkins.jpg)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:![Test Photo](resources/sample_project/pumpkins.jpg)ready submit project, push branch remote repo. Follow tutorial create pull request.ready submit project, push branch remote repo. Follow tutorial create pull request.point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.","code":""},{"path":"github-submission-instructions.html","id":"optional-tweaks","chapter":"3 GitHub submission instructions","heading":"3.4 Optional tweaks","text":"prefer links chapter open new tabs, add {target=\"_blank\"} link, :\n[edav.info](edav.info){target=\"_blank\"}prefer links chapter open new tabs, add {target=\"_blank\"} link, :[edav.info](edav.info){target=\"_blank\"}Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.","code":""},{"path":"github-submission-instructions.html","id":"faq","chapter":"3 GitHub submission instructions","heading":"3.5 FAQ","text":"","code":""},{"path":"github-submission-instructions.html","id":"what-should-i-expect-after-creating-a-pull-request","chapter":"3 GitHub submission instructions","heading":"3.5.1 What should I expect after creating a pull request?","text":"Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.take time can process pull requests, long see pull request repo, don’t worry.take time can process pull requests, long see pull request repo, don’t worry.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-before-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.2 What if I catch mistakes before my pull request is merged?","text":"Just make changes branch, commit push GitHub. automatically added pull request.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-after-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.3 What if I catch mistakes after my pull request is merged?","text":"may submit additional pull requests fix material site. edits small, fixing typos, easiest make edits directly GitHub, following instructions. merge first pull requests edits, please patient.","code":""},{"path":"github-submission-instructions.html","id":"other-questions","chapter":"3 GitHub submission instructions","heading":"3.5.4 Other questions","text":"additional questions, please ask Discussions section respond.Thank contributions!","code":""},{"path":"sample-project.html","id":"sample-project","chapter":"4 Sample project","heading":"4 Sample project","text":"Joe Biden Donald TrumpThis chapter gives sample layout Rmd file.Test Photo","code":""},{"path":"introduction-to-the-lattice-package.html","id":"introduction-to-the-lattice-package","chapter":"5 Introduction to the lattice package","heading":"5 Introduction to the lattice package","text":"Eubin ParkThe lattice package data visualization package created Deepayan Sarkar. add-package improves defaults base R, emphasis displaying multivariate data - supporting creation trellis graphs. strength lattice package mainly ability manage dependent data.general format plotting using lattice functions : graph_type(formula, data). main workhorse function lattice package xyplot().","code":"\nlibrary(lattice)\nlibrary(car)"},{"path":"introduction-to-the-lattice-package.html","id":"producing-a-plot-in-lattice","chapter":"5 Introduction to the lattice package","heading":"5.1 Producing a Plot in Lattice","text":"can begin creating basic plots Lattice - scatterplot. Lattice, done using xyplot. example, use iris dataset.type scatterplot familiar many. seen , can see basic method plotting using xyplot() symbolic formula y ~ x, x independent variable y dependent variable.","code":"\ndata(iris)\nxyplot(Sepal.Length ~ Sepal.Width, data = iris,\n       xlab = \"Sepal Width\",\n       ylab = \"Sepal Length\")"},{"path":"introduction-to-the-lattice-package.html","id":"plotting-by-groups","chapter":"5 Introduction to the lattice package","heading":"5.2 Plotting by Groups","text":"2 main ways going plotting multivariate data lattice.1. Superposition:\ndata plotted region graph, distinct groups able categorized varying plot features color, shapes, etc. use superposition plots, groups argument must specified.2. Juxtaposition:\nData plotted separate regions larger graph. use juxtaposition plots, one must specify conditioning statement, : y ~ x | z, z conditioning variable.difference superposition juxtaposition can shown :Superposition:Juxtaposition:seen , real difference plotting two graphs whether one uses groups argument (Supposition) conditioning statement (Juxtaposition), Lattice able create two different graphs small difference.many problems supposition plot juxtaposition plot overcomes. example, good deal -plotting first plot, result difficult distinguish clear trends within species group. However, problems seen juxtaposition plot.sort advanrage becomes much conspicuous dealing larger multivariate datasets. next example, use quakes dataset.example, created shingles order essentially bin data. shingle contains data subset variable created .example, clear see advantages using juxtaposition method plotting.want re-arrange panels plot, can use layout argument. argument takes vector three values: number rows, number columns, number pages.However, using argument, skewed shapes plots. can fix using aspect argument, controls ratio plots.wanted fit regression lines panel, can use panel function argument xyplot function.","code":"\nxyplot(Sepal.Length ~ Sepal.Width, data=iris, \n       groups=iris$Species, # use groups argument\n       auto.key=list(text=c(\"setosa\", \"versicolor\", \"virginica\")),\n       xlab = \"Sepal Width\",\n       ylab = \"Sepal Length\")\nxyplot(Sepal.Length ~ Sepal.Width | Species, data=iris, # add conditioning statement\n       pch=1, col=\"black\",\n       xlab = \"Sepal Width\",\n       ylab = \"Sepal Length\")\ndata(quakes)\n# Create shingles\nDepth = equal.count(quakes$depth, number = 8, overlap = .1)\n\n# Plot graph using supposition\nxyplot(lat ~ long, data = quakes,\n       groups = Depth,\n       xlab = \"Longitude\",\n       ylab = \"Latitude\")\n# Plot graph using juxtaposition\nxyplot(lat ~ long | Depth, data = quakes,\n       xlab = \"Longtitude\",\n       ylab = \"Latitude\")\n# Use layout argument\nxyplot(lat ~ long | Depth, data = quakes,\n       layout = c(3, 3, 1),\n       xlab = \"Longtitude\",\n       ylab = \"Latitude\")\n# Use aspect argument\nxyplot(lat ~ long | Depth, data = quakes,\n       aspect = 1,\n       layout = c(3, 3, 1),\n       xlab = \"Longtitude\",\n       ylab = \"Latitude\")\n# Use the panel function argument\nxyplot(lat ~ long | Depth, data = quakes,\n       panel = function(x,y,subscripts,...){\n           panel.points(x,y,...)\n           panel.lmline(x,y,...) })"},{"path":"introduction-to-the-lattice-package.html","id":"histograms-and-density-plots","chapter":"5 Introduction to the lattice package","heading":"5.3 Histograms and Density Plots","text":"Lattice offers options , histograms density plots. example, use Duncan dataset car package.make histogram lattice package, use histogram() function.make density plot lattice package, use densityplot() function.can even combine histograms density plots using panel function argument. , can split data separate panels, useful multivariate data.","code":"\ndata(Duncan)\nhistogram(~ prestige, data=Duncan, \n          type=\"count\", # can take 'count', 'percent', or 'density'\n          nint = 10, # number of bins\n          endpoints = c(0, 100)\n          )\ndensityplot(~ prestige, data = Duncan,\n            col = \"black\",\n            plot.points = F # specify whether to have data points\n            )\nb <- with(Duncan, do.breaks(range(income), 3))\n\nxyplot(~income | type, data=Duncan,\n       xlim = range(b), ylim = c(0, 0.04),\n       panel = function(x){\n           panel.histogram(x, \n                           breaks = b, \n                           col=\"gray80\")\n           panel.densityplot(x, \n                             darg =list(n=100),\n                             col=\"red\",\n                             lwd=1.5,\n                             plot.points=F)\n       })"},{"path":"introduction-to-the-lattice-package.html","id":"boxplots-violinplots-and-dotplots","chapter":"5 Introduction to the lattice package","heading":"5.4 Boxplots, Violinplots, and Dotplots","text":"options offered Lattice package include boxplots dotplots. example, use ToothGrowth dataset.make boxplot, use bwplot() function. always lattice package, can use conditioning statement create juxtaposing panels.make violin plot, use bwplot() function specify panel argument.make dotplot, use dotplot() function.","code":"\nbwplot(len ~ supp | dose,  data = ToothGrowth,\n       layout = c(3, 1),\n        xlab = \"Dose\", ylab = \"Length\")\nbwplot(len ~ supp | dose,  data = ToothGrowth,\n       layout = c(3, 1), \n       panel = panel.violin, # specify panel argument to make violin plot\n        xlab = \"Dose\", ylab = \"Length\")\ndotplot(len ~ supp | dose,  data = ToothGrowth,\n       layout = c(3, 1),\n        xlab = \"Dose\", ylab = \"Length\")"},{"path":"introduction-to-the-lattice-package.html","id":"trivariate-plots","chapter":"5 Introduction to the lattice package","heading":"5.5 Trivariate Plots","text":"One option displaying trivariate continuous data utilize 3 axes. can done three-dimensional scatterplot.lattice package, one can create plot using cloud() function. function takes symbolic formula first argument, form: z ~ x * y, x, z, y three continuous variables.example use quakes dataset .view data different perspective, can rotate plot using screen argument. can play feature find best view data.Unfortunately, interactive options available Lattice package.","code":"\ncloud(depth ~ lat * long, data=quakes)\ncloud(depth ~ lat * long, data=quakes,\n      screen = list(z = 105, x = -70))"},{"path":"introduction-to-the-lattice-package.html","id":"pros-and-cons-of-lattice","chapter":"5 Introduction to the lattice package","heading":"5.6 Pros and Cons of Lattice","text":"Now basic overview kinds things lattice package can , let’s discuss advantages disadvantages data visualization package.Pros:good allowing one visualize multivariate data, .e. comparing variable y changes variable x across levels variable zMany settings set automatically entire plot created .Cons:Can difficult flesh entire plot one method callCannot add elements plot created; modified.","code":""},{"path":"graph-animation-with-gganimate-and-after-effects.html","id":"graph-animation-with-gganimate-and-after-effects","chapter":"6 Graph animation with gganimate and After Effects","heading":"6 Graph animation with gganimate and After Effects","text":"Anh-Vu Nguyen\nsite explains graph animation gganimate effects\nlink: https://anhvung.github.io/EDAV-CC/\n","code":""},{"path":"interactive-plots.html","id":"interactive-plots","chapter":"7 Interactive Plots","heading":"7 Interactive Plots","text":"Zhirui Yang","code":"\n# Libraries\n# Make sure these packages are installed before running code.\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(dygraphs)\nlibrary(xts)\nlibrary(dplyr)\nlibrary(igraph)\nlibrary(networkD3)"},{"path":"interactive-plots.html","id":"why-use-interactive-plots","chapter":"7 Interactive Plots","heading":"7.1 Why use interactive plots","text":"general, use ggplot2 plot static graph. way, can know data macroscopic perspective. want know subtle contents, like exact difference two points, still need write code find values two points. However, tedious step can avoided interactive graph. Interactive graphs important data analysis. can check value every point graph, also zoom graph check details.tutorial, introduce frequently used plot, provide examples. Moreover, introduce important parameters, can help customize plot.beginning tutorial, packages use . installed , can use install.packages(‘package_name’) install .","code":""},{"path":"interactive-plots.html","id":"interactive-scatter-plot","chapter":"7 Interactive Plots","heading":"7.2 Interactive Scatter plot","text":"Plotly’s R graphing library makes interactive, publication-quality graphs. Examples make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, 3D (WebGL based) charts.give examples show draw Scatterplot Plotly.","code":""},{"path":"interactive-plots.html","id":"compare-plotly-and-ggplot2","chapter":"7 Interactive Plots","heading":"7.2.1 Compare Plotly and ggplot2","text":"use iris dataset, provided natively R. part dataset.draw Scatterplot Sepal.Length vs Sepal.Width.can check value data point basic scatterplot, can interactive scatterplot.","code":"\nhead(iris)##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n# basic scatterplot\nggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + \n  geom_point()\n# interactive scatterplot\nfig <- plot_ly(\n  data = iris, x = ~Sepal.Length, y = ~Petal.Length)\nfig"},{"path":"interactive-plots.html","id":"styled-scatter-plot","chapter":"7 Interactive Plots","heading":"7.2.2 Styled Scatter Plot","text":"can customize Scatter Plot using parameters plot_ly. Plotly three main attributions including plot_ly, add_trace layout. Every aspect Plotly chart (colors, grid-lines, data, ) corresponding key attributions.","code":""},{"path":"interactive-plots.html","id":"plot_ly","chapter":"7 Interactive Plots","heading":"7.2.2.1 plot_ly","text":"plot_ly used 1.1. function basically draw main content plot. basic structure .","code":"\n# plot_ly(\n# data,\n# type = \"scatter\", # all \"scatter\" attributes. We don't need to change it for scatter plot\n# x = ~x, # x of scatter plot\n# y = ~y, # y of scatter plot\n# marker = list(\n#   size = 5,\n#   color=\"#264E86\") # marker is used to change color, size and so on.\n# ) \nfig <- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length,\n               marker = list(size = 10,\n                             color = 'red',\n                             line = list(color = 'blue',\n                                         width = 2)))\nfig"},{"path":"interactive-plots.html","id":"add_trace","chapter":"7 Interactive Plots","heading":"7.2.2.2 add_trace","text":"Plotly’s graph made two parts. One trace, can regard key component graph. example, trace scatter plot points graph, trace line plot line graph. can regard plot plot_ly background. sometimes want add multiple modes plot, need add_trace. basic structure , similar plot_ly.use example understand add_trace works.","code":"\n# add_trace(x = ~x2, # x2 of new trace\n#           y = ~y2, # y2 of new trace\n#           mode = 'lines', # type of mode\n#           line = list(\n#             color = \"#5E88FC\",  \n#             dash = \"dashed\"\n#             )\n# )\nfig <- plot_ly(data = iris, x = ~Sepal.Length)\nfig <- fig %>% add_trace(x = ~Sepal.Length, y = ~Petal.Length, name = 'Petal.Length',mode = 'markers', color = \"red\")\nfig <- fig %>% add_trace(x = ~Sepal.Length, y = ~Sepal.Width, name = 'Sepal.Width',mode = 'markers', color = \"blue\")\nfig <- fig %>% add_trace(x = ~Sepal.Length, y = ~Petal.Width, name = 'Petal.Width',mode = 'markers', color = \"green\")\nfig"},{"path":"interactive-plots.html","id":"layout","chapter":"7 Interactive Plots","heading":"7.2.2.3 layout","text":"layout used rest chart, like title, xaxis, annotations. basic structure .use example understand layout works.","code":"\n# layout(\n#   title = \"Unemployment\", # add title\n#   xaxis = list( # add xaxis\n#     title = \"Time\",\n#     showgrid = F),\n#   yaxis = list( # add xaxis\n#     title = \"uidx\")\n# )\nfig <- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species)\nfig <- fig %>% layout(title = 'Styled Scatter',\n         yaxis = list(title = \"y - Petal.Length\"),\n         xaxis = list(title = \"x - Sepal.Length\")\n         )\nfig"},{"path":"interactive-plots.html","id":"interactive-time-series-chart","chapter":"7 Interactive Plots","heading":"7.3 Interactive time series chart","text":"","code":""},{"path":"interactive-plots.html","id":"ggplot2-and-plotly","chapter":"7 Interactive Plots","heading":"7.3.1 ggplot2 and plotly","text":"two ways create interactive time series chart. first way creat line plot ggplot2, use plotly turn ggplot2 chart object interactive. example .can also just use plotly. make area plot interior filling set fill “tozeroy” call second trace. informations options fill option checkout https://plotly.com/r/reference/#scatter-fill.","code":"\n# Load dataset from github\ndata <- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date <- as.Date(data$date)\n\n# Usual area chart\np <- data %>%\n  ggplot( aes(x=date, y=value)) +\n    geom_area(fill=\"red\", alpha=0.5) +\n    geom_line(color=\"blue\") +\n    ylab(\"bitcoin price ($)\")\np\n# Turn it interactive with ggplotly\np <- ggplotly(p)\np\nplot_ly(data=data, x=~date, y=~value, type=\"scatter\", mode=\"line\", fill='tozeroy')"},{"path":"interactive-plots.html","id":"dygraphs-and-xts","chapter":"7 Interactive Plots","heading":"7.3.2 dygraphs and xts","text":"dygraphs useful R package creating interactive time series plot. dygraphs package R interface dygraphs JavaScript charting library. provides rich facilities charting time-series data R, including: rich interactive features including zoom/pan series/point highlighting, automatically plots xts time series objects (object convertible xts), highly configurable axis series display (including optional second Y-axis) . use plot functions.input data dygraphs, transform data frame xts format (xts=eXtensible Time Series).can select interval data analysis, example.","code":"\nts <- xts(x = data$value, order.by = data$date)\n\n# Make the chart\np <- dygraph(ts)\np"},{"path":"interactive-plots.html","id":"interactive-network-diagram","chapter":"7 Interactive Plots","heading":"7.4 Interactive network diagram","text":"networkD3 useful creating interactive network diagrams. input contains edge, specifies two nodes.","code":""},{"path":"interactive-plots.html","id":"simple-network","chapter":"7 Interactive Plots","heading":"7.4.1 Simple Network","text":"can use simpleNetwork networkD3.","code":"\n# Create fake data\nsrc <- c(\"A\", \"A\", \"A\", \"A\",\n        \"B\", \"B\", \"C\", \"C\", \"D\")\ntarget <- c(\"B\", \"C\", \"D\", \"J\",\n            \"E\", \"F\", \"G\", \"H\", \"I\")\nnetworkData <- data.frame(src, target)\n\n# Plot\nsimpleNetwork(networkData)"},{"path":"interactive-plots.html","id":"customized-network","chapter":"7 Interactive Plots","heading":"7.4.2 Customized Network","text":"Many option available customize interactive diagram. options allow customize node, links label feature, like nodeColour fontSize. example, linkDistance controls numeric distance links pixels. charge controls numeric value indicating either strength node repulsion (negative value) attraction (positive value). fontSize fontFamily can control label. can use help(simpleNetwork) better understanding.can also customized complicated networks using forceNetwork. forceNetwork different input simpleNetwork. two main input Links Nodes. Links data frame object links nodes. include Source Target link. numbered starting 0. Nodes data frame containing node id properties nodes. ID specified nodes must order Source variable column Links data frame. Currently grouping variable allowed. examlpe forceNetwork.","code":"\n# create a dataset:\ndata <- data_frame(\n  from=c(\"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"D\", \"E\", \"K\", \"L\", \"M\", \"M\"),\n  to=c(\"C\", \"F\", \"D\", \"F\", \"G\", \"E\", \"L\", \"F\", \"E\", \"G\", \"Z\", \"J\", \"M\", \"L\", \"Z\")\n)\n\n# Plot\np <- simpleNetwork(data, height=\"100px\", width=\"100px\",        \n        Source = 1,                 # column number of source\n        Target = 2,                 # column number of target\n        linkDistance = 20,          # distance between node. Increase this value to have more space between nodes\n        charge = -900,                # numeric value indicating either the strength of the node repulsion (negative value) or attraction (positive value)\n        fontSize = 14,               # size of the node names\n        fontFamily = \"serif\",       # font og node names\n        linkColour = \"#666\",        # colour of edges, MUST be a common colour for the whole graph\n        nodeColour = \"#69b3a2\",     # colour of nodes, MUST be a common colour for the whole graph\n        opacity = 0.9,              # opacity of nodes. 0=transparent. 1=no transparency\n        zoom = T                    # Can you zoom on the figure?\n        )\np\nhead(MisLinks)##   source target value\n## 1      1      0     1\n## 2      2      0     8\n## 3      3      0    10\n## 4      3      2     6\n## 5      4      0     1\n## 6      5      0     1\nhead(MisNodes)##              name group size\n## 1          Myriel     1   15\n## 2        Napoleon     1   20\n## 3 Mlle.Baptistine     1   23\n## 4    Mme.Magloire     1   30\n## 5    CountessdeLo     1   11\n## 6        Geborand     1    9\nforceNetwork(Links = MisLinks, Nodes = MisNodes,\n            Source = \"source\", Target = \"target\",\n            Value = \"value\", NodeID = \"name\",\n            Group = \"group\", opacity = 0.8)"},{"path":"interactive-plots.html","id":"reference","chapter":"7 Interactive Plots","heading":"7.5 Reference","text":"https://www.r-graph-gallery.com/index.htmlhttps://www.r-bloggers.com/2020/05/7-useful-interactive-charts--r/http://christophergandrud.github.io/networkD3/https://plotly.com/r/","code":""},{"path":"create-regression-assumption-plots-in-word.html","id":"create-regression-assumption-plots-in-word","chapter":"8 Create regression assumption plots in Word","heading":"8 Create regression assumption plots in Word","text":"Nikhil GopalExplanation:.RMD file code run current form. added eval=FALSE flag chunk make code run executed. ggsave() function incompatible GitHub pages, cause errors, work local machine copy code.Example code use function detailed explanation package can found github repository link:https://github.com/ng4567/LM2APA","code":"\nlibrary(ggplot2)\nlibrary(apaTables)\nlibrary(officer)\n\nmake_lm_plots<-function(model, output_path, reg_table, reg_table_name){\n  word_doc <- read_docx()\n  p1<-ggplot(model, aes(.fitted, .resid))+geom_point()\n  p1<-p1+stat_smooth(method=\"loess\")+geom_hline(yintercept=0, col=\"red\", linetype=\"dashed\")\n  p1<-p1+xlab(\"Fitted values\")+ylab(\"Residuals\")\n  p1<-p1+ggtitle(\"Residual vs Fitted Plot\")+theme_bw()\n  \n  ggsave(\"FittedvResid.png\",\n         p1,\n         width = 4,\n         height = 3,\n         units = \"in\")\n  word_doc <- body_add_img(word_doc,\n                           src = \"FittedvResid.png\",\n                           width = 4,\n                           height = 3)\n  \n  p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)\n  p3<-p3+stat_smooth(method=\"loess\", na.rm = TRUE)+xlab(\"Fitted Value\")\n  p3<-p3+ylab(expression(sqrt(\"|Standardized residuals|\")))\n  p3<-p3+ggtitle(\"Scale-Location\")+theme_bw()\n  \n  ggsave(\"Scale-Location.png\",\n         p3,\n         width = 4,\n         height = 3,\n         units = \"in\")\n  word_doc <- body_add_img(word_doc,\n                           src = \"Scale-Location.png\",\n                           width = 4,\n                           height = 3)\n  \n  p4<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat=\"identity\", position=\"identity\")\n  p4<-p4+xlab(\"Obs. Number\")+ylab(\"Cook's distance\")\n  p4<-p4+ggtitle(\"Cook's distance\")+theme_bw()\n  \n  ggsave(\"Cook's distance.png\",\n         p4,\n         width = 4,\n         height = 3,\n         units = \"in\")\n  word_doc <- body_add_img(word_doc,\n                           src = \"Cook's distance.png\",\n                           width = 4,\n                           height = 3)\n  \n  p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)\n  p5<-p5+stat_smooth(method=\"loess\", na.rm=TRUE)\n  p5<-p5+xlab(\"Leverage\")+ylab(\"Standardized Residuals\")\n  p5<-p5+ggtitle(\"Residual vs Leverage Plot\")\n  p5<-p5+scale_size_continuous(\"Cook's Distance\", range=c(1,5))\n  p5<-p5+theme_bw()+theme(legend.position=\"bottom\")\n  \n  ggsave(\"Residual vs Leverage Plot.png\",\n         p5,\n         width = 4,\n         height = 3,\n         units = \"in\")\n  word_doc <- body_add_img(word_doc,\n                           src = \"Residual vs Leverage Plot.png\",\n                           width = 4,\n                           height = 3)\n  \n  p6<-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method=\"loess\", na.rm=TRUE)\n  p6<-p6+xlab(\"Leverage hii\")+ylab(\"Cook's Distance\")\n  p6<-p6+ggtitle(\"Cook's dist vs Leverage hii/(1-hii)\")\n  p6<-p6+geom_abline(slope=seq(0,3,0.5), color=\"gray\", linetype=\"dashed\")\n  p6<-p6+theme_bw()\n  \n  ggsave(\"Cook's dist vs Leverage.png\",\n         p6,\n         width = 4,\n         height = 3,\n         units = \"in\")\n  word_doc <- body_add_img(word_doc,\n                           src = \"Cook's dist vs Leverage.png\",\n                           width = 4,\n                           height = 3)\n  \n  print(word_doc, target = output_path)\n  unlink(\"Cook's dist vs Leverage.png\")\n  unlink(\"Cook's distance.png\")\n  unlink(\"FittedvResid.png\")\n  unlink(\"Scale-Location.png\")\n  unlink(\"Residual vs Leverage Plot.png\")\n  \n  if(reg_table == 1){\n    apa.reg.table(\n      mod,\n      filename = reg_table_name,\n      table.number = NA,\n      prop.var.conf.level = 0.95\n    )\n  }\n}"},{"path":"from-neo4j-to-rstudio.html","id":"from-neo4j-to-rstudio","chapter":"9 From Neo4j to RStudio","heading":"9 From Neo4j to RStudio","text":"Chaoying Zheng","code":""},{"path":"from-neo4j-to-rstudio.html","id":"introduction-of-graph-database","chapter":"9 From Neo4j to RStudio","heading":"9.0.1 Introduction of Graph Database","text":"Graph database designed visual relationship data.Six degree separation popular example graph database helps visualizing relationship. theory also known six handshake rule, states people six fewer social connection away . However, relational database, time consuming find relatinoship generate visualization. Therefore, graph database, one non-relational database, can help us handling type data. side, table non-relational database reading-friendly, converting back relational database can help better understand elements data.Neo4j software widely used visualizing graph database. time, RStudio powerful tool data visualization. , connection two tools can useful analyzing data.two key components grpah database:node edge. node edge label properties. Neo4j uses Cypher query language, structured visually ASCII-art make query-building maintenance easy read adapt. tutorial, Game Thrones (GOT) data used illustrate.GOT databse, node character, label, named charaters, properties, name id. Characters (nodes) connected different edges: “parents”, “siblings”, “killed”, “allies”, etc. Figure 1 shows partial graph GOT databse.Figure 1: graph database visualize Neo4j","code":""},{"path":"from-neo4j-to-rstudio.html","id":"installation","chapter":"9 From Neo4j to RStudio","heading":"9.0.2 Installation","text":"","code":""},{"path":"from-neo4j-to-rstudio.html","id":"neo4j-installation","chapter":"9 From Neo4j to RStudio","heading":"9.0.2.1 Neo4j Installation","text":"First, install Neo4j (https://neo4j.com/docs/operations-manual/current/installation/) run databsae local machine. already install Neo4j, ignore step.","code":""},{"path":"from-neo4j-to-rstudio.html","id":"r-package-installation","chapter":"9 From Neo4j to RStudio","heading":"9.0.2.2 R Package Installation","text":"","code":""},{"path":"from-neo4j-to-rstudio.html","id":"connection","chapter":"9 From Neo4j to RStudio","heading":"9.0.3 Connection","text":"starting graph database Neo4j, open brower go default url (http://localhost:7474). Neo4j may require enter user password authentication, shown Figure 2. information repeat following code connect RStudio Neo4j.Figure 2: Neo4j local sign page","code":"\nlibrary(neo4r)\ncon <- neo4j_api$new(\n  url = \"http://localhost:7474\",\n  user = \"neo4j\", \n  password = \"password\"\n  )"},{"path":"from-neo4j-to-rstudio.html","id":"retrieving-data-from-neo4j","chapter":"9 From Neo4j to RStudio","heading":"9.0.4 Retrieving data from Neo4j","text":"basic idea write cypher query language pass Neo4j connection created function call_neo4j(). parameter type convert graph database table graph object R. query extract characters “marriedEngaged” relationship Sansa Stark.Next, convert nodes relationships relational table. (Reminder: first col id unique id given Neo4j default, last col id1 character id character)","code":"\nlibrary(dplyr)\nlibrary(purrr)\nSansa_Marriage <- 'MATCH a = (sansa:Character {name:\"Sansa Stark\"})-[:killed|marriedEngaged]-(c:Character) RETURN a' %>%\n  call_neo4j(con, type=\"graph\")\nSansa_Marriage$nodes <- Sansa_Marriage$nodes %>%\n  unnest_nodes(what = \"properties\") %>% \n  mutate(label = map_chr(label, 1))\n(Sansa_Marriage$nodes)\n\nSansa_Marriage$relationships <- Sansa_Marriage$relationships %>%\n  unnest_relationships() %>%\n  select(startNode, endNode, type, everything())\n(Sansa_Marriage$relationships)"},{"path":"from-neo4j-to-rstudio.html","id":"visualize-with-ggraph","chapter":"9 From Neo4j to RStudio","heading":"9.0.5 Visualize with ggraph","text":"Also, can use ggprah regenerate graph relational table.","code":"\nlibrary(ggraph)\ngraph_object <- igraph::graph_from_data_frame(\n  d = Sansa_Marriage$relationships, \n  directed = TRUE, \n  vertices = Sansa_Marriage$nodes\n)\n\ngraph_object %>%\n  ggraph() + \ngeom_node_label(aes(label = name)) +\ngeom_edge_link() + \ntheme_graph()"},{"path":"r-vs.-python-visualization-cheatsheet.html","id":"r-vs.-python-visualization-cheatsheet","chapter":"10 R vs. Python Visualization Cheatsheet","heading":"10 R vs. Python Visualization Cheatsheet","text":"Dawei Minhui LiaoWe found primarily work R data visualization may familiar graphs Python. lot differences R Python, usually use ggplot2 R, closest comparable libraries Matplotlib Seaborn working Python. Therefore, made PDF version cheat sheet people primarily use R Python need use one another programming language make plots, easily compare difference graphs.Click following link check cheat sheet:https://github.com/Aaralyn-Liao/EDAV_Contribution_CC8/blob/main/r_vs_python_visualization.pdf","code":""},{"path":"rpackage-waffle-cheatsheet.html","id":"rpackage-waffle-cheatsheet","chapter":"11 Rpackage waffle cheatsheet","heading":"11 Rpackage waffle cheatsheet","text":"Yibo ChenThis project mainly include pdf version cheatsheet R package waffle.Take look cheatsheet using link :\nhttps://github.com/ChenYb9807/cu_edav_cc/blob/main/Community%20Contribution.pdf","code":""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"r-cheatsheet-on-data-transforamtion-and-exploration","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12 R cheatsheet on data transforamtion and exploration","text":"Sai Krupa JangalaFew Pointers:purpose cheatsheet describe basic data operations start new project. also includes different types data transformations, explorations management.use data frames throughtout cheatsheet.use packages commonly used R. using packages different purposes.Different datasets used best illustrate transformation, management exploration.data sets used openintro::fastfood , openintro::seatlepets, cars, openintro::ames mtcars.Every chunk code loads data, ’s , means used data previous chunk.","code":""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"required-packages","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.1 Required packages","text":"packages commonly R used following cheatsheet. using packages different purposes.","code":"\n#install.packages(\"tidyverse\")\n#install.packages(\"dplyr\")\n#install.packages(\"reshape\")\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(reshape)"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"output-the-head-tail-and-sample-of-the-dataframe.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.2 Output the head, tail and sample of the dataframe.","text":"Head - get first 5 rows dataframe.Tail - get last 5 rows dataframe","code":"\ndata <- cars\nhead(data, n=5)##   speed dist\n## 1     4    2\n## 2     4   10\n## 3     7    4\n## 4     7   22\n## 5     8   16\ntail(data, n=5)##    speed dist\n## 46    24   70\n## 47    24   92\n## 48    24   93\n## 49    24  120\n## 50    25   85"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"selection-of-only-a-few-columns-from-a-dataframe.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.3 Selection of only a few columns from a dataframe.","text":"Select restaurant, item calorie columns fastfood dataset. Two variations shown .can see , transformations produce result.","code":"\ndata <- openintro::fastfood\n# Variation 1\ndata <- data[, c(\"restaurant\",\"item\",\"calories\")]\n# Variation 2\ndata_1 <- select(data, c(restaurant, item, calories))\nhead(data)## # A tibble: 6 × 3\n##   restaurant item                                      calories\n##   <chr>      <chr>                                        <dbl>\n## 1 Mcdonalds  Artisan Grilled Chicken Sandwich               380\n## 2 Mcdonalds  Single Bacon Smokehouse Burger                 840\n## 3 Mcdonalds  Double Bacon Smokehouse Burger                1130\n## 4 Mcdonalds  Grilled Bacon Smokehouse Chicken Sandwich      750\n## 5 Mcdonalds  Crispy Bacon Smokehouse Chicken Sandwich       920\n## 6 Mcdonalds  Big Mac                                        540\nhead(data_1)## # A tibble: 6 × 3\n##   restaurant item                                      calories\n##   <chr>      <chr>                                        <dbl>\n## 1 Mcdonalds  Artisan Grilled Chicken Sandwich               380\n## 2 Mcdonalds  Single Bacon Smokehouse Burger                 840\n## 3 Mcdonalds  Double Bacon Smokehouse Burger                1130\n## 4 Mcdonalds  Grilled Bacon Smokehouse Chicken Sandwich      750\n## 5 Mcdonalds  Crispy Bacon Smokehouse Chicken Sandwich       920\n## 6 Mcdonalds  Big Mac                                        540"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-column-names","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.4 Get the column names","text":"Generate column names data frame","code":"\ndata <- openintro::fastfood\nnames(data)##  [1] \"restaurant\"  \"item\"        \"calories\"    \"cal_fat\"     \"total_fat\"  \n##  [6] \"sat_fat\"     \"trans_fat\"   \"cholesterol\" \"sodium\"      \"total_carb\" \n## [11] \"fiber\"       \"sugar\"       \"protein\"     \"vit_a\"       \"vit_c\"      \n## [16] \"calcium\"     \"salad\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"drop-one-or-more-columns.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.5 Drop one or more columns.","text":"Drop columns restaurant, item, calories data. can observe, column names dropped.","code":"\ndata <- openintro::fastfood\ndata <- select(data, -c(restaurant, item, calories))\nnames(data)##  [1] \"cal_fat\"     \"total_fat\"   \"sat_fat\"     \"trans_fat\"   \"cholesterol\"\n##  [6] \"sodium\"      \"total_carb\"  \"fiber\"       \"sugar\"       \"protein\"    \n## [11] \"vit_a\"       \"vit_c\"       \"calcium\"     \"salad\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"transformation-using-the-transform-function-in-r","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.6 Transformation using the transform() function in R","text":"creating new dataframe changing speed column. transformed multiplying 100Here transformed original dataframe creating new column.","code":"\ndata <- cars\ndata_1 <- transform(data, speed=speed*100)\nhead(data_1)##   speed dist\n## 1   400    2\n## 2   400   10\n## 3   700    4\n## 4   700   22\n## 5   800   16\n## 6   900   10\ndata <- transform(data, time=speed*dist)\nhead(data)##   speed dist time\n## 1     4    2    8\n## 2     4   10   40\n## 3     7    4   28\n## 4     7   22  154\n## 5     8   16  128\n## 6     9   10   90"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"conditional-transformation-in-r","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.7 Conditional Transformation in R","text":"Transformation based condition. creating new column called Grilled, assigned Grilled item contains Grilled ’s name else classified Grilled.","code":"\n#Check if it contains the word grilled.\ndata <- openintro::fastfood\ndata <- transform(data, Grilled=ifelse(str_detect(item, \"Grilled\"), \"Grilled\", \"Not Grilled\"))\nhead(data[,c(\"item\",\"Grilled\")])##                                        item     Grilled\n## 1          Artisan Grilled Chicken Sandwich     Grilled\n## 2            Single Bacon Smokehouse Burger Not Grilled\n## 3            Double Bacon Smokehouse Burger Not Grilled\n## 4 Grilled Bacon Smokehouse Chicken Sandwich     Grilled\n## 5  Crispy Bacon Smokehouse Chicken Sandwich Not Grilled\n## 6                                   Big Mac Not Grilled"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"add-a-new-column-to-the-dataframe-without-transform-function.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.8 Add a new column to the dataframe without transform() function.","text":"added new column called time.","code":"\ndata <- cars\ndata$time <- data$speed * data$dist\nhead(data)##   speed dist time\n## 1     4    2    8\n## 2     4   10   40\n## 3     7    4   28\n## 4     7   22  154\n## 5     8   16  128\n## 6     9   10   90"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-all-the-unique-values-of-a-column-in-a-dataframe.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.9 Get all the unique values of a column in a dataframe.","text":"","code":"\ndata <- openintro::fastfood\nunique(data$restaurant)## [1] \"Mcdonalds\"   \"Chick Fil-A\" \"Sonic\"       \"Arbys\"       \"Burger King\"\n## [6] \"Dairy Queen\" \"Subway\"      \"Taco Bell\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"using-the-filter-function-in-r.-different-logical-operators-can-be-used-to-filter-the-data.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.10 Using the filter function in R. Different logical operators can be used to filter the data.","text":"Filter rows mpg column value 21.0Filter rows mpg column value less 21.0Filter rows mpg column value greater 21.0Filter rows cyl column 4 carb column greater 1.","code":"\ndata <- mtcars\ndata_filtered <- filter(data, mpg==21.0)\nunique(data_filtered$mpg)## [1] 21\ndata_filtered_1 <- filter(data, mpg<21.0)\nunique(data_filtered_1$mpg)##  [1] 18.7 18.1 14.3 19.2 17.8 16.4 17.3 15.2 10.4 14.7 15.5 13.3 15.8 19.7 15.0\ndata_filtered_2 <- filter(data, mpg>21.0)\nunique(data_filtered_2$mpg)## [1] 22.8 21.4 24.4 32.4 30.4 33.9 21.5 27.3 26.0\ndata_filtered_logical <- filter(data, cyl == 4 & carb > 1)\nunique(data_filtered_logical$mpg)## [1] 24.4 22.8 30.4 26.0 21.4"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"select-only-few-rows-in-a-column-based-on-a-condition-without-using-the-filter-function.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.11 Select only few rows in a column based on a condition without using the filter() function.","text":"selecting rows restuarant name subway","code":"\ndata <- openintro::fastfood\n# Select the rows where the item contains the word \"Grilled\ndata <- data[data$restaurant == \"Subway\", ] \nunique(data$restaurant)## [1] \"Subway\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"merge-two-dataframes","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.12 Merge two dataframes","text":"Merging two dataframes based column names. Authors dataframe books dataframe merged surname name.","code":"\nauthors <- data.frame(\n    surname = c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"McNeil\"),\n    nationality = c(\"US\", \"Australia\", \"US\", \"UK\", \"Australia\"),\n    retired = c(\"yes\", rep(\"no\", 4)))\nbooks <- data.frame(\n    name = c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"Ripley\", \"McNeil\"),\n    title = c(\"Exploratory Data Analysis\",\n              \"Probability and Statistics\",\n              \"Finance and Structuring for Data Science\",\n              \"Algorithms for Data Science\",\n               \"Interactive Data Analysis\",\n              \"Deep Learning\"))\n    #other.author = c(NA, \"Ripley\", NA, NA, NA, NA))\nmerged <- merge(authors, books, by.x=\"surname\", by.y=\"name\")\nhead(merged)##    surname nationality retired                                    title\n## 1   McNeil   Australia      no                            Deep Learning\n## 2   Ripley          UK      no              Algorithms for Data Science\n## 3   Ripley          UK      no                Interactive Data Analysis\n## 4  Tierney          US      no Finance and Structuring for Data Science\n## 5    Tukey          US     yes                Exploratory Data Analysis\n## 6 Venables   Australia      no               Probability and Statistics"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"arrange-the-data-in-ascending-order-based-on-a-column","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.13 Arrange the data in ascending order based on a column","text":"","code":"\ndata <- openintro::fastfood\n# Arranging the data in ascending order\ndata <- data[order(data$total_fat),] \nhead(data[, c(\"restaurant\",\"total_fat\")])## # A tibble: 6 × 2\n##   restaurant  total_fat\n##   <chr>           <dbl>\n## 1 Dairy Queen         0\n## 2 Subway              1\n## 3 Chick Fil-A         2\n## 4 Dairy Queen         2\n## 5 Subway              2\n## 6 Subway              2"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"arrange-the-data-in-descending-order-based-on-a-column","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.14 Arrange the data in descending order based on a column","text":"","code":"\n# Arranging the data in descending order\ndata <- data[order(data$total_fat, decreasing = TRUE),]\nhead(data[, c(\"restaurant\",\"total_fat\")])## # A tibble: 6 × 2\n##   restaurant  total_fat\n##   <chr>           <dbl>\n## 1 Mcdonalds         141\n## 2 Burger King       126\n## 3 Mcdonalds         107\n## 4 Sonic             100\n## 5 Sonic              92\n## 6 Mcdonalds          88"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-summary-of-a-column---mean-median-var-sd-etc","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.15 Get the summary of a column -> mean, median, var, SD etc","text":"function gives us Minimum value, 1st Quartile value, Median, Mean, 3rd Quartile value, Maximum value column data frame.","code":"\nsummary(data$total_fat)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    0.00   14.00   23.00   26.59   35.00  141.00"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"convert-all-the-values-in-the-column-to-upper-case.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.16 Convert all the values in the column to upper case.","text":"","code":"\ndata$item <- tolower(data$item)\nhead(data$item)## [1] \"20 piece buttermilk crispy chicken tenders\"      \n## [2] \"american brewhouse king\"                         \n## [3] \"40 piece chicken mcnuggets\"                      \n## [4] \"garlic parmesan dunked ultimate chicken sandwich\"\n## [5] \"super sonic bacon double cheeseburger (w/mayo)\"  \n## [6] \"12 piece buttermilk crispy chicken tenders\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"convert-all-the-values-in-the-column-to-lower-case.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.17 Convert all the values in the column to lower case.","text":"","code":"\ndata$item <- toupper(data$item)\nhead(data$item)## [1] \"20 PIECE BUTTERMILK CRISPY CHICKEN TENDERS\"      \n## [2] \"AMERICAN BREWHOUSE KING\"                         \n## [3] \"40 PIECE CHICKEN MCNUGGETS\"                      \n## [4] \"GARLIC PARMESAN DUNKED ULTIMATE CHICKEN SANDWICH\"\n## [5] \"SUPER SONIC BACON DOUBLE CHEESEBURGER (W/MAYO)\"  \n## [6] \"12 PIECE BUTTERMILK CRISPY CHICKEN TENDERS\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"dropping-nas","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.18 Dropping NAs","text":"Dropping rows one columns NA","code":"\ndata <- openintro::seattlepets\ndata <- data[complete.cases(data), ]\n# Removes the rows with one or more columns having a NA"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"groupby-and-summarize-functions-usage","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.19 groupby( ) and summarize( ) functions usage","text":"use ames data demonstrate functions. want find minimum, maximum area houses particular neighborhood, group Neighborhood compute minimum maximum area columns using summarise function.","code":"\ndata <- openintro::ames\ndata <- data %>% group_by(Neighborhood) %>% summarise(max_area= max(area), min_area=min(area))\nhead(data, n=10)## # A tibble: 10 × 3\n##    Neighborhood max_area min_area\n##    <fct>           <int>    <int>\n##  1 Blmngtn          1589     1142\n##  2 Blueste          1556     1020\n##  3 BrDale           1365      948\n##  4 BrkSide          2134      334\n##  5 ClearCr          3086      988\n##  6 CollgCr          2828      768\n##  7 Crawfor          3447      694\n##  8 Edwards          5642      498\n##  9 Gilbert          2462      864\n## 10 Greens           1295      788"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-shape-of-the-dataframe-in-r.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.20 Get the shape of the dataframe in R.","text":"know number rows columns dataframe.First number list number rows second number list number columns dataframe","code":"\ndata <- openintro::ames\ndim(data)## [1] 2930   82"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"select-only-top-30-rows-or-90-rows-bottom-30-rows","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.21 Select only top 30 rows or 90 rows, bottom 30 rows","text":"","code":"\ndata <- openintro::ames\n# Selecting the top 30 rows\ndata <- data[1:30,]\n# Selecting the top 90 rows\ndata <- data[1:90,]\ndata <- openintro::ames\ndata <- data[2900:2930,] \n# Select the last 30 rows if your dataframe consists of 2930 rows"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-data-type-of-each-column-in-a-dataframe.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.22 Get the data type of each column in a dataframe.","text":"","code":"\ndata <- openintro::seattlepets\nmap(data, class)## $license_issue_date\n## [1] \"Date\"\n## \n## $license_number\n## [1] \"character\"\n## \n## $animal_name\n## [1] \"character\"\n## \n## $species\n## [1] \"character\"\n## \n## $primary_breed\n## [1] \"character\"\n## \n## $secondary_breed\n## [1] \"character\"\n## \n## $zip_code\n## [1] \"character\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"changing-the-type-of-data---int-to-char","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.23 Changing the type of data -> int to char","text":"Change values column character integer valid values.Note: rows may marked NA coercion.","code":"\ndata <- openintro::seattlepets\nhead(data$zip_code)## [1] \"98108\" \"98117\" \"98136\" \"98117\" \"98144\" \"98103\"\ndata$zip_code <- as.numeric(data$zip_code)\nhead(data$zip_code)## [1] 98108 98117 98136 98117 98144 98103"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"rename-column-names-in-r","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.24 Rename column names in R","text":"renaming column name zip_code zip_code_modified","code":"\nnames(data)[names(data) == 'zip_code'] <- 'zip_code_modified'\nnames(data)## [1] \"license_issue_date\" \"license_number\"     \"animal_name\"       \n## [4] \"species\"            \"primary_breed\"      \"secondary_breed\"   \n## [7] \"zip_code_modified\"\n# Zip code has been modified to zip_code_modified"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"find-minimum-and-maximum-values-in-a-column-in-r","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.25 Find minimum and maximum values in a column in R","text":"Minimum value area columnMaximum value area column","code":"\ndata <- openintro::ames\nmin <- min(data$area)\nmax <- max(data$area)\nmin## [1] 334\nmax## [1] 5642"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"number-of-unique-values-in-every-column-in-the-data-frame","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.26 Number of unique values in every column in the data frame","text":"dataset, 13930 unique animal names, 4 different species etc.","code":"\ndata <- openintro::seattlepets\ndata %>% summarize_all(n_distinct)## # A tibble: 1 × 7\n##   license_issue_date license_number animal_name species primary_breed\n##                <int>          <int>       <int>   <int>         <int>\n## 1               1064          52497       13930       4           336\n## # … with 2 more variables: secondary_breed <int>, zip_code <int>"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"reorder-columns-in-r-by-column-name.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.27 Reorder columns in R by column name.","text":"","code":"\nauthors <- data.frame(\n    surname = c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"McNeil\"),\n    nationality = c(\"US\", \"Australia\", \"US\", \"UK\", \"Australia\"),\n    retired = c(\"yes\", rep(\"no\", 4)))\nauthors##    surname nationality retired\n## 1    Tukey          US     yes\n## 2 Venables   Australia      no\n## 3  Tierney          US      no\n## 4   Ripley          UK      no\n## 5   McNeil   Australia      no\n#reorder by column name\nauthors <- authors[, c(\"retired\", \"nationality\", \"surname\")]\nauthors##   retired nationality  surname\n## 1     yes          US    Tukey\n## 2      no   Australia Venables\n## 3      no          US  Tierney\n## 4      no          UK   Ripley\n## 5      no   Australia   McNeil"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"reorder-columns-by-column-index.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.28 Reorder columns by column index.","text":"","code":"\nauthors <- authors[, c(1,3,2)]\nauthors##   retired  surname nationality\n## 1     yes    Tukey          US\n## 2      no Venables   Australia\n## 3      no  Tierney          US\n## 4      no   Ripley          UK\n## 5      no   McNeil   Australia"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"remove-duplicates-from-the-dataframe-based-on-one-column-or-multiple-columns.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.29 Remove Duplicates from the dataframe based on one column or multiple columns.","text":"","code":"\n# Remove the duplicate rows based on one variable\ndata <- mtcars\n# Have the rows with distinct carb\ndata_one_var <- distinct(data, carb, .keep_all= TRUE)\n# Keep the distinct data based on multiple variables\ndata_multi <- distinct(data, cyl, vs, .keep_all= TRUE)"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"calculate-mean-median-of-a-column-in-the-data-frame","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.30 Calculate Mean, Median of a column in the Data Frame","text":"MeanMedian","code":"\nmean <- mean(data$carb)\nmedian <- median(data$carb)\nmean## [1] 2.8125\nmedian## [1] 2"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"value-counts-in-r.-check-how-many-times-a-unique-variable-occurs-in-like-male---5-female--10-in-the-column-name-gender.","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.31 Value counts in R. Check how many times a unique variable occurs in like Male - 5, Female -10 in the column name Gender.","text":"Number rows cyl = 4 11, cyl=6 7 etcNumber rows carb = 4 10, carb=6 1 etc","code":"\ndata %>% count(cyl)##   cyl  n\n## 1   4 11\n## 2   6  7\n## 3   8 14\ndata %>% count(carb)##   carb  n\n## 1    1  7\n## 2    2 10\n## 3    3  3\n## 4    4 10\n## 5    6  1\n## 6    8  1"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"convert-the-index-column-to-a-new-column","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.32 Convert the index column to a new column","text":"created new column using index dataframe.","code":"\ndata <- cbind(car_name = rownames(data), data)\nhead(data)##                            car_name  mpg cyl disp  hp drat    wt  qsec vs am\n## Mazda RX4                 Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1\n## Datsun 710               Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive       Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0\n## Hornet Sportabout Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0\n## Valiant                     Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"add-a-new-row-to-the-dataframe","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.33 Add a new row to the dataframe","text":"can new rows dataframe using rbind","code":"\nnew_row_to_add <- data.frame(\"Volvo 125\",22.5,3,120.2,108,2.23,2.89,19.08,1,0,4,3)\nnames(new_row_to_add) <- c(\"car_name\", \"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\")\ndata <- rbind(data,new_row_to_add)\ntail(data)##                      car_name  mpg cyl  disp  hp drat    wt  qsec vs am gear\n## Lotus Europa     Lotus Europa 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5\n## Ford Pantera L Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5\n## Ferrari Dino     Ferrari Dino 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5\n## Maserati Bora   Maserati Bora 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5\n## Volvo 142E         Volvo 142E 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4\n## 1                   Volvo 125 22.5   3 120.2 108 2.23 2.890 19.08  1  0    4\n##                carb\n## Lotus Europa      2\n## Ford Pantera L    4\n## Ferrari Dino      6\n## Maserati Bora     8\n## Volvo 142E        2\n## 1                 3"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"logsquare-root-cube-root-transformation-in-r","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.34 Log,Square root, Cube root transformation in R","text":"","code":"\ndata <- cars\n# Taking the log of the speed column\ndata$log_transformation <- log10(data$speed)\n# Taking the square root of the speed column\ndata$sqrt_transformation <- sqrt(data$speed)\n# Taking the cube root of the speed column\ndata$cube_transformation <-(data$speed)^1/3\nhead(data)##   speed dist log_transformation sqrt_transformation cube_transformation\n## 1     4    2          0.6020600            2.000000            1.333333\n## 2     4   10          0.6020600            2.000000            1.333333\n## 3     7    4          0.8450980            2.645751            2.333333\n## 4     7   22          0.8450980            2.645751            2.333333\n## 5     8   16          0.9030900            2.828427            2.666667\n## 6     9   10          0.9542425            3.000000            3.000000"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"changing-the-dataframe-dimensions-from-wide-to-long","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.35 Changing the dataframe dimensions from wide to long","text":"following types can use melt() function R.Type 1:\ncreate two columns called variable - represent subject value represents grade subject.Type 2:\ninterested grade english math, can pass measure.vars parameter.","code":"\ndf_wide <- data.frame(\n  student = c(\"Krupa\", \"Goutham\", \"Sailaja\", \"Murthy\"),\n  school = c(\"St. Joseph's\", \"Timpany\", \"St.Joseph's\", \"Timpany\"),\n  exploratory_data_analysis = c(10, 100, 1000, 10000),  # eng grades\n  probability_and_statistics = c(20, 200, 2000, 20000),  # math grades\n  algorithms_for_ds = c(30, 300, 3000, 30000)   # physics grades\n)\ndf_long <- melt(data = df_wide, \n                id.vars = c(\"student\", \"school\"))\ndf_long##    student       school                   variable value\n## 1    Krupa St. Joseph's  exploratory_data_analysis    10\n## 2  Goutham      Timpany  exploratory_data_analysis   100\n## 3  Sailaja  St.Joseph's  exploratory_data_analysis  1000\n## 4   Murthy      Timpany  exploratory_data_analysis 10000\n## 5    Krupa St. Joseph's probability_and_statistics    20\n## 6  Goutham      Timpany probability_and_statistics   200\n## 7  Sailaja  St.Joseph's probability_and_statistics  2000\n## 8   Murthy      Timpany probability_and_statistics 20000\n## 9    Krupa St. Joseph's          algorithms_for_ds    30\n## 10 Goutham      Timpany          algorithms_for_ds   300\n## 11 Sailaja  St.Joseph's          algorithms_for_ds  3000\n## 12  Murthy      Timpany          algorithms_for_ds 30000\ndf_long <- melt(data = df_wide, \n                id.vars = \"student\",\n                measure.vars = c(\"exploratory_data_analysis\", \"algorithms_for_ds\"))\ndf_long##   student                  variable value\n## 1   Krupa exploratory_data_analysis    10\n## 2 Goutham exploratory_data_analysis   100\n## 3 Sailaja exploratory_data_analysis  1000\n## 4  Murthy exploratory_data_analysis 10000\n## 5   Krupa         algorithms_for_ds    30\n## 6 Goutham         algorithms_for_ds   300\n## 7 Sailaja         algorithms_for_ds  3000\n## 8  Murthy         algorithms_for_ds 30000"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"replace-na-with-a-specific-value","chapter":"12 R cheatsheet on data transforamtion and exploration","heading":"12.0.0.36 Replace NA with a specific value","text":"replacing NA None.References:\n* https://towardsdatascience.com/data-cleaning--r-made-simple-1b77303b0b17\n* https://towardsdatascience.com/data-transformation--r-288e95438ff9\n* https://bookdown.org/aschmi11/RESMHandbook/data-preparation--cleaning--r.html","code":"\ndata <- openintro::seattlepets\ndata[is.na(data)] <- \"None\""},{"path":"r-vs-python-basic-data-wrangling.html","id":"r-vs-python-basic-data-wrangling","chapter":"13 R vs Python Basic Data Wrangling","heading":"13 R vs Python Basic Data Wrangling","text":"Yihao GaoThis pdf syntax comparison cheat sheet basic data wrangling R (dplyr, tidyr) Python (pandas) including row/column selection, missing value imputation, sorting, mapping, grouping aggregation etc. motivation creating cheat sheet give quick reference might sometimes mix syntax Python R save time.Access cheat sheet using link .\nhttps://github.com/yiiihao/yg2820_edav_CC/blob/main/cheetsheet.pdf","code":""},{"path":"tidyverse_cheatsheet.html","id":"tidyverse_cheatsheet","chapter":"14 tidyverse_cheatsheet","heading":"14 tidyverse_cheatsheet","text":"Ziyu SongMotivation:R one widely used programming languages today’s society. R can help people build statistical models also visualize data analysis results. However, many R packages functions users may feel hard remember details apply data analysis process. project, create cheat sheet Tidyverse, powerful collection R packages data tools transforming visualizing data, help users quickly find information need want exploratory data visualization R.cheat sheet includes main knowledge points popular functions Tidyverse, including definition Tidyverse, core Tidyverse packages, typical data analysis workflow using Tidyverse, key functions core packages. order make easier users understand function provided, incorporated example axis parameters.Self-Evaluation:creating cheat sheet summarizing main knowledge points Tidyverse, gained better understanding main packages Tidyverse consolidated knowledge learned class. found Tidyverse provides efficient, fast, well-documented workflow general data modeling visualization tasks. hand, many important useful packages Tidyverse. Therefore, hope create complete cheat sheet Tidyverse next time help people better understand apply data analysis process.Cheatsheet:\nhttps://github.com/zs2488/cc21fall1/blob/main/Community%20Contribution%20-%20Cheat%20Sheet.pdf","code":""},{"path":"cheatsheet-tidyverse.html","id":"cheatsheet-tidyverse","chapter":"15 Cheatsheet Tidyverse","heading":"15 Cheatsheet Tidyverse","text":"Jiazhen Li","code":""},{"path":"cheatsheet-tidyverse.html","id":"section","chapter":"15 Cheatsheet Tidyverse","heading":"15.1 ","text":"cheatsheet helping students start learn analysis graph R. class, learn tidyverse, powerful collection R packages. cheatsheet, pay attention two packages: ggplot2 dplyr. specifically list functions dylyr frequently use grammer graphics ggplot2 packages. useful new student learn.Cheatsheet Tidyverse: https://github.com/Jiazhen980326/contribution-/blob/master/cheatsheet-Jiazhen%20Li%20.pdf","code":""},{"path":"use-of-ggprism-and-ggsci-packages.html","id":"use-of-ggprism-and-ggsci-packages","chapter":"16 Use of ggprism and ggsci packages","heading":"16 Use of ggprism and ggsci packages","text":"Yifan JiangThis cheatsheet introduces use ggprism ggsci respectively set style color picture generated ggplot. style color used large number papers.cheatsheet link:\nhttps://github.com/Josieeeeee/cc_github/blob/main/ggprism_and_ggsci_cheatsheets.pdf","code":""},{"path":"interactive-plot-introduction.html","id":"interactive-plot-introduction","chapter":"17 Interactive plot introduction","heading":"17 Interactive plot introduction","text":"Yujie ZhouInteractive plots used widely today’s data analysis.tutorial introduce four commonly used interactive plots: interactive bubble plot, interactive area plot, interactive heatmap, interactive network. Unlike static plot, interactive plot can enable users zoom give user better use experience simplify analysis process.","code":"\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(gapminder)\nlibrary(\"tidyr\")\nlibrary(\"heatmaply\")\nlibrary(igraph)\nlibrary(networkD3)"},{"path":"interactive-plot-introduction.html","id":"interactive-bubble-plot","chapter":"17 Interactive plot introduction","heading":"17.1 Interactive Bubble Plot:","text":"bubble plot third dimension added scatterplot. size bubble represent additional numeric variable.can draw plot directly using “plotly” just right draw bubble plot using “ggplot”. following example see number lifespan people different nations associated.interactive bubble plot show positive relationship GPD human’s lifespan, also gives additional information number people dot. However, drawback bubble plots hard judge relationship x y variables variables (polpulation size example).","code":"\nknitr::opts_chunk$set(warning = F, message = F)\n\nhead(gapminder)## # A tibble: 6 × 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786.\np <- gapminder %>%\n  filter(year==1967) %>%\n  ggplot( aes(x=gdpPercap, y=lifeExp, size=pop,color=continent)) +\n  geom_point() +\n  scale_x_log10() +\n  theme_bw()\n\nggplotly(p)"},{"path":"interactive-plot-introduction.html","id":"interactive-area-plot","chapter":"17 Interactive plot introduction","heading":"17.2 Interactive Area Plot:","text":"Area plot differ lot regular line graphs, two exceptions:\n1. area x-axis individual line filled color\n2. x-axis must zero valueArea plots use different colors give users good sense quantities changed time period time. However, users carefully use area plots, overlapping areas plots hide information area line hard percisely calculated user just looking graph.can easily see student mean performance subject associated parents’ educational background just glancing area separate line.","code":"\nknitr::opts_chunk$set(warning = F, message = F)\ndf <- read.csv(\"resources/interactive_plot_plotly_cheatsheet/StudentsPerformancePart.csv\")\nhead(df)##   gender race.ethnicity parental.level.of.education        lunch\n## 1 female        group B           bachelor's degree     standard\n## 2 female        group C                some college     standard\n## 3 female        group B             master's degree     standard\n## 4   male        group A          associate's degree free/reduced\n## 5   male        group C                some college     standard\n## 6 female        group B          associate's degree     standard\n##   test.preparation.course math.score reading.score writing.score\n## 1                    none         72            72            74\n## 2               completed         69            90            88\n## 3                    none         90            95            93\n## 4                    none         47            57            44\n## 5                    none         76            78            75\n## 6                    none         71            83            78\ndf %>%\n  filter(gender == \"female\",race.ethnicity==\"group B\")%>%\n  pivot_longer(\n    cols = c('math.score','reading.score','writing.score'),\n    names_to = \"Subject\",\n    values_to = \"score\"\n  ) %>%\n  group_by(Subject, parental.level.of.education) %>%\n  summarise(score = mean(score)) %>%\n  mutate(parental.level.of.education = factor(parental.level.of.education,levels=c(\"associate's degree\", \"bachelor's degree\", \"high school\", \"master's degree\", \"some college\", \"some high school\")))%>%\n  mutate(parental.level.of.education = as.numeric(parental.level.of.education)) %>%\n  ggplot(aes(x= parental.level.of.education ,y=score,fill=Subject))+\n  geom_area(alpha = 0.7) +\n  scale_fill_manual(values = c(\"#F6D7A7\", \"#C8E3D4\", \"#87AAAA\")) +\n  scale_x_continuous(breaks =1:6,labels = c(\"associate's degree\", \"bachelor's degree\", \"high school\", \"master's degree\", \"some college\", \"some high school\"))+\n  theme_minimal() -> p2\n\np2\nggplotly(p2)"},{"path":"interactive-plot-introduction.html","id":"interactive-heatmap","chapter":"17 Interactive plot introduction","heading":"17.3 Interactive Heatmap:","text":"Heatmap useful visualization tool two-dimensional data reveal patterns correlations rows columns. easy--use, introduce interactive heatmap. already familiar heatmap R package draw heatmap, use useful R package “heatmaply” build interactive cluster heatmap.mtcars collection fuel consumption corresponding 10 automobile designs 32 automobiles. noteworthy first data transformations, normalizing percentizing make variables comparable. Normalizing matrix done using scale argument. can applied row column. column option chosen.Passing NULL Colv heatmap tends reorder column clustering algorithm. Removing column dendrogram can enable users compare raw data.also good use terrain.color(), rainbow(), heat.colors(), topo.colors() cm.colors() interchangeably selecting different color palette heatmap.Instead vertically naming x-axis heatmap , interactive heatmap automatically rotate name x-axis values names long fit.visual user friendly tool,shinyHeatmaply, create interactive heatmap invented byJonathan Sidi. apply tool, can install ‘shinyHeatmaply’ R package, , alternatively, run GitHub entering “devtools::install_github(‘yonicd/shinyHeatmaply’)”. output heatmap shinyHeatmaply provide detail parameter summaries.","code":"\nknitr::opts_chunk$set(warning = F, message = F)\ndata(\"mtcars\")\nheatmaply(mtcars, scale=\"column\", Colv = NULL,col =  topo.colors(10),xlab=\"design\", ylab=\"car type\", main=\" interactive heatmap\")"},{"path":"interactive-plot-introduction.html","id":"interactive-network","chapter":"17 Interactive plot introduction","heading":"17.4 Interactive Network","text":"Network consist mainly two parts: nodes edge. graph reflect interrelationships note (.e. entity). advantage using network include important entities analyze whole instead see entity separately. Two addtional packages need import interactive network igraph networkD3. Let’s first see igraph used plot static network square matrix generated ramdonly.arrow indicates direction relationship two notes. example, one relationship 4 2, relationship 2 4.Now, let’s step interactive networks. simple function: simpleNetwork can generate interactive network handy way.Interactive network can rotate, zoom , zoom network see 3D layout. Compared static network, interactive network much better looking data gets bigger, avoid overlapping links.","code":"\nknitr::opts_chunk$set(warning = F, message = F)\n\nset.seed(12345)\nrandomdf <- matrix(sample(0:3, 16, replace=TRUE), nrow=4)\n\noutput <- graph_from_adjacency_matrix(randomdf)\nplot(output)\nknitr::opts_chunk$set(warning = F, message = F)\n\ndata <- data.frame(\n  from=c(\"A\", \"A\", \"B\", \"D\", \"C\", \"D\", \"E\", \"B\", \"C\", \"D\", \"K\", \"A\", \"M\"),\n  to=c(\"B\", \"E\", \"F\", \"A\", \"C\", \"A\", \"B\", \"Z\", \"A\", \"C\", \"A\", \"B\", \"K\") #reference:https://www.r-graph-gallery.com/network-interactive.html\n)\np <- simpleNetwork(data,height=\"50px\", width=\"50px\",        \n                   Source = 1,\n                   Target = 2,\n                   fontSize = 25,                    \n                   linkColour = \"#123\",   \n                   nodeColour = \"#F47E5E\",    \n                   opacity = 0.9,             \n                   zoom = T)\np"},{"path":"interactive-plot-introduction.html","id":"reference-1","chapter":"17 Interactive plot introduction","heading":"17.5 Reference:","text":"https://www.r-graph-gallery.com/network-interactive.html","code":""},{"path":"common-r-markdown-syntax-cheatsheet.html","id":"common-r-markdown-syntax-cheatsheet","chapter":"18 Common R markdown syntax cheatsheet","heading":"18 Common R markdown syntax cheatsheet","text":"Yufan Cao Yingfei ZhuR Markdown file format making dynamic documents R. provides authoring frame work data science. R Markdown two main purpose:\n1. Save execute code\n2. Generate high quality reports can shared audienceR Markdown documents can support various dynamic static output format, pdf Html. syntax R Markdown also important, students need learn write Markdown syntax produce concise clear reports.","code":""},{"path":"common-r-markdown-syntax-cheatsheet.html","id":"markdown-syntax","chapter":"18 Common R markdown syntax cheatsheet","heading":"18.1 Markdown Syntax","text":"","code":""},{"path":"common-r-markdown-syntax-cheatsheet.html","id":"workflow","chapter":"18 Common R markdown syntax cheatsheet","heading":"18.1.1 Workflow","text":"Open new.Rmd file RStudio IED goting File > New File > R Markdown\nOpen new.Rmd file RStudio IED goting File > New File > R MarkdownEmbed code chunks, run code line, chunk, .\nEmbed code chunks, run code line, chunk, .Write text add tables, images, figures, citations. Format Markdown syntax RStudio Visual Markdown Editor.\nWrite text add tables, images, figures, citations. Format Markdown syntax RStudio Visual Markdown Editor.Set Output format(s) options YAML header. Customize themes add parameters execute add interactivity Shiny.\nSet Output format(s) options YAML header. Customize themes add parameters execute add interactivity Shiny.Save render whole document. Knit periodically preview work write.\nSave render whole document. Knit periodically preview work write.Present share work.\nPresent share work.","code":""},{"path":"common-r-markdown-syntax-cheatsheet.html","id":"inline-formatting","chapter":"18 Common R markdown syntax cheatsheet","heading":"18.1.2 Inline Formatting","text":"Italic inline text: surrounded underscores asterisks (e.g., _text_ *text*)\nItalic inline text: surrounded underscores asterisks (e.g., _text_ *text*)Bold text: produced using pair double asterisks surrounded two underscores (e.g., **text** `__text_``).\nBold text: produced using pair double asterisks surrounded two underscores (e.g., **text** `__text_``).Turn text subscript: using pair tildes (~) (e.g., H~3~PO~4~ renders H3PO4)\nTurn text subscript: using pair tildes (~) (e.g., H~3~PO~4~ renders H3PO4)Turn text superscript: using pair carets(^) (e.g., Cu^2+^ renders Cu2+)\nTurn text superscript: using pair carets(^) (e.g., Cu^2+^ renders Cu2+)add line text: using two pair tildes(~~), (e.g., ~~text~~ renders text)\nadd line text: using two pair tildes(~~), (e.g., ~~text~~ renders text)hyperlink: use syntax [text](link), (e.g.,link` renders link)\nhyperlink: use syntax [text](link), (e.g.,link` renders link)Mark text inline code: use pair backticks, (e.g., `code`), include n literal backticks, use least n+1 backticks outside, (e.g., can use three backticks preserve two backtick inside: ``` ``code`` ```, rendered ``code`` )\nMark text inline code: use pair backticks, (e.g., `code`), include n literal backticks, use least n+1 backticks outside, (e.g., can use three backticks preserve two backtick inside: ``` ``code`` ```, rendered ``code`` )\n```text```\nrenders>block quotes render \n>block quotes render asblock quotesEquation: $e^{\\pi} +1 = 0$ renders \\(e^{\\pi} +1 = 0\\)\nEquation: $e^{\\pi} +1 = 0$ renders \\(e^{\\pi} +1 = 0\\)Equation blocks: $$E = mc^{2}$$ renders\n\\[E = mc^{2}\\]\nEquation blocks: $$E = mc^{2}$$ renders\n\\[E = mc^{2}\\]‘- ordered list’ renders\n‘- ordered list’ rendersordered listordered listtext size: use number sign # adjust text size header, # increase, size header decrease.\ntext size: use number sign # adjust text size header, # increase, size header decrease.(e.g., # Header renders","code":"text "},{"path":"header.html","id":"header","chapter":"19 Header","heading":"19 Header","text":"(e.g., #### Header renders","code":""},{"path":"header.html","id":"header-1","chapter":"19 Header","heading":"19.0.0.1 Header","text":"Syntax images: add exclamation mark, (e.g., ![alt text image title](path//image))\nSyntax images: add exclamation mark, (e.g., ![alt text image title](path//image))Footnotes: put inside square brackets caret ^, (e.g., ^[footnote].)\nFootnotes: put inside square brackets caret ^, (e.g., ^[footnote].)Insert citation: typing [@cite] @cite\nInsert citation: typing [@cite] @citePlain text: just type text chunk.\nPlain text: just type text chunk.Start new paragraph: End line two spaces start new paragraph\nStart new paragraph: End line two spaces start new paragraphmake new line: End backslash make new line\nmake new line: End backslash make new linespecial symbol, (e.g., escaped: \\*\\_ \\\\ renders escaped: *_ \\)\nspecial symbol, (e.g., escaped: \\*\\_ \\\\ renders escaped: *_ \\)endash: (e.g., endash:--, renders endash:–; endash:---, renders endash:—,)\nendash: (e.g., endash:--, renders endash:–; endash:---, renders endash:—,)List: `- unordered list’ renders\nList: `- unordered list’ rendersunordered listunordered listOrdered List:\nOrdered List:- Item 1\n   - Item 1a\n   - Item 2b\n               rendersItem 1\nItem 1a\nItem 2b\nItem 1aItem 2bIndent text: use line blocks starting line vertical bar (|). line breaks leading spaces preserved output. e.g.,\nIndent text: use line blocks starting line vertical bar (|). line breaks leading spaces preserved output. e.g.,put isR free software comes ABSOLUTELY WARRANTY.\n    R collaborative project many contributors.\nwelcome redistribute certain conditions.\n     Platform: x86_64-apple-darwin17.0 (64-bit)Generate grey text block: start ```\\ end ```, write text block, e.g.,\nGenerate grey text block: start ```\\ end ```, write text block, e.g.,```\\R useful software.```output ","code":"| R is free software and comes with ABSOLUTELY NO WARRANTY.\n|     R is a collaborative project with many contributors.\n| You are welcome to redistribute it under certain conditions.\n|      Platform: x86_64-apple-darwin17.0 (64-bit)R is a useful software."},{"path":"header.html","id":"out-put-format","chapter":"19 Header","heading":"19.0.1 Out put format","text":"html_document creates .html\nhtml_document creates .htmlpdf_document* creates .pdf\npdf_document* creates .pdfword_document creates .docx(Microsoft Word)\nword_document creates .docx(Microsoft Word)powerpoint_presentation creates .pptx(Microsoft Powerpoint)\npowerpoint_presentation creates .pptx(Microsoft Powerpoint)odt_document creates OpenDocument Text\nodt_document creates OpenDocument Textmd_document creates Markdown\nmd_document creates Markdowngithub_document creates Markdown Github\ngithub_document creates Markdown Githubioslides_presentation creates ioslides HTML slides\nioslides_presentation creates ioslides HTML slidesslidy_presentation creates Slidy HTML slides\nslidy_presentation creates Slidy HTML slidesrtf_document creates Rich Text Format\nrtf_document creates Rich Text Formatbeamer_presentation creates Beamer Slides\nbeamer_presentation creates Beamer SlidesRequires LateX, bookdown format, use tinytex::install_tinytex()\nRequires LateX, bookdown format, use tinytex::install_tinytex()output format names YAML metadata Rmd file, need include package name format extension package, e.g.,format Rmarkdown package, need rmarkdown:: prefix","code":"output:tufte::tufte_html"},{"path":"header.html","id":"embed-code-with-knitr","chapter":"19 Header","heading":"19.0.2 Embed Code with knitr","text":"4.1 CODE CHUNKSSurround code chunks ```{r}``` use insert Code Chunk button. Add chunk label /chunk options inside curly braces r.4.2 SET GLOBAL OPTIONSSet options entire document first chunk.4.3 INLINE CODESet r<code> text sections. Code evaluated render results appear text.","code":"`  ```{r chunk-label, include = False}\nsummary(mtcars)``` ` `  ```{r include = FALSE}\nknitr::opts_chunk$set(message = FALSE)``` ` \n"},{"path":"header.html","id":"render","chapter":"19 Header","heading":"19.0.3 Render","text":"Use rmarkdown::render() render/knit cmd line.\nImportant args:5.1 input- file render5.1 input- file render5.2 output_options - list render options(YAML)5.2 output_options - list render options(YAML)5.3 output_file/output_dir5.3 output_file/output_dir5.4 params - list parameters use5.4 params - list parameters use5.5 envir - environment evaluate code chunks in5.5 envir - environment evaluate code chunks in5.6 encoding - input file5.6 encoding - input file","code":""},{"path":"header.html","id":"chunk-options","chapter":"19 Header","heading":"19.0.4 CHUNK OPTIONS","text":"cache - cache results future knits(default = FALSE)cache - cache results future knits(default = FALSE)dependson - chunk dependencies caching(default = NULL)dependson - chunk dependencies caching(default = NULL)echo - Display code output document(default = TRUE)echo - Display code output document(default = TRUE)engine - code language used chunk(default = ‘R’)engine - code language used chunk(default = ‘R’)error - Display error messages doc(TRUE) stop render errors occur(FALSE) (default = FALSE)error - Display error messages doc(TRUE) stop render errors occur(FALSE) (default = FALSE)eval - Run code chunk(default = TRUE)eval - Run code chunk(default = TRUE)cache.path - directory save cached results (default = ‘cache/’)cache.path - directory save cached results (default = ‘cache/’)child - file(s) knit include (default = NULL)child - file(s) knit include (default = NULL)collapse - collapse output single block (default = FALSE)collapse - collapse output single block (default = FALSE)comment - prefix line results (default = ‘##’)comment - prefix line results (default = ‘##’)fig.align -‘left’,‘right’, ‘center’ (default = ‘default’)fig.align -‘left’,‘right’, ‘center’ (default = ‘default’)fig.cap - figure caption character string (default = NULL)fig.cap - figure caption character string (default = NULL)fig.height, fig.width - Dimensions plots inchesfig.height, fig.width - Dimensions plots incheshighlight - highlight source code (default = TRUE)highlight - highlight source code (default = TRUE)include - include chunk doc running (default = TRUE)include - include chunk doc running (default = TRUE)message - display code messages document (default = TRUE)message - display code messages document (default = TRUE)results (default = ‘markup’)\n‘asis’ - passthrough results\n‘hide’ - display results\n‘hold’ - put results coderesults (default = ‘markup’)\n‘asis’ - passthrough results\n‘hide’ - display results\n‘hold’ - put results codetidy - tidy code display (default = FALSE)tidy - tidy code display (default = FALSE)warning - display code warnings document (default = TRUE)warning - display code warnings document (default = TRUE)","code":""},{"path":"header.html","id":"mathematical-expression","chapter":"19 Header","heading":"19.1 Mathematical Expression","text":"","code":""},{"path":"header.html","id":"mathematical-notation","chapter":"19 Header","heading":"19.1.1 Mathematical notation","text":"side text chunk rmd file, can use mathematical notation dollar sign two different styles.Inline LaTeX equations can written surround pair dollar signs using LaTeX syntax. Example: $f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$ produce \\(f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\)Display style math expressions can written pair double dollar signs. Example $$f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$$ produce \\[f(k) = {n \\choose k} p^{k} (1-p)^{n-k}\\]Notice: space $ mathematical notation.\ncan also use math environments inside $ $ $$ $$","code":""},{"path":"header.html","id":"common-mathematical-symbol","chapter":"19 Header","heading":"19.1.2 Common Mathematical Symbol","text":"Math Mode AccentsBinary RelationOperators / Statistical ExpressionOther symbols","code":""},{"path":"header.html","id":"matrix","chapter":"19 Header","heading":"19.1.3 Matrix","text":"Matrix bracket\\[X = \\begin{array}{ccc}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{array}\\]square bracket\\[X = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{bmatrix}\\]parentheses\\[X = \\begin{pmatrix}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{pmatrix}\\]determinant / vertical bar bracket\\[\\begin{vmatrix} \n   a_{11} & a_{12} & a_{13}  \\\\\n   a_{21} & a_{22} & a_{23}  \\\\\n   \\end{vmatrix}\\]curly brackets\\[\\begin{Bmatrix} \n   a_{11} & a_{12} & a_{13}  \\\\\n   a_{21} & a_{22} & a_{23}  \\\\\n   \\end{Bmatrix} \\]double vertical bar brackets\\[\\begin{Vmatrix} \n   a_{11} & a_{12} & a_{13}  \\\\\n   a_{21} & a_{22} & a_{23}  \\\\\n   \\end{Vmatrix} \\]Small inline matrixSmall inline matrix$\\big(\\begin{smallmatrix} & b\\\\ c & d \\end{smallmatrix}\\big)$ produce\nSmall inline matrix\\(\\big(\\begin{smallmatrix} & b\\\\ c & d \\end{smallmatrix}\\big)\\)","code":"$$X = \\begin{array}{ccc}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{array}$$  $$X = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{bmatrix}$$$$X = \\begin{pmatrix}\nx_{11} & x_{12} & x_{13}\\\\\nx_{21} & x_{22} & x_{23}\n\\end{pmatrix}$$$$\\begin{vmatrix} \n   a_{11} & a_{12} & a_{13}  \\\\\n   a_{21} & a_{22} & a_{23}  \\\\\n   \\end{vmatrix} $$$$\\begin{Bmatrix} \n   a_{11} & a_{12} & a_{13}  \\\\\n   a_{21} & a_{22} & a_{23}  \\\\\n   \\end{Bmatrix} $$$$\\begin{Vmatrix} \n   a_{11} & a_{12} & a_{13}  \\\\\n   a_{21} & a_{22} & a_{23}  \\\\\n   \\end{Vmatrix} $$"},{"path":"header.html","id":"greek-letters","chapter":"19 Header","heading":"19.1.4 Greek letters","text":"","code":""},{"path":"header.html","id":"aligning-equations","chapter":"19 Header","heading":"19.1.5 Aligning Equations","text":"want sequence aligned equations (often useful demonstrating algebraic manipulation plugging values equations), use \\begin{align*} ... \\end{align*}. Separate lines \\\\ use & mark things line . Note: dollar signs needed mathematical expression method.Example:\\(\\begin{aligned} AR(p): Y_i &= c + \\epsilon_i + \\phi_i Y_{-1} \\dots \\\\ Y_{} &= c + \\phi_i Y_{-1} \\dots \\end{aligned}\\)","code":"$\\begin{aligned}\nAR(p): Y_i &= c + \\epsilon_i + \\phi_i Y_{i-1} \\dots \\\\\nY_{i} &= c + \\phi_i Y_{i-1} \\dots\n\\end{aligned}$"},{"path":"header.html","id":"reference-2","chapter":"19 Header","heading":"19.2 Reference","text":"Latex math symbols. Kapeli. (n.d.). Retrieved October 28, 2021, fromhttps://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index.Pruim, R. (2016, October 19). Mathematics R markdown. Retrieved October 28, 2021, fromhttps://rpruim.github.io/s341/S19/-class/MathinRmd.html.R markdown : : Cheat sheet - ETH Z. (n.d.). Retrieved October 28, 2021, fromhttps://ethz.ch/content/dam/ethz/special-interest/math/statistics/sfs/Education/Advanced%20Studies%20in%20Applied%20Statistics/course-material-1719/Datenanalyse/rmarkdown-2.pdf.Yihui Xie, C. D. (2021, October 7). R markdown cookbook. 5.2 Indent text. Retrieved October 28, 2021, https://bookdown.org/yihui/rmarkdown-cookbook/indent-text.html.Yihui Xie, J. J. . (2021, April 9). R markdown: definitive guide. Home. Retrieved October 28, 2021, fromhttps://bookdown.org/yihui/rmarkdown/.","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"parallel-coordinates-plot-cheatsheet","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20 Parallel coordinates plot cheatsheet","text":"Andrew SchaeferThis cheatsheet goes different options coding techniques displaying parallel coordinate plots.","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"libraries-ggally-ggparallel-and-parcoords","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.1 Libraries: GGally, ggparallel, and parcoords","text":"general, GGally used continuous data ggparallel useful discrete data. ’ll start beginning examples . parcoords introduces interactive parallel coordinate plots, explained . First, import libraries","code":"\nlibrary(GGally)\nlibrary(ggparallel)\nlibrary(parcoords)\nlibrary(ggplot2)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"package-ggally","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.2 Package: GGally","text":"","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"method-ggparcoord","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.3 Method: ggparcoord","text":"discussed class, ggparcoord usually go-parallel coordinate plots. dataset loaded, can pass directly method indicate columns show.","code":"\ndata(mtcars)\nhead(mtcars)##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nggparcoord(mtcars)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"parameters","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.3.1 Parameters","text":"interesting graph right? Let’s slowly add useful parameters. Columns can either range column numbers (2:4) vector shown . groupColumn coloring data single column (input take column value numerical, name column string). data continuous, gradient used.\nsplineFactors curves lines, may make trends visible depending data. alphaLines make lines transparent, prticularly useful large datasets. also prevents colors different lines overlapping . parameters help visualizing data.","code":"\nggparcoord(mtcars, columns = c(1:6,9,10), groupColumn = 2)\nggparcoord(mtcars, columns = c(1:6,9,10), groupColumn = 2, splineFactor = 10, alphaLines=0.6)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"grouping-by-features","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.3.2 Grouping by features","text":"Discrete data categorical colors, much like legend. However discrete data fit well rest data (see scaling ). difficult discern trends variables.\nLuckily option exclude discrete data column vector (still using group data).","code":"\nmtcars$cyl <- as.character(mtcars$cyl)\nggparcoord(mtcars, columns = c(1:6,9,10), groupColumn = 2, splineFactor = 10, alphaLines=0.6)\nggparcoord(mtcars, columns = c(1,3:6,9,10), groupColumn = 2, splineFactor = 10, alphaLines=0.6)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"distributions","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.3.3 Distributions","text":"Boxplots may used observe distribution data identify outliers. shadeBox may used color box min max value behind every column, boxplot achieve goal (detail visual appeal ).Note: splineFactor parameter may used boxplot shadeBox","code":"\nggparcoord(mtcars, \n           columns = c(1,3:6,9,10), \n           groupColumn = 2, \n           alphaLines=0.7,\n           boxplot = TRUE)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"ggplot2-commands","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.3.4 ggplot2 commands","text":"title parameter may used alongside themes make graph visually appealing. includes powerful methods facet_wrap. way can observe distribution trend category. x-axis labels plots “squished” together (especially datasets many features), angle text may adjusted avoid overlap.","code":"\nggparcoord(mtcars, \n           columns = c(1,3:6,9,10), \n           groupColumn = 2, \n           alphaLines=0.7,\n           boxplot = TRUE\n           ) +\n  xlab(\"Car Features\") +\n  ylab(\"Count\") +\n  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1)) +\n  facet_wrap(~cyl, nrow = 2)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"packagemethod-ggparallel","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.4 Package/Method: ggparallel","text":"ways introduce discrete data ggparcoords. different library much flexibility discrete values. Let’s try ggparallel\nCompared ggparallel, ggparcoord tell us much information regarding discrete variables. main difference. Let’s shift attention back ggparallel","code":"\nggparallel(vars = list(\"cyl\", \"gear\", \"carb\"), data = mtcars)\nggparcoord(mtcars, columns = c(2,8,9))"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"methods","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.4.1 Methods","text":"columns added, columns may get “squished” together starts looking busy. can adjust width parameter change method get better view;\n- angle default method\n- adj.angle particularly good dealing crowded data.\n- Hammock just cluttered angle takes less space visually good fewer columns data\n- parset similar angle, straight rather curved wedges. May look visually appealing depending dataset","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"parameters-1","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.4.2 Parameters","text":"-method: changes view data (angle, adj.angle, parset, hammock)\n-alpha: transparency wedges\n-width: visual width column\n-ratio: changes height wedges (conjunction hammock adj.angle)\n-order: changes order categories column (based size proportion)\nparameters:\n-order: changes order categories column based size proportion (decreasing, increasing, unchanged order)\n-label.size: changes size text column\n-text.angle: adjusts angle text within columns","code":"\nggparallel(vars = list(\"cyl\", \"gear\", \"carb\", \"vs\", \"am\"), data = mtcars) + \n  ggtitle(\"angle\")\nggparallel(vars = list(\"cyl\", \"gear\", \"carb\", \"vs\", \"am\"), data = mtcars, width = 0.15,\n           method = \"adj.angle\") +\n  ggtitle(\"adj.angle\")\nggparallel(vars = list(\"cyl\", \"gear\", \"carb\", \"vs\", \"am\"), data = mtcars, width = 0.15,\n           method = \"hammock\", ratio = 0.3, alpha = 0.3) +\n  ggtitle(\"hammock\")\nggparallel(vars = list(\"cyl\", \"gear\", \"carb\", \"vs\", \"am\"), data = mtcars,\n           method = \"parset\") +\n  ggtitle(\"parset\")\nggparallel(vars = names(mtcars)[c(2,10,11,8,9)], data = mtcars, width = 0.15, order=0,\n           method = \"adj.angle\", label.size=3.5, text.angle=0)\nggparallel(vars = names(mtcars)[c(2,10,11,8,9)], data = mtcars, width = 0.15,\n           method = \"adj.angle\", label.size=3.5, text.angle=0) +\n  ggtitle(\"Car Parts\") +\n  xlab(\"Car Feature\") +\n  ylab(\"Amount\") +\n  theme_bw() +\n  facet_wrap(~carb, nrow=3)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"package-and-method-parcoords","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.5 Package and Method: parcoords","text":"Unlike previous two packages, parcoords allows interactive parallel coordinate plots. Default graph .","code":"\nparcoords(mtcars)"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"interactive-parameters","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.5.1 Interactive parameters","text":"-reorderable (T F): allows columns moved around\n-brushMode: allows filtering column dataset (options include 1D-axes, 1D-axes-multi, 2D-strums). main difference 1D axes filters columns, 2D strums filters columns\n-alphaOnBrushed: brushing, makes filtered-lines visible (useful looking lines brushing)\n-brushPredicate: logical “” “” multiple brush filters (default “”)","code":"\nlibrary(d3r)\nparcoords(mtcars,\n          brushMode = \"1D-axes\",\n          alpha=0.5,\n          reorderable = T,\n          withD3 = TRUE,\n          color = list(\n            colorScale = \"scaleOrdinal\",\n            colorBy = \"cyl\",\n            colorScheme = c(\"blue\",\"green\")\n            )\n          )\nparcoords(mtcars,\n          reorderable = T,\n          brushMode = \"2D-strums\",\n          alphaOnBrushed = 0.15,\n          brushPredicate = \"or\"\n          )"},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"color-parameter","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.5.2 Color parameter","text":"parameter takes list (can contain colorScale, colorBy, colorScheme, etc). Since data include continuous variables, ensure install import library “d3r”, graph get error\n- set withD3 = TRUE using color parameter","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"ending-notes-and-analysis","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.6 Ending Notes and Analysis:","text":"","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"ggparcoord-ggally","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.6.1 ggparcoord (GGally)","text":"can display continuous discrete data, though best continuouscan compare distributions featuresutilizes ggplot2 commandshas flexibility 3 methods","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"ggparallel","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.6.2 ggparallel","text":"visualizes proportions discrete datamany visualization options seen wedgesutilizes ggplot2 commandsnot flexible ggparcoord","code":""},{"path":"parallel-coordinates-plot-cheatsheet.html","id":"parcoords","chapter":"20 Parallel coordinates plot cheatsheet","heading":"20.6.3 parcoords","text":"contains interactive functionalitybest observing relationships continuous discrete featuresleast flexible; use ggplot2 commands","code":""},{"path":"r-data-science-workflow-cheatsheet.html","id":"r-data-science-workflow-cheatsheet","chapter":"21 R Data Science Workflow CheatSheet","heading":"21 R Data Science Workflow CheatSheet","text":"Kailande Cassamajor Victoria EdwardsWe created cheat sheet data science students like . \ndecided idea felt simple reference page \nexplains workflow typical project handy. first\npage cheat sheet shows flow typical data science\nproject. step workflow includes libraries used \nbrief description library . access details\nspecific library (common/useful functions resource\nlinks), click library name. Creating cheat sheet gave us better\nunderstanding workflow data science project r \nclarified purpose libraries (\nusing class). Another iteration project include\naesthetic improvements.Please access workflow cheatsheet :https://github.com/rootsnpoetry/r_datasci_workflow_cheatsheet/blob/main/r_datascience_workflow_cheatsheet.pdf","code":""},{"path":"a-linear-regression-bestiary.html","id":"a-linear-regression-bestiary","chapter":"22 A linear regression Bestiary","heading":"22 A linear regression Bestiary","text":"Wael BoukhobzaThis document shows enhancements can make regressions let cover complex cases, just linear, simple (almost non-existent reality) example ax + b good approximate target variable y.file available :\nhttps://github.com/WaelBKZ/cc21fall1/blob/linear_regression_bestiary/resources/linear_regression_bestiary/linear_regression_bestiary.pdf","code":""},{"path":"graphql-cheat-sheet.html","id":"graphql-cheat-sheet","chapter":"23 GraphQL Cheat Sheet","heading":"23 GraphQL Cheat Sheet","text":"Hongou Liu","code":""},{"path":"graphql-cheat-sheet.html","id":"introduction","chapter":"23 GraphQL Cheat Sheet","heading":"23.1 Introduction","text":"CC work use neo4j visualize graph data. uploaded video tutorial canvas. github link contains cheat sheet GraphQL Graph database query language like SQL enables play graph data generate flexible visualizations neo4j.","code":""},{"path":"graphql-cheat-sheet.html","id":"github-link","chapter":"23 GraphQL Cheat Sheet","heading":"23.2 Github Link:","text":"https://github.com/oliverliuoo/EDAcc-Neo4j-tutorial-GraphQL-cheatsheet","code":""},{"path":"questions-answers-for-r-technical-interview.html","id":"questions-answers-for-r-technical-interview","chapter":"24 30 questions & answers for r technical interview","heading":"24 30 questions & answers for r technical interview","text":"Anna DaiThis 30 Questions & Answers Help Prepare R Technical InterviewMotivations\nstudents graduating soon ’s time job hunting interview\nprepping! coming 30 relevant technical R interview questions answers\nhelp students ’s preparing interview. Currently, many great interview\nprep materials Python online, many useful materials R interviews. R\ninterview questions web technical (e.g., ’s R? ’s favorite R\nfunctions? etc.). wanted make technical, concise ready--use \ncan solve problems using things learned EDAV class. divided questions &\nanswers three sections:\n1. (Data Manipulation)\n2. (Data Visualization & Plots)\n3. (Statistical Tests R)Data Manipulations\nQ1: difference sapply lapply? use one versus \n?\nA1: Use lapply want output list, sapply want output \nvector dataframe.\nQ2: subset observations based value?\nA2: Use Filter(). allows subset observations based values. first\nargument name data frame. second subsequent arguments \nexpressions filter data frame. example, can select flights January 1st :\nfilter(flights, month == 1, day == 1)\nQ3: determine value missing?\nA3: Use .na() return true false.\nQ4: difference arrange filter?\nA4: arrange() works similarly filter() except instead selecting rows, changes \norder. takes data frame set column names (complicated expressions) \norder . provide one column name, additional column used \nbreak ties values preceding columns:\nQ5: subset dataset using operations based names variable?\nA5: select() allows rapidly zoom useful subset using operations based \nnames variables.\nQ6: add new columns functions existing columns?\nA6: mutate() always adds new columns end dataset ’ll start creating \nnarrower dataset can see new variables.\nQ7: collapse dataframe single row?\nA7: Use summaries(). summarise() useful pair group_by(). changes\nunit analysis complete dataset individual groups.\nQ8: combine multiple operations?\nA8: Use pipe %>%\nQ9: remove missing value prior computation?\nA9: Use na.rm aggregation functions na.rm argument removes missing\nvalues prior computation.\nQ10: return operations ungrouped data?\nA10: need remove grouping, return operations ungrouped data, use\nungroup()\nQ11: use pivot longer pivot wider?\nA11: pivot_longer() “lengthens” data, increasing number rows decreasing \nnumber columns. inverse transformation pivot_wider()\nQ12: create factors R?\nA12: can create factor using function factor(). Levels factor inferred \ndata provided. Similarly, levels factor can checked using levels() function.\nexample:\nx <- factor(c(“single”, “married”, “married”, “single”));Visualizations & Plots\nQ13: use boxplot?\nA13: Boxplots used display continuous variables. particularly useful \nidentifying outliers comparing different groups. use boxplot base R, can simply\nuse boxplot() function.\nQ14: use violin plot?\nA14: Violin plots used display continuous variables .\nQ15: QQ plot?\nA15: statistics, Q-Q (quantile-quantile) plot probability plot, graphical method\ncomparing two probability distributions plotting quantiles . \npoint (x, y) plot corresponds one quantiles second distribution (ycoordinate)\nplotted quantile first distribution (x-coordinate). Thus \nline parametric curve parameter number interval \nquantile.\nQ16: difference barchart cleveland plot? advantages \ndisadvantages?\nA16: Bar Charts best categorical data. Often collection factors \nwant split different groups. Cleveland dot plots great alternative simple bar\nchart, particularly items. doesn’t take much bar chart \nlook cluttered. amount space, many values can included dot plot,\n’s easier read well. R built-base function, dotchart()\nQ17: want understand correlations variables use?\nA17: Scatterplots great exploring relationships variables. Basically, \ninterested variables relate , scatterplot great place start.\nQ18: Create Interpret Pairs Plots R?\nA18: pairs plot matrix scatterplots lets understand pairwise relationship\ndifferent variables dataset. ’s easy create pairs plot R using pairs ()\nfunction.\nQ19: Mosaic plot? use ?\nA19: mosaic plot special type stacked bar chart shows percentages data \ngroups. works well multivariate categorical variables. plot graphical\nrepresentation contingency table. Mosaic plots used show relationships \nprovide visual comparison groups.\nQ20: want compare different parameters also seeing relative distributions,\nuse?\nA20: Heat maps like combination scatterplots histograms: allow \ncompare different parameters also seeing relative distributions.\nQ21: ’s Alluvial diagrams use create one?\nA21: Parallel coordinates plot one tools visualizing multivariate data. Every\nobservation dataset represented polyline crosses set parallel axes\ncorresponding variables dataset. can create plots R using function\nparcoord package MASS.\nQ22: R visualization packages?\nA22: Plotly, ggplot2, tidyquant, geofacet, googleVis, Shiny.Statistical Tests R\nQ23: t-test() R?\nA23: t-test() function used determine mean two groups equal\n.\nQ24: Shapiro Test used?\nA24: Sahpiro test used test sample follows normal distribution. basic syntax R\nShapiro test :\nshapiro.test(numericVector)\nQ25: Fisher’s F-Test?\nA25: Fisher’s F test can used check two samples variance. example:\nvar.test(x, y)\nQ26: interpret P-value?\nA26: P values determine whether hypothesis test results statistically significant. P\nvalue probability obtaining effect least extreme one sample\ndata, assuming truth null hypothesis. example, suppose cancer treatment\nstudy produced P value 0.04. P value indicates cancer treatment \neffect, ’d obtain observed difference 4% studies due random sampling\nerror. P values address one question: likely data, assuming true null\nhypothesis? measure support alternative hypothesis.\nQ27: Mann-Whitney-Wilcoxon Test?\nA27: Two data samples independent come distinct populations \nsamples affect . Using Mann-Whitney-Wilcoxon Test, can decide\nwhether population distributions identical without assuming follow normal\ndistribution.\nQ28: compute Compute one-way ANOVA test R?\nA28: R function aov() can used answer question. function summary.aov() \nused summarize analysis variance model.\nexample:\nres.aov <- aov(weight ~ group, data = my_data)\nsummary(res.aov)\nQ29: Perform Friedman Test R?\nA29: Friedman Test non-parametric alternative Repeated Measures ANOVA. \nused determine whether statistically significant difference \nmeans three groups subjects show group.\nperform Friedman Test R, can use friedman.test() function, uses\nfollowing syntax:\nfriedman.test(y, groups, blocks).\nQ30: Chi-Square test R?\nA30: Chi-Square Test used analyze frequency table (.e., contingency table), \nformed two categorical variables. chi-square test evaluates whether \nsignificant relationship categories two variables. example, can build\ndata set observations people’s candy buying pattern try correlate gender\nperson flavor candy prefer. correlation found, can plan \nappropriate stock flavors knowing number gender people visiting. basic\nsyntax creating Chi-Square :\nchisq.test(data)Citations\nhttps://www.rdocumentation.org/packages/tidyverse/versions/1.3.1\nhttps://www.rdocumentation.org/packages/ggplot2/versions/3.3.5\nhttps://plotly.com/r/\nhttps://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test\nhttps://r-pkgs.org/tests.html\nhttps://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/chisq.test\nhttps://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test","code":""},{"path":"multithreading-crawler-frame.html","id":"multithreading-crawler-frame","chapter":"25 Multithreading Crawler Frame","heading":"25 Multithreading Crawler Frame","text":"Yi YangAll work uploaded Github, ’s url:https://github.com/yiyangnju/multithreading-crawler-frame","code":""},{"path":"regression-and-classification-in-r.html","id":"regression-and-classification-in-r","chapter":"26 Regression and Classification in R","heading":"26 Regression and Classification in R","text":"Parv JoshiThis video tutorial, can found https://youtu./J2rnDy9PB3E. code created part video given contents file, reference. links used data set:Data.csvData.csvTitanic.csvTitanic.csvAmes_Housing_data.csvAmes_Housing_data.csvBoxcox Implementation R - 1Boxcox Implementation R - 1Boxcox Implementation R - 2Boxcox Implementation R - 2Peanalized RegressionPeanalized RegressionStepwise Selection MethodStepwise Selection MethodAccuracy MetricsAccuracy MetricsRmd CheatsheetRmd Cheatsheet","code":""},{"path":"regression-and-classification-in-r.html","id":"libraries-and-warnings","chapter":"26 Regression and Classification in R","heading":"26.0.1 Libraries and Warnings","text":"","code":"\n# Removing messages and warnings from knited version\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\n\n# Libraries\n# Make sure these are installed before running them. They all are a part of CRAN.\n\nlibrary(RCurl)\nlibrary(tidyverse)\nlibrary(randomForest)\nlibrary(caTools)\nlibrary(car)\nlibrary(MASS)\nlibrary(leaps)\nlibrary(caret)\nlibrary(bestglm)\nlibrary(rpart)\nlibrary(rattle)"},{"path":"regression-and-classification-in-r.html","id":"reading-data","chapter":"26 Regression and Classification in R","heading":"26.0.2 Reading Data","text":"","code":"\n# Importing the dataset\n\ndataset = read.csv(\"https://raw.githubusercontent.com/Parv-Joshi/EDAV_CC_Datasets/main/Data.csv\")\n\n# str(dataset)\n# View(dataset)"},{"path":"regression-and-classification-in-r.html","id":"data-preprocessing","chapter":"26 Regression and Classification in R","heading":"26.0.3 Data Preprocessing","text":"","code":"\n# Mean Imputation for Missing Data\ndataset$Age = ifelse(is.na(dataset$Age),\n                     ave(dataset$Age, FUN = function(x) mean(x, na.rm = T)),\n                     dataset$Age)\n\ndataset$Salary = ifelse(is.na(dataset$Salary),\n                        ave(dataset$Salary, FUN = function(x) mean(x, na.rm = T)),\n                        dataset$Salary)\n\n# Encoding Categorical Variables\ndataset$Country = factor(dataset$Country, \n                         labels = c(\"France\", \"Spain\", \"Germany\"), \n                         levels = c(\"France\", \"Spain\", \"Germany\"))\ndataset$Purchased = factor(dataset$Purchased, \n                           levels = c(\"Yes\", \"No\"), \n                           labels = c(1, 0))\n\n# Splitting Data into Training and Testing\n\nset.seed(123)\n\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.8)\ntraining_set = subset(dataset, split == T)\ntest_set = subset(dataset, split == F)\n\n# Feature Scaling\ntraining_set[, 2:3] = scale(training_set[, 2:3])\ntest_set[, 2:3] = scale(test_set[, 2:3])"},{"path":"regression-and-classification-in-r.html","id":"regression","chapter":"26 Regression and Classification in R","heading":"26.0.4 Regression","text":"","code":"\n# Data \n\ndata(\"Salaries\", package = \"carData\")\n# force(Salaries)\n\nattach(Salaries)\ndetach(Salaries)\n\n# str(Salaries)\n# View(Salaries)\n\n# Simple Variable Regression\n\nmodel = lm(Salaries$salary ~ Salaries$yrs.since.phd)\nmodel = lm(salary ~ yrs.since.phd, data = Salaries)\n\nmodel## \n## Call:\n## lm(formula = salary ~ yrs.since.phd, data = Salaries)\n## \n## Coefficients:\n##   (Intercept)  yrs.since.phd  \n##       91718.7          985.3\nsummary(model)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -84171 -19432  -2858  16086 102383 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    91718.7     2765.8  33.162   <2e-16 ***\n## yrs.since.phd    985.3      107.4   9.177   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27530 on 395 degrees of freedom\n## Multiple R-squared:  0.1758, Adjusted R-squared:  0.1737 \n## F-statistic: 84.23 on 1 and 395 DF,  p-value: < 2.2e-16\nstargazer::stargazer(model, type = \"text\")## \n## ===============================================\n##                         Dependent variable:    \n##                     ---------------------------\n##                               salary           \n## -----------------------------------------------\n## yrs.since.phd               985.342***         \n##                              (107.365)         \n##                                                \n## Constant                   91,718.680***       \n##                             (2,765.792)        \n##                                                \n## -----------------------------------------------\n## Observations                    397            \n## R2                             0.176           \n## Adjusted R2                    0.174           \n## Residual Std. Error    27,533.580 (df = 395)   \n## F Statistic           84.226*** (df = 1; 395)  \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01\n# Multiple Variable Regression\n\nmodel1 = lm(salary ~ yrs.since.phd + yrs.service, data = Salaries)\nsummary(model1)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79735 -19823  -2617  15149 106149 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    89912.2     2843.6  31.620  < 2e-16 ***\n## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***\n## yrs.service     -629.1      254.5  -2.472   0.0138 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27360 on 394 degrees of freedom\n## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 \n## F-statistic: 45.71 on 2 and 394 DF,  p-value: < 2.2e-16\n### Model:\n### salary = 89912.2 + 1562.9 * yrs.since.phd + (-629.1) * yrs.service\n\n\n# Categorical Variables\n\ncontrasts(Salaries$sex)##        Male\n## Female    0\n## Male      1\n# sex = relevel(sex, ref = \"Male\")\n\nmodel2 = lm(salary ~ yrs.since.phd + yrs.service + sex, data = Salaries)\nsummary(model2)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + yrs.service + sex, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79586 -19564  -3018  15071 105898 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    82875.9     4800.6  17.264  < 2e-16 ***\n## yrs.since.phd   1552.8      256.1   6.062 3.15e-09 ***\n## yrs.service     -649.8      254.0  -2.558   0.0109 *  \n## sexMale         8457.1     4656.1   1.816   0.0701 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27280 on 393 degrees of freedom\n## Multiple R-squared:  0.1951, Adjusted R-squared:  0.189 \n## F-statistic: 31.75 on 3 and 393 DF,  p-value: < 2.2e-16\ncar::Anova(model2)## Anova Table (Type II tests)\n## \n## Response: salary\n##                   Sum Sq  Df F value   Pr(>F)    \n## yrs.since.phd 2.7346e+10   1 36.7512 3.15e-09 ***\n## yrs.service   4.8697e+09   1  6.5447  0.01089 *  \n## sex           2.4547e+09   1  3.2990  0.07008 .  \n## Residuals     2.9242e+11 393                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel3 = lm(salary ~ ., data = Salaries)\ncar::Anova(model3)## Anova Table (Type II tests)\n## \n## Response: salary\n##                   Sum Sq  Df F value    Pr(>F)    \n## rank          6.9508e+10   2 68.4143 < 2.2e-16 ***\n## discipline    1.9237e+10   1 37.8695 1.878e-09 ***\n## yrs.since.phd 2.5041e+09   1  4.9293   0.02698 *  \n## yrs.service   2.7100e+09   1  5.3348   0.02143 *  \n## sex           7.8068e+08   1  1.5368   0.21584    \n## Residuals     1.9812e+11 390                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(model3)## \n## Call:\n## lm(formula = salary ~ ., data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -65248 -13211  -1775  10384  99592 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    65955.2     4588.6  14.374  < 2e-16 ***\n## rankAssocProf  12907.6     4145.3   3.114  0.00198 ** \n## rankProf       45066.0     4237.5  10.635  < 2e-16 ***\n## disciplineB    14417.6     2342.9   6.154 1.88e-09 ***\n## yrs.since.phd    535.1      241.0   2.220  0.02698 *  \n## yrs.service     -489.5      211.9  -2.310  0.02143 *  \n## sexMale         4783.5     3858.7   1.240  0.21584    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 22540 on 390 degrees of freedom\n## Multiple R-squared:  0.4547, Adjusted R-squared:  0.4463 \n## F-statistic:  54.2 on 6 and 390 DF,  p-value: < 2.2e-16\n# Transformations and Interaction Terms\n\nmodel4 = lm(salary ~ yrs.since.phd^2 + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd^2 + yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79735 -19823  -2617  15149 106149 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    89912.2     2843.6  31.620  < 2e-16 ***\n## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***\n## yrs.service     -629.1      254.5  -2.472   0.0138 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27360 on 394 degrees of freedom\n## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 \n## F-statistic: 45.71 on 2 and 394 DF,  p-value: < 2.2e-16\nmodel4 = lm(salary ~ yrs.since.phd + I(yrs.since.phd^2) + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + I(yrs.since.phd^2) + yrs.service, \n##     data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -63538 -18063  -1946  14919 105025 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        64971.002   3950.746  16.445  < 2e-16 ***\n## yrs.since.phd       4222.493    394.237  10.711  < 2e-16 ***\n## I(yrs.since.phd^2)   -62.321      7.389  -8.434 6.42e-16 ***\n## yrs.service         -234.596    239.075  -0.981    0.327    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 25210 on 393 degrees of freedom\n## Multiple R-squared:  0.3127, Adjusted R-squared:  0.3075 \n## F-statistic: 59.61 on 3 and 393 DF,  p-value: < 2.2e-16\nmodel4 = lm(salary ~ yrs.since.phd + I(yrs.since.phd^2) + I(yrs.since.phd^3) + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + I(yrs.since.phd^2) + I(yrs.since.phd^3) + \n##     yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -63538 -18062  -1947  14917 105023 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         6.498e+04  5.559e+03  11.688  < 2e-16 ***\n## yrs.since.phd       4.221e+03  8.990e+02   4.696 3.68e-06 ***\n## I(yrs.since.phd^2) -6.227e+01  3.877e+01  -1.606    0.109    \n## I(yrs.since.phd^3) -6.720e-04  4.935e-01  -0.001    0.999    \n## yrs.service        -2.346e+02  2.395e+02  -0.979    0.328    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 25240 on 392 degrees of freedom\n## Multiple R-squared:  0.3127, Adjusted R-squared:  0.3057 \n## F-statistic:  44.6 on 4 and 392 DF,  p-value: < 2.2e-16\nmodel4 = lm(I(log(salary)) ~ yrs.since.phd + I(yrs.since.phd^2) + I(yrs.since.phd^3) + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = I(log(salary)) ~ yrs.since.phd + I(yrs.since.phd^2) + \n##     I(yrs.since.phd^3) + yrs.service, data = Salaries)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.68247 -0.15590 -0.00244  0.14242  0.74830 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         1.115e+01  4.617e-02 241.467  < 2e-16 ***\n## yrs.since.phd       4.073e-02  7.467e-03   5.454 8.72e-08 ***\n## I(yrs.since.phd^2) -6.626e-04  3.220e-04  -2.058   0.0403 *  \n## I(yrs.since.phd^3)  6.168e-07  4.099e-06   0.150   0.8805    \n## yrs.service        -1.433e-03  1.989e-03  -0.720   0.4718    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2096 on 392 degrees of freedom\n## Multiple R-squared:  0.3575, Adjusted R-squared:  0.3509 \n## F-statistic: 54.52 on 4 and 392 DF,  p-value: < 2.2e-16\nmodel5 = lm(salary ~ yrs.since.phd:yrs.service, data = Salaries)\nsummary(model5)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd:yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -80936 -21633  -3841  17621 106895 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)               1.071e+05  1.983e+03  53.982  < 2e-16 ***\n## yrs.since.phd:yrs.service 1.218e+01  2.431e+00   5.009 8.26e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29410 on 395 degrees of freedom\n## Multiple R-squared:  0.05973,    Adjusted R-squared:  0.05735 \n## F-statistic: 25.09 on 1 and 395 DF,  p-value: 8.263e-07\n#### Boxcox\n\nsal = Salaries[, c(3,4,6)]\nshapiro.test(Salaries$salary)## \n##  Shapiro-Wilk normality test\n## \n## data:  Salaries$salary\n## W = 0.95988, p-value = 6.076e-09\n# Null: Data is normally distributed\n# p-value = 6.076e-09 < 0.05, reject null -> NOT Normal.\n\nmodel1 = lm(salary ~ yrs.since.phd + yrs.service, data = Salaries)\nsummary(model1)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79735 -19823  -2617  15149 106149 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    89912.2     2843.6  31.620  < 2e-16 ***\n## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***\n## yrs.service     -629.1      254.5  -2.472   0.0138 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27360 on 394 degrees of freedom\n## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 \n## F-statistic: 45.71 on 2 and 394 DF,  p-value: < 2.2e-16\nbc = boxcox(model1)\nbest.lam = bc$x[which(bc$y == max(bc$y))]\nbest.lam## [1] -0.2222222\nmodel6 = lm(I(salary^best.lam) ~ yrs.since.phd + yrs.service, data = Salaries)\nsummary(model6)## \n## Call:\n## lm(formula = I(salary^best.lam) ~ yrs.since.phd + yrs.service, \n##     data = Salaries)\n## \n## Residuals:\n##        Min         1Q     Median         3Q        Max \n## -0.0099656 -0.0027195 -0.0000644  0.0028614  0.0150252 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    7.942e-02  4.095e-04 193.953  < 2e-16 ***\n## yrs.since.phd -2.260e-04  3.698e-05  -6.110 2.39e-09 ***\n## yrs.service    8.892e-05  3.665e-05   2.426   0.0157 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.00394 on 394 degrees of freedom\n## Multiple R-squared:  0.1929, Adjusted R-squared:  0.1888 \n## F-statistic: 47.09 on 2 and 394 DF,  p-value: < 2.2e-16\n### Adj. R^2 increased\n\n# Predictions using Training and Testing data\n\nset.seed(123)\nsplit = sample.split(Salaries$salary, SplitRatio = 0.8)\ntraining_set = subset(Salaries, split == T)\ntest_set = subset(Salaries, split == F)\n\nmodel7 = lm(salary ~ ., data = training_set)\ny_pred = predict(model7, test_set)\n# y_pred\ndata.frame(y_pred, test_set$salary)##        y_pred test_set.salary\n## 2   133991.08          173200\n## 4   138905.04          115000\n## 5   134650.37          141500\n## 8   135930.61          147765\n## 11  100457.55          119800\n## 16  135214.59          117150\n## 20  116504.49          137000\n## 21  121107.75           89565\n## 24  120009.45          113068\n## 31  139939.95          132261\n## 32   89375.54           79916\n## 34   87417.62           80225\n## 50   85955.45           70768\n## 53   86623.38           74692\n## 59   98656.53          100135\n## 65   86921.88           68404\n## 67  137279.32          101000\n## 68  136344.57           99418\n## 69  126670.46          111512\n## 84   88722.90           88825\n## 87  134675.41          152708\n## 88   86112.35           88400\n## 89  132792.62          172272\n## 104 130115.59          127512\n## 106 120116.27          113543\n## 107  84699.94           82099\n## 111 118886.12          112429\n## 114 119570.45          104279\n## 118 121371.46          117515\n## 126 124716.43           78162\n## 132 122055.79           76840\n## 137 117267.04          108262\n## 139  84543.04           73877\n## 145 133106.42          112696\n## 151 132058.21          128148\n## 173 141120.02           93164\n## 179 139551.03          147349\n## 181 130596.04          142467\n## 189 100984.98          106300\n## 190 135767.06          153750\n## 193 132346.97          122100\n## 195 101644.27           90000\n## 202 135146.11          119700\n## 206 141584.07           96545\n## 219  97391.59          109650\n## 220 131901.31          119500\n## 222 138923.43          145200\n## 230 120379.98          133900\n## 238  67420.64           63100\n## 240 123190.87           96200\n## 248 118547.28          101100\n## 249 120637.05          128800\n## 260 119777.43           92550\n## 261  91885.61           88600\n## 262 120825.64          107550\n## 264 118629.05          126000\n## 271 121346.42          143250\n## 277 123906.89          107200\n## 294  88170.11          104800\n## 296 122024.10           97150\n## 297 116589.36          126300\n## 300  91521.73           70700\n## 316  88227.16           84716\n## 317  95094.83           71065\n## 320 131876.27          135027\n## 321 133131.46          104428\n## 327 136444.74          124714\n## 330 132478.83          134778\n## 334 140988.16          145098\n## 340 145581.67          137317\n## 347 142243.36          142023\n## 352 134832.31           93519\n## 356 134775.58          145028\n## 360  75889.64           78785\n## 363 118472.15          138771\n## 373 118126.66          109707\n## 376 119149.83          103649\n## 380  84699.94          104121\n## 386 119093.10          114330\n## 391 130451.66          166605\n# Variable Selection\n\n# data\ndata(\"swiss\")\nattach(swiss)\n\n# ?swiss\n\n# Best Subsets regression\n\nmodels = leaps::regsubsets(Fertility ~ ., data = swiss, nvmax = 5)\nsummary(models)## Subset selection object\n## Call: regsubsets.formula(Fertility ~ ., data = swiss, nvmax = 5)\n## 5 Variables  (and intercept)\n##                  Forced in Forced out\n## Agriculture          FALSE      FALSE\n## Examination          FALSE      FALSE\n## Education            FALSE      FALSE\n## Catholic             FALSE      FALSE\n## Infant.Mortality     FALSE      FALSE\n## 1 subsets of each size up to 5\n## Selection Algorithm: exhaustive\n##          Agriculture Examination Education Catholic Infant.Mortality\n## 1  ( 1 ) \" \"         \" \"         \"*\"       \" \"      \" \"             \n## 2  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \" \"             \n## 3  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \"*\"             \n## 4  ( 1 ) \"*\"         \" \"         \"*\"       \"*\"      \"*\"             \n## 5  ( 1 ) \"*\"         \"*\"         \"*\"       \"*\"      \"*\"\n### Therefore, \n### Best 1-variable model: Fertility ~ Education\n### Best 2-variables model: Fertility ~ Education + Catholic\n### Best 3-variables model: Fertility ~ Education + Catholic + Infant.Mortality\n### Best 4-variables model: Fertility ~ Agriculture + Education + Catholic + Infant.Mortality\n### Best 5-variables model: Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality\n\nmodels.summary = summary(models)\ndata.frame(Adj.R2 = which.max(models.summary$adjr2),\n           CP = which.min(models.summary$cp),\n           BIC = which.min(models.summary$bic))##   Adj.R2 CP BIC\n## 1      5  4   4\n### Fertility ~ Agriculture + Education + Catholic + Infant.Mortality\n\n# Stepwise Variable Selection\nfit = lm(Fertility ~ ., data = swiss)\nstep = MASS::stepAIC(fit, direction = \"both\", trace = F) # change both to forward and backward\nstep## \n## Call:\n## lm(formula = Fertility ~ Agriculture + Education + Catholic + \n##     Infant.Mortality, data = swiss)\n## \n## Coefficients:\n##      (Intercept)       Agriculture         Education          Catholic  \n##          62.1013           -0.1546           -0.9803            0.1247  \n## Infant.Mortality  \n##           1.0784\ndetach(swiss)\n\n\n# Penalized Regression\n\names = read.csv(\"https://raw.githubusercontent.com/Parv-Joshi/EDAV_CC_Datasets/main/Ames_Housing_Data.csv\")\n# str(ames)\nanyNA(ames)## [1] FALSE\nset.seed(123)\ntraining.samples = createDataPartition(ames$SalePrice, p = 0.75, list = FALSE)\n\ntrain.data = ames[training.samples,]\ntest.data = ames[-training.samples,]\n\nlambda = 10^seq(-3, 3, length = 100)\n\n# Ridge Regression\nset.seed(123)\nridge = train(SalePrice ~ ., data = train.data, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneGrid = expand.grid(alpha = 0, lambda = lambda))\n\n# LASSO\nset.seed(123)\nlasso = train(SalePrice ~ ., data = train.data, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneGrid = expand.grid(alpha = 1, lambda = lambda))\n\n# Elastic Net\nset.seed(123)\nelastic = train(SalePrice ~ ., data = train.data, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneLength = 10)\n\n# Comparison\nmodels = list(ridge = ridge, lasso = lasso, elastic = elastic)\nresamples(models) %>% summary(metric = \"RMSE\")## \n## Call:\n## summary.resamples(object = ., metric = \"RMSE\")\n## \n## Models: ridge, lasso, elastic \n## Number of resamples: 10 \n## \n## RMSE \n##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n## ridge   24044.91 29603.57 30799.80 32919.29 37210.80 45150.33    0\n## lasso   23614.27 29604.97 31077.67 32800.93 37787.06 44271.58    0\n## elastic 23725.92 29688.82 31124.32 32770.19 37752.31 44115.51    0\n# Since Elastic model has the lowest mean RMSE, we can conclude that the Elastic model is the best."},{"path":"regression-and-classification-in-r.html","id":"classification","chapter":"26 Regression and Classification in R","heading":"26.0.5 Classification","text":"","code":"\n# Data\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\n# str(PimaIndiansDiabetes2)\n# View(PimaIndiansDiabetes2)\n\nPimaIndiansDiabetes2$diabetes = as.factor(PimaIndiansDiabetes2$diabetes)\nPimaIndiansDiabetes2 = na.omit(PimaIndiansDiabetes2)\n\nattach(PimaIndiansDiabetes2)\n\n# Training and Testing\n\nset.seed(123)\n\ntraining.samples = createDataPartition(diabetes, p = 0.8, list = FALSE)\n\ntrain.data = PimaIndiansDiabetes2[training.samples,]\ntest.data = PimaIndiansDiabetes2[-training.samples,]\n\n# Logistic Regression\n\nmodel = glm(diabetes ~ ., data = train.data, family = binomial)\nsummary(model)## \n## Call:\n## glm(formula = diabetes ~ ., family = binomial, data = train.data)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.5832  -0.6544  -0.3292   0.6248   2.5968  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.053e+01  1.440e+00  -7.317 2.54e-13 ***\n## pregnant     1.005e-01  6.127e-02   1.640  0.10092    \n## glucose      3.710e-02  6.486e-03   5.719 1.07e-08 ***\n## pressure    -3.876e-04  1.383e-02  -0.028  0.97764    \n## triceps      1.418e-02  1.998e-02   0.710  0.47800    \n## insulin      5.940e-04  1.508e-03   0.394  0.69371    \n## mass         7.997e-02  3.180e-02   2.515  0.01190 *  \n## pedigree     1.329e+00  4.823e-01   2.756  0.00585 ** \n## age          2.718e-02  2.020e-02   1.346  0.17840    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 398.80  on 313  degrees of freedom\n## Residual deviance: 267.18  on 305  degrees of freedom\n## AIC: 285.18\n## \n## Number of Fisher Scoring iterations: 5\nprobabilities = predict(model, test.data, type = \"response\")\nprobabilities##          19          21          32          55          64          71 \n## 0.192628377 0.485262263 0.662527248 0.798681474 0.278073391 0.145877334 \n##          72          74          98          99         108         111 \n## 0.314265178 0.232071188 0.007697533 0.066394837 0.357546947 0.552586956 \n##         115         128         154         182         215         216 \n## 0.715548197 0.132717063 0.706670106 0.189297618 0.291196167 0.911862874 \n##         224         229         260         293         297         313 \n## 0.704569592 0.993419157 0.932506403 0.721153294 0.328274489 0.293808296 \n##         316         326         357         369         376         385 \n## 0.120862955 0.201559764 0.390156419 0.022075579 0.885924184 0.075765425 \n##         386         393         394         410         429         447 \n## 0.042383376 0.102435789 0.095664525 0.885380718 0.379195274 0.053098974 \n##         450         453         468         470         477         487 \n## 0.091641699 0.097155564 0.122327231 0.831420989 0.216021458 0.525840641 \n##         541         542         546         552         555         556 \n## 0.461122260 0.270914264 0.890122464 0.066719675 0.068520682 0.197336318 \n##         562         563         564         577         589         592 \n## 0.894087110 0.075000107 0.091244654 0.163897301 0.912857186 0.200938223 \n##         595         600         609         610         621         634 \n## 0.491041316 0.048192839 0.549602575 0.034910473 0.203922043 0.081878938 \n##         666         673         674         681         683         694 \n## 0.133609108 0.100033198 0.782544310 0.007547670 0.145787456 0.629221735 \n##         697         699         710         716         717         719 \n## 0.485455842 0.290737653 0.141965217 0.925604098 0.839268863 0.161190610 \n##         722         731         733         734         746         766 \n## 0.168129887 0.191170873 0.852375783 0.078840151 0.305248512 0.125461309\ncontrasts(diabetes)##     pos\n## neg   0\n## pos   1\npredicted.classes = ifelse(probabilities > 0.5, \"pos\", \"neg\")\npredicted.classes##    19    21    32    55    64    71    72    74    98    99   108   111   115 \n## \"neg\" \"neg\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"pos\" \n##   128   154   182   215   216   224   229   260   293   297   313   316   326 \n## \"neg\" \"pos\" \"neg\" \"neg\" \"pos\" \"pos\" \"pos\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \n##   357   369   376   385   386   393   394   410   429   447   450   453   468 \n## \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \n##   470   477   487   541   542   546   552   555   556   562   563   564   577 \n## \"pos\" \"neg\" \"pos\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \n##   589   592   595   600   609   610   621   634   666   673   674   681   683 \n## \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \n##   694   697   699   710   716   717   719   722   731   733   734   746   766 \n## \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\"\ncaret::confusionMatrix(factor(predicted.classes),\n                factor(test.data$diabetes),\n                positive = \"pos\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction neg pos\n##        neg  44  11\n##        pos   8  15\n##                                          \n##                Accuracy : 0.7564         \n##                  95% CI : (0.646, 0.8465)\n##     No Information Rate : 0.6667         \n##     P-Value [Acc > NIR] : 0.05651        \n##                                          \n##                   Kappa : 0.4356         \n##                                          \n##  Mcnemar's Test P-Value : 0.64636        \n##                                          \n##             Sensitivity : 0.5769         \n##             Specificity : 0.8462         \n##          Pos Pred Value : 0.6522         \n##          Neg Pred Value : 0.8000         \n##              Prevalence : 0.3333         \n##          Detection Rate : 0.1923         \n##    Detection Prevalence : 0.2949         \n##       Balanced Accuracy : 0.7115         \n##                                          \n##        'Positive' Class : pos            \n## \n# Stepwise regression\n\nstep = MASS::stepAIC(model, direction = \"both\", k = log(nrow(PimaIndiansDiabetes2)), trace = FALSE)\nstep$anova## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## diabetes ~ pregnant + glucose + pressure + triceps + insulin + \n##     mass + pedigree + age\n## \n## Final Model:\n## diabetes ~ pregnant + glucose + mass + pedigree\n## \n## \n##         Step Df     Deviance Resid. Df Resid. Dev      AIC\n## 1                                  305   267.1825 320.9239\n## 2 - pressure  1 0.0007857024       306   267.1833 314.9534\n## 3  - insulin  1 0.1591672501       307   267.3425 309.1413\n## 4  - triceps  1 0.4434205054       308   267.7859 303.6135\n## 5      - age  1 2.4276790188       309   270.2136 300.0699\n# Best subset regression\n\ncv_data = model.matrix( ~ ., PimaIndiansDiabetes2)[,-1]\ncv_data = data.frame(cv_data)\nbest = bestglm(cv_data, IC = \"BIC\", family = binomial)\nbest## BIC\n## BICq equivalent for q in (0.359009418385306, 0.859446547266463)\n## Best Model:\n##                 Estimate  Std. Error   z value     Pr(>|z|)\n## (Intercept) -10.09201799 1.080251137 -9.342289 9.427384e-21\n## glucose       0.03618899 0.004981946  7.264026 3.757357e-13\n## mass          0.07444854 0.020266697  3.673442 2.393046e-04\n## pedigree      1.08712862 0.419408437  2.592052 9.540525e-03\n## age           0.05301206 0.013439480  3.944502 7.996582e-05\ndetach(PimaIndiansDiabetes2)\n\n# Decision Tree Classification\n\ndata = read.csv(\"https://raw.githubusercontent.com/Parv-Joshi/EDAV_CC_Datasets/main/Titanic.csv\")\nattach(data)\n\n# str(data)\n\n# Excluding Variables\ndata = subset(data, select = -c(Name, Ticket, Cabin))\n\n# Removing Missing Data\ndata = subset(data, !is.na(Age))\n\n# Testing and Training set\n\nset.seed(123)\ntraining.samples = data$Survived %>% \n  createDataPartition(p = 0.8, list = FALSE)\n\ntrain.data = data[training.samples,]\ntest.data = data[-training.samples,]\n\n# Factoring Survived\ntrain.data$Survived = as.factor(train.data$Survived)\ntest.data$Survived = as.factor(test.data$Survived)\n\n# Decision Trees\nmodel = rpart::rpart(Survived ~ ., data = train.data, control = rpart.control(cp = 0))\nrattle::fancyRpartPlot(model, cex = 0.5)\nset.seed(123)\ntrain.data$Survived = as.factor(train.data$Survived)\nmodel2 = train(Survived ~ ., \n               data = train.data, \n               method = \"rpart\", \n               trControl = trainControl(\"cv\", number = 10), \n               tuneLength = 100)\n\nfancyRpartPlot(model2$finalModel, cex = 0.6)\nprobabilities = predict(model2, newdata = test.data)\n# we don't need to do  contrasts since Survived is already given in o and 1.\npredicted.classes = ifelse(probabilities == 1, \"1\", \"0\")\n\ncaret::confusionMatrix(factor(predicted.classes),\n                factor(test.data$Survived),\n                positive = \"1\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction  0  1\n##          0 74 19\n##          1  5 44\n##                                          \n##                Accuracy : 0.831          \n##                  95% CI : (0.759, 0.8886)\n##     No Information Rate : 0.5563         \n##     P-Value [Acc > NIR] : 3.722e-12      \n##                                          \n##                   Kappa : 0.6497         \n##                                          \n##  Mcnemar's Test P-Value : 0.007963       \n##                                          \n##             Sensitivity : 0.6984         \n##             Specificity : 0.9367         \n##          Pos Pred Value : 0.8980         \n##          Neg Pred Value : 0.7957         \n##              Prevalence : 0.4437         \n##          Detection Rate : 0.3099         \n##    Detection Prevalence : 0.3451         \n##       Balanced Accuracy : 0.8176         \n##                                          \n##        'Positive' Class : 1              \n## \n# Random Forest\n\nset.seed(123)\nmodel3 = train(Survived ~ ., \n              data = train.data, \n              method = \"rf\",\n              trControl = trainControl(\"cv\", number = 10),\n              importance = TRUE)\n\nprobabilities = predict(model3, newdata = test.data)\npredicted.classes = ifelse(probabilities == 1, \"1\", \"0\")\ncaret::confusionMatrix(factor(predicted.classes),\n                factor(test.data$Survived),\n                positive = \"1\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction  0  1\n##          0 76 21\n##          1  3 42\n##                                          \n##                Accuracy : 0.831          \n##                  95% CI : (0.759, 0.8886)\n##     No Information Rate : 0.5563         \n##     P-Value [Acc > NIR] : 3.722e-12      \n##                                          \n##                   Kappa : 0.6474         \n##                                          \n##  Mcnemar's Test P-Value : 0.0005202      \n##                                          \n##             Sensitivity : 0.6667         \n##             Specificity : 0.9620         \n##          Pos Pred Value : 0.9333         \n##          Neg Pred Value : 0.7835         \n##              Prevalence : 0.4437         \n##          Detection Rate : 0.2958         \n##    Detection Prevalence : 0.3169         \n##       Balanced Accuracy : 0.8143         \n##                                          \n##        'Positive' Class : 1              \n## \nrandomForest::varImpPlot(model3$finalModel, type = 1) # MeanDecreaseAccuracy\ncaret::varImp(model3, type = 1)## rf variable importance\n## \n##             Overall\n## Sexmale     100.000\n## Pclass       54.019\n## Fare         38.444\n## Age          37.316\n## SibSp        24.658\n## Parch        19.187\n## EmbarkedQ     4.655\n## EmbarkedS     3.630\n## EmbarkedC     3.560\n## PassengerId   0.000\nrandomForest::varImpPlot(model3$finalModel, type = 2) # MeanDecreaseGini\ncaret::varImp(model3, type = 2)## rf variable importance\n## \n##             Overall\n## Sexmale     100.000\n## Fare         62.753\n## Age          54.613\n## PassengerId  48.686\n## Pclass       35.447\n## SibSp        15.503\n## Parch        14.097\n## EmbarkedS     3.623\n## EmbarkedC     3.606\n## EmbarkedQ     0.000\ndetach(data)"},{"path":"tibble-vs.-dataframe.html","id":"tibble-vs.-dataframe","chapter":"27 Tibble vs. DataFrame","heading":"27 Tibble vs. DataFrame","text":"Jingfei Fang","code":"\nlibrary(tidyverse)\nlibrary(tibble)"},{"path":"tibble-vs.-dataframe.html","id":"introduction-1","chapter":"27 Tibble vs. DataFrame","heading":"27.0.1 Introduction","text":"tibble often considered neater format data frame, often used tidyverse ggplot2 packages. contains information data frame, manipulation representation tibbles different data frames aspects.","code":""},{"path":"tibble-vs.-dataframe.html","id":"getting-started-with-tibbles","chapter":"27 Tibble vs. DataFrame","heading":"27.0.2 1. Getting started with tibbles","text":"can tidyverse:can installing tibble package directly:","code":"\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n#install.packages(\"tibble\")\nlibrary(tibble)"},{"path":"tibble-vs.-dataframe.html","id":"creating-a-tibble","chapter":"27 Tibble vs. DataFrame","heading":"27.0.3 2. Creating a tibble","text":"can create tibble directly:can create tibble existing data frame using as_tibble(). use ‘iris’ dataset example:","code":"\ntib <- tibble(a = c(1,2,3), b = c(4,5,6), c = c(7,8,9))\ntib## # A tibble: 3 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     4     7\n## 2     2     5     8\n## 3     3     6     9\ndf <- iris\nclass(df)## [1] \"data.frame\"\ntib <- as_tibble(df)\ntib## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # … with 140 more rows"},{"path":"tibble-vs.-dataframe.html","id":"unlike-data-frames-tibbles-dont-show-the-entire-dataset-when-you-print-it.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.4 3. Unlike data frames, tibbles don’t show the entire dataset when you print it.","text":"","code":"\ntib## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # … with 140 more rows"},{"path":"tibble-vs.-dataframe.html","id":"tibbles-cannot-access-a-column-when-you-provide-a-partial-name-of-the-column-but-data-frames-can.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.5 4. Tibbles cannot access a column when you provide a partial name of the column, but data frames can.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble","chapter":"27 Tibble vs. DataFrame","heading":"27.0.5.1 Tibble","text":"try match column name partial name, work.provide entire column name, work.","code":"\ntib <- tibble(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\ntib$st## NULL\ntib$str## [1] \"a\" \"b\" \"c\" \"d\""},{"path":"tibble-vs.-dataframe.html","id":"data-frame","chapter":"27 Tibble vs. DataFrame","heading":"27.0.5.2 Data Frame","text":"However, can access “str” column providing partial column name “st” (long partial name unique).","code":"\ndf <- data.frame(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\ndf$st## [1] \"a\" \"b\" \"c\" \"d\""},{"path":"tibble-vs.-dataframe.html","id":"when-you-access-only-one-column-of-a-tibble-it-will-keep-the-tibble-structure.-but-when-you-access-one-column-of-a-data-frame-it-will-become-a-vector.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.6 5. When you access only one column of a tibble, it will keep the tibble structure. But when you access one column of a data frame, it will become a vector.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-1","chapter":"27 Tibble vs. DataFrame","heading":"27.0.6.1 Tibble","text":"Checking ’s still tibble:can see tibble structure preserved.","code":"\ntib[,\"str\"]## # A tibble: 4 × 1\n##   str  \n##   <chr>\n## 1 a    \n## 2 b    \n## 3 c    \n## 4 d\nis_tibble(tib[,\"str\"])## [1] TRUE"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-1","chapter":"27 Tibble vs. DataFrame","heading":"27.0.6.2 Data Frame","text":"Checking ’s still data frame:’s longer data frame.","code":"\ndf[,\"str\"]## [1] \"a\" \"b\" \"c\" \"d\"\nis.data.frame(df[,\"str\"])## [1] FALSE"},{"path":"tibble-vs.-dataframe.html","id":"however-other-forms-of-subsetting-including-and-work-the-same-for-tibbles-and-data-frames.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.6.3 However, other forms of subsetting, including [[ ]] and $, work the same for tibbles and data frames.","text":"can see subsetting [[ ]] $ also don’t preserve tibble structure.","code":"\ntib[[\"str\"]]## [1] \"a\" \"b\" \"c\" \"d\"\ndf[[\"str\"]]## [1] \"a\" \"b\" \"c\" \"d\"\ntib$str## [1] \"a\" \"b\" \"c\" \"d\"\ndf$str## [1] \"a\" \"b\" \"c\" \"d\""},{"path":"tibble-vs.-dataframe.html","id":"when-assigning-a-new-column-to-a-tibble-the-input-will-not-be-recycled-which-means-you-have-to-provide-an-input-of-the-same-length-of-the-other-columns.-but-a-data-frame-will-recycle-the-input.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.7 6. When assigning a new column to a tibble, the input will not be recycled, which means you have to provide an input of the same length of the other columns. But a data frame will recycle the input.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-2","chapter":"27 Tibble vs. DataFrame","heading":"27.0.7.1 Tibble","text":"gives error tibble columns length 4, input (5,6) length 2 recycled.\nprovide input length:","code":"\ntib## # A tibble: 4 × 2\n##   str     int\n##   <chr> <dbl>\n## 1 a         1\n## 2 b         2\n## 3 c         3\n## 4 d         4\ntib$newcol <- c(5,6)## Error: Assigned data `c(5, 6)` must be compatible with existing data.\n## ✖ Existing data has 4 rows.\n## ✖ Assigned data has 2 rows.\n## ℹ Only vectors of size 1 are recycled.\ntib$newcol <- rep(c(5,6),2)\ntib## # A tibble: 4 × 3\n##   str     int newcol\n##   <chr> <dbl>  <dbl>\n## 1 a         1      5\n## 2 b         2      6\n## 3 c         3      5\n## 4 d         4      6"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-2","chapter":"27 Tibble vs. DataFrame","heading":"27.0.7.2 Data Frame","text":"Data frames recycle input.","code":"\ndf##   str int\n## 1   a   1\n## 2   b   2\n## 3   c   3\n## 4   d   4\ndf$newcol <- c(5,6)\ndf##   str int newcol\n## 1   a   1      5\n## 2   b   2      6\n## 3   c   3      5\n## 4   d   4      6"},{"path":"tibble-vs.-dataframe.html","id":"reading-with-builtin-read.csv-function-will-output-data-frames-while-reading-with-read_csv-in-readr-package-inside-tidyverse-will-output-tibbles.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.8 7. Reading with builtin read.csv() function will output data frames, while reading with read_csv() in “readr” package inside tidyverse will output tibbles.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"reading-csv-file-with-read.csv","chapter":"27 Tibble vs. DataFrame","heading":"27.0.8.1 Reading csv file with read.csv()","text":"","code":"\ndata <- read.csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")\nclass(data)## [1] \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"reading-csv-file-with-read_csv","chapter":"27 Tibble vs. DataFrame","heading":"27.0.8.2 Reading csv file with read_csv()","text":"","code":"\ndata <- read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")\nclass(data)## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"tibbles-dont-support-support-arithmetic-operations-on-all-columns-well-the-result-will-be-converted-into-a-data-frame-without-any-notice.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.9 8. Tibbles don’t support support arithmetic operations on all columns well, the result will be converted into a data frame without any notice.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-3","chapter":"27 Tibble vs. DataFrame","heading":"27.0.9.1 Tibble","text":"can see try multiply elements tibble 2, result correct turned data frame without notifications.","code":"\ntib <- tibble(a = c(1,2,3), b = c(4,5,6), c = c(7,8,9))\nclass(tib*2)## [1] \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"data-frame-3","chapter":"27 Tibble vs. DataFrame","heading":"27.0.9.2 Data Frame","text":"data frames issue , converted type.","code":"\ndf <- data.frame(a = c(1,2,3), b = c(4,5,6), c = c(7,8,9))\nclass(df*2)## [1] \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"tibbles-preserve-all-the-variable-types-while-data-frames-have-the-option-to-convert-string-into-factor.-in-older-versions-of-r-data-frames-will-convert-string-into-factor-by-default","chapter":"27 Tibble vs. DataFrame","heading":"27.0.10 9. Tibbles preserve all the variable types, while data frames have the option to convert string into factor. (In older versions of R, data frames will convert string into factor by default)","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-4","chapter":"27 Tibble vs. DataFrame","heading":"27.0.10.1 Tibble","text":"can see original data types variables preserved tibble.","code":"\ntib <- tibble(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\nstr(tib)## tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ str: chr [1:4] \"a\" \"b\" \"c\" \"d\"\n##  $ int: num [1:4] 1 2 3 4"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-4","chapter":"27 Tibble vs. DataFrame","heading":"27.0.10.2 Data Frame","text":"use data frame, also preserve original types, “stringAsFactors = FALSE” default new versions R.However, also option convert string factor creating data frame setting “stringAsFactors = TRUE”.can see “str” column converted factor.","code":"\ndf <- data.frame(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\nstr(df)## 'data.frame':    4 obs. of  2 variables:\n##  $ str: chr  \"a\" \"b\" \"c\" \"d\"\n##  $ int: num  1 2 3 4\ndf <- data.frame(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4), stringsAsFactors = TRUE)\nclass(df$str)## [1] \"factor\""},{"path":"tibble-vs.-dataframe.html","id":"tibbles-work-well-with-ggplot2-just-like-data-frames.","chapter":"27 Tibble vs. DataFrame","heading":"27.0.11 10. Tibbles work well with ggplot2, just like data frames.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-5","chapter":"27 Tibble vs. DataFrame","heading":"27.0.11.1 Tibble:","text":"","code":"\nggplot(data = tib, mapping = aes(x=str, y=int)) +\n  geom_col(width = 0.3)"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-5","chapter":"27 Tibble vs. DataFrame","heading":"27.0.11.2 Data Frame:","text":"","code":"\nggplot(data = df, mapping = aes(x=str, y=int)) +\n  geom_col(width = 0.3)"},{"path":"tibble-vs.-dataframe.html","id":"works-cited","chapter":"27 Tibble vs. DataFrame","heading":"27.1 Works Cited","text":"https://tibble.tidyverse.org/https://cran.r-project.org/web/packages/tibble/vignettes/tibble.htmlhttps://www.youtube.com/watch?v=_qHdqWx-vsQ&ab_channel=JoshuaFrench","code":""},{"path":"introduction-to-time-series.html","id":"introduction-to-time-series","chapter":"28 Introduction to Time Series","heading":"28 Introduction to Time Series","text":"Parth Gupta (pg2677)writing tutorial time series forecasting covering classical techniques time series forecasting.","code":""},{"path":"introduction-to-time-series.html","id":"motivation","chapter":"28 Introduction to Time Series","heading":"28.1 Motivation,","text":"Modeling temporal processes always one important problems. wide variety applications, ranging stock price prediction weather forecasting forecasting web traffic website.three types temporal data,Time SeriesTime SeriesPanelPanelCross SectionalCross SectionalIn time series data one individual observed different time steps. Panel data multiple individuals observed different time steps. Cross sectional data different individuals observed time step.Time series analysis comprises methods analyzing time series data order extract meaningful statistics characteristics data. Time series forecasting use model predict future values based previously observed values. Time series forecasting useful lot real life applications.Visualizing time series data important able get much better understanding underlying patterns data. Like trends increase decrease price item time, seasonal changes like people watch movies weekends. far prediction actual truth.using birth per month dataset. tune parameters different models based training period Jan 1946 Dec 1951. forecast next year. generally use Mean Absolute Percentage Error Root Mean Squared Error evaluate model.$$MAPE = _{= 0}^{N}|( - y_i)|/ y_i\\RMSE = (_{= 0}^{N}( - y_i)^2/ N)^{1/2}$$Every time series three components, trends, seasonality residualsTime Series = Trend + Seasonality + ResidualsTrend, refers overall general direction data, obtained ignoring short term effects seasonal variations noise.Trend, refers overall general direction data, obtained ignoring short term effects seasonal variations noise.Seasonality, refers periodic fluctuations repeated throughout time series period.Seasonality, refers periodic fluctuations repeated throughout time series period.Residuals, refers left part trend seasonality.Residuals, refers left part trend seasonality.plots trying visualize seasonality aspect time series data. plotted data month whole training period.","code":"\ndf <-births <- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")\ndata <- df[1:72]\nfuture <- df[73:84]\nprint(data)##  [1] 26.663 23.598 26.931 24.740 25.806 24.364 24.477 23.901 23.175 23.227\n## [11] 21.672 21.870 21.439 21.089 23.709 21.669 21.752 20.761 23.479 23.824\n## [21] 23.105 23.110 21.759 22.073 21.937 20.035 23.590 21.672 22.222 22.123\n## [31] 23.950 23.504 22.238 23.142 21.059 21.573 21.548 20.000 22.424 20.615\n## [41] 21.761 22.874 24.104 23.748 23.262 22.907 21.519 22.025 22.604 20.894\n## [51] 24.677 23.673 25.320 23.583 24.671 24.454 24.122 24.252 22.084 22.991\n## [61] 23.287 23.049 25.076 24.037 24.430 24.667 26.451 25.618 25.014 25.110\n## [71] 22.964 23.981\ntime_series <- ts(data, start=c(1946, 1), end=c(1951, 12), frequency=12)\nplot(time_series)\nfit <- stl(time_series, s.window=\"period\")\nplot(fit)\nmonthplot(time_series)\nseasonplot(time_series)"},{"path":"introduction-to-time-series.html","id":"stationary-time-series","chapter":"28 Introduction to Time Series","heading":"28.2 Stationary Time Series","text":"Stationarity one important concepts time series forecasting. stationary time series one whose properties depend time series observed. Thus, time series trends, seasonality, stationary — trend seasonality affect value time series different times. hand, white noise series stationary — matter observe , look much point time.Differencing process computing differences consecutive observations.apply log transformations stabilise variance time series. Differencing can also help stabilising mean time series removing changes level time series, therefore eliminating (reducing) trend seasonality. also tests check stationarity time series.","code":""},{"path":"introduction-to-time-series.html","id":"holtwinters-model","chapter":"28 Introduction to Time Series","heading":"28.3 HoltWinters model,","text":"model three parameters alpha, beta gamma.alpha: refers “base value”. Higher alpha puts weight recent observations.alpha: refers “base value”. Higher alpha puts weight recent observations.beta: corresponds “trend value”. Higher beta means trend slope dependent recent trend slopes.beta: corresponds “trend value”. Higher beta means trend slope dependent recent trend slopes.gamma: weighs “seasonal component”. Higher gamma puts weighting recent seasonal cycles.gamma: weighs “seasonal component”. Higher gamma puts weighting recent seasonal cycles.mathematical formulation follows,$$\n{y}{t+h|t} = l_t + hb_t + s{t+h-m(k+1)} \\l_t = (y_t - s_{t-m}) + (1- )*(l_{t-1} + b_{t-1})\\b_t = ^* (l_t - l_{t-1}) + (1-^*)b_{t-1}\\s_t = (y_t - l_{t-1} - b_{t-1}) + (1-)s_{t-m}\n$$Let’s start simplest model, model forecast future value last value , .e.y[t+] = y[t], >=1 t last time step training period.$$_{T+} = y_T \\\n>=1\n$$Now can set beta non zero value can account trends, model forecast future value last value + trend.Finally, set three parameters, alpha, beta gamma non zero values account trend seasonality .Naturally, model performs better previous two models.","code":"\n# simple exponential - models level\nfit_1 <- HoltWinters(time_series, beta=FALSE, gamma=FALSE)\nf1 = forecast(fit_1, 12)\nplot(forecast(fit_1, 12))\n# double exponential - models level and trend\nfit_2 <- HoltWinters(time_series, gamma=FALSE)\nf2 = forecast(fit_2, 12)\nplot(forecast(fit_2, 12))\n# triple exponential - models level, trend, and seasonal components\nfit_3 <- HoltWinters(time_series)\nf3 = forecast(fit_3, 12)\nplot(forecast(fit_3, 12))\n# Automated forecasting using an exponential model\nfit_4 <- ets(time_series)\nf4 = forecast(fit_4, 12)\nplot(forecast(fit_4, 12))"},{"path":"introduction-to-time-series.html","id":"exponential-smooting","chapter":"28 Introduction to Time Series","heading":"28.4 Exponential Smooting","text":"model, assign weights past observations decrease exponentially go back time. model ensures give weights recent observations less less weights older observations$$\n{T+1|T} = y_T + (1-)* y{t-1} + (1-)^2 *y_{t-2} + …..\\{T+1|T} = {j = 0}{T-1}(1-)jy_{T-j} + (1-)^Tl_0\n$$","code":""},{"path":"introduction-to-time-series.html","id":"arima-model","chapter":"28 Introduction to Time Series","heading":"28.5 ARIMA model,","text":"ARIMA stands Auto Regressive Integrated Moving Average. generalization simpler AutoRegressive Moving Average adds notion integration. Let’s look term individually.AR: Autoregression. refers model predicts future values using past observations.: Integrated. use concept differencing raw observations (e.g. subtracting observation observation previous time step) order make time series stationary.MA: Moving Average. refers model uses dependency observation residual error moving average model applied lagged observations.need three parameters define model. use notation ARIMA(p,d,q).parameters ARIMA model defined follows:p: number lag observations included model, also called lag order.\nd: number times raw observations differenced, also called degree differencing.\nq: size moving average window, also called order moving average.seasonal effect , generally considered better use SARIMA (seasonal ARIMA) model increase order AR MA parts model.Now, difficult choose values p, q can use Autocorrelation function (ACF), Partial autocorrelation function (PACF) plots series determine order AR / MA terms.autocorrelation function (ACF) technique can use identify correlated values time series . ACF plots correlation coefficient lag. lag corresponds certain point time observe first value time series.Partial autocorrelation statistical measure captures correlation two variables controlling effects variables.Now, can use another implementation ARIMA R .e auto.arima() function uses variation Hyndman-Khandakar algorithm (Hyndman & Khandakar, 2008). combines unit root tests, minimisation AICc MLE obtain ARIMA model., can clearly see MAPE RMSE bar plots Auto ARIMA ETS performed best followed Holt Winter\nmodel non zero alpha, beta gamma.References,Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles practice, 2nd edition, OTexts: Melbourne,\nAustralia.OTexts.com/fpp2.Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles practice, 2nd edition, OTexts: Melbourne,\nAustralia.OTexts.com/fpp2.https://en.wikipedia.org/wiki/Time_serieshttps://en.wikipedia.org/wiki/Time_serieshttps://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/","code":"\n#using an ARIMA model\nfit_5 <- Arima(time_series, order = c(10, 1, 1))\nf5 = forecast(fit_5, 12)\nplot(forecast(fit_5, 12))\n#using an ARIMA model\nacf(time_series)\npacf(time_series)\n# Forecasting using an ARIMA model\nfit_6 <- auto.arima(time_series)\nf6 = forecast(fit_6, 12)\nplot(forecast(fit_6, 12))\nrmse_1 = accuracy(forecast(fit_1))[2]\nrmse_2 = accuracy(forecast(fit_2))[2]\nrmse_3 = accuracy(forecast(fit_3))[2]\nrmse_4 = accuracy(forecast(fit_4))[2]\nrmse_5 = accuracy(forecast(fit_5))[2]\nrmse_6 = accuracy(forecast(fit_6))[2]\nrmse = c(rmse_1, rmse_2, rmse_3, rmse_4, rmse_5, rmse_6)\n\nbarplot(rmse, main=\"RMSE Plot\", xlab=\"Models\", names.arg = c(\"HW_1\", \"HW_2\", \"HW_3\", \"ETS\", \"ARIMA\", \"Auto ARIMA\"))\nMAPE_1 = accuracy(forecast(fit_1))[5]\nMAPE_2 = accuracy(forecast(fit_2))[5]\nMAPE_3 = accuracy(forecast(fit_3))[5]\nMAPE_4 = accuracy(forecast(fit_4))[5]\nMAPE_5 = accuracy(forecast(fit_5))[5]\nMAPE_6 = accuracy(forecast(fit_6))[5]\nMAPE = c(MAPE_1, MAPE_2, MAPE_3, MAPE_4, MAPE_5, MAPE_6)\n\nbarplot(MAPE, main=\"MAPE Plot\", xlab=\"Models\", names.arg = c(\"HW_1\", \"HW_2\", \"HW_3\", \"ETS\", \"ARIMA\", \"Auto ARIMA\"))"},{"path":"vipul-edav-community-contribution-kickstart-r.html","id":"vipul-edav-community-contribution-kickstart-r","chapter":"29 Vipul EDAV Community Contribution Kickstart R","heading":"29 Vipul EDAV Community Contribution Kickstart R","text":"Vipul H HariharThis hand written notes specially made understanding basics Exploratory Data Analysis R. material summation Professor Joyce’s teachings classroom lectures R documentation files.\ntried write neatly clearly reader might benefit notes easily, hope easily understandable legible.\ntopics involved notes follows:\n1) Introduction\n2) R Programming EDAV\n3) Fun Facts R\n4) Features R makes perfect “EDAV”\n5) Application R Programming\n6) Understanding Basic Data Types R\n7a) Handy Functions\n7b) Vector\n7c) List\n7d) Factors - IMPORTANT\n7e) Data Frame\n7f) Missing Values\n8) Functions R\n9) Data Science Process (Visual Description)\n10) visulizations use EDAV?Please find link submitted file:\nhttps://github.com/virslaan/EDAV_Community_Contribution/blob/main/Community%20Contribution%202.pdf","code":""},{"path":"caret---a-machine-learning-package-turotial.html","id":"caret---a-machine-learning-package-turotial","chapter":"30 caret - a machine learning package turotial","heading":"30 caret - a machine learning package turotial","text":"Jiang Zhu Xuechun BaiMotivation: motivation community contribution project introduce classmates package R can easily implement machine learning algorithms lines code. python users commonly use package scikit-learn deploy machine laerning models, strived find similar R package similar functions. finding resources online, discovered caret great package neatly defined functions create easy-use interface machine learning algorithms, time, abundant machine learning models preprocessing model selection functions. However, found great tutorial easy--read API’s caret. Therefore, markdown file, illustrate use caret package example-based manner. eliminate mathematics behind model much possible focus implementation algorithm R. also demonstrate results experiments caret model datasets visualization tools.","code":"\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(kernlab)\nlibrary(earth)\nlibrary(RANN)"},{"path":"caret---a-machine-learning-package-turotial.html","id":"introduction-and-description-of-dataset","chapter":"30 caret - a machine learning package turotial","heading":"30.1 Introduction and description of dataset","text":"tutorial, use data frame Orange Juice includes two orange juice brands, Citrus Hill (CH) Minute Maid (MM) selling information according brand. URL dataset : https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv, contains 18 cloumns 1070 rows.first column “Purchase” introduces brand orange juice. data contains weekly amount purchase two brands (WeekofPurchase) store id (StoreID) orange juice sold, price orange juice according brand CH (PriceCH) MM (PriceMM). data also includes discount (“DiscCH” “DiscMM”), sale price(“SalePriceCH” “SalePriceMM”) useful information two orange juice brands. goal tutorial predict customers’ preference two brands choosing buy orange juice, focusing “Purchase” now.Now let us firstly import data, state briefly summary data:","code":"\n# Import dataset\ndf <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')\n\n# Structure of the dataframe\nhead(df)##   Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n## 1       CH            237       1    1.75    1.99   0.00    0.0         0\n## 2       CH            239       1    1.75    1.99   0.00    0.3         0\n## 3       CH            245       1    1.86    2.09   0.17    0.0         0\n## 4       MM            227       1    1.69    1.69   0.00    0.0         0\n## 5       CH            228       7    1.69    1.69   0.00    0.0         0\n## 6       CH            230       7    1.69    1.99   0.00    0.0         0\n##   SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM\n## 1         0 0.500000        1.99        1.75      0.24     No  0.000000\n## 2         1 0.600000        1.69        1.75     -0.06     No  0.150754\n## 3         0 0.680000        2.09        1.69      0.40     No  0.000000\n## 4         0 0.400000        1.69        1.69      0.00     No  0.000000\n## 5         0 0.956535        1.69        1.69      0.00    Yes  0.000000\n## 6         1 0.965228        1.99        1.69      0.30    Yes  0.000000\n##   PctDiscCH ListPriceDiff STORE\n## 1  0.000000          0.24     1\n## 2  0.000000          0.24     1\n## 3  0.091398          0.23     1\n## 4  0.000000          0.00     1\n## 5  0.000000          0.00     0\n## 6  0.000000          0.30     0"},{"path":"caret---a-machine-learning-package-turotial.html","id":"pre-processing","chapter":"30 caret - a machine learning package turotial","heading":"30.2 Pre-processing","text":"importing dataset, need separate data training testing sets. training set used examples machine learning models learn generalize, test set used evaluate preformance models. can achieve using createDataPartition() caret package.y specifies column partition datay specifies column partition datatimes specifies many times partition nthe datatimes specifies many times partition nthe datap specifies proportion split datap specifies proportion split datalist boolean indicating whether return listlist boolean indicating whether return listHere set 70% training set 30% testing set.next step clear refill missing values NAs dataset. several ways trasnformation fill missing value mean, mode simply delete row missing values. Using caret package, can apply NAs practical values, predict missing values values listed dataset. way caret package can use preProcess() function make k-Nearest Neighbors use training set.data specifies dataframe want apply preprocessingdata specifies dataframe want apply preprocessingmethod string specifies method preprocessingmethod string specifies method preprocessingThen get Nearest model perdicting missing values. , use predict() fill missing values prediction.result, can see prediction model centered 16 variables, ignored 2 created 825 samples 18 variables. importing NAs “predict” function, check missing values remaining dataset using angNA() function. result “FALSE” shows missing values replaced.Another work caret package can transform categorical variables one-hot vectors numerical representation categorical variables. one-hot vector constructed follows: suppose \\(n\\) classes categorical variables. represent class \\(\\) \\(n\\)-dimensional vector \\(o_i\\) \\(o_i[]=1\\) \\(o_i[j]=0\\) \\(j\\neq \\)can acheive one-hot transformation using dummyVars()formula repersents way inpute dataformula repersents way inpute datadata represents dataframedata represents dataframeAs can see, Store7.Store7.Yes represent one-hot encoding. actually apply dataset caret supports factor labels.","code":"\ncreateDataPartition(\n  y,\n  times,\n  p,\n  list\n)\n#set random seed to produce the same result\nset.seed(1)\n\n#use createDataPartition() function to get row of training. The input variable here is \"Purchase\" in df, and p = .7 implies the percentage of dataset we want to take. Here we set the training with 70%, so we let p equals to 0.7. Using List = F we are able to prevent the result as being a list instead of a dataframe.\nrowOfTrain <- createDataPartition(df$Purchase, p=0.7, list=FALSE)\n\n# create datasets for training and testing\ntrain_df <- df[rowOfTrain,]\ntest_df <- df[-rowOfTrain,]\npreProcess(\n  data,\n  method\n)\n# Create k-Nearest with training data using method = 'knnImpute'\nknn_inputer <- preProcess(train_df, method='knnImpute')\nknn_inputer## Created from 727 samples and 18 variables\n## \n## Pre-processing:\n##   - centered (16)\n##   - ignored (2)\n##   - 5 nearest neighbor imputation (16)\n##   - scaled (16)\n# Import the prediction into missing values using function predict()\ntrain_df <- predict(knn_inputer, newdata = train_df)\n\n#Check the NAs in the training dataset\nanyNA(train_df)## [1] FALSE\ndummyVars(\n  formula,\n  data\n)\n# define the one-hot encoding object\none_hot_encoder <- dummyVars(Purchase~., train_df)\n\n# results after applying one-hot encoding\nhead(data.frame(predict(one_hot_encoder, train_df)))##   WeekofPurchase   StoreID     PriceCH     PriceMM     DiscCH     DiscMM\n## 1     -1.1406992 -1.267157 -1.16230500 -0.70167093 -0.4532457 -0.5650681\n## 2     -1.0116414 -1.267157 -1.16230500 -0.70167093 -0.4532457  0.8702789\n## 3     -0.6244679 -1.267157 -0.08909746  0.03485153  0.9895487 -0.5650681\n## 4     -1.7859884 -1.267157 -1.74769093 -2.91123832 -0.4532457 -0.5650681\n## 5     -1.7214595  1.327114 -1.74769093 -2.91123832 -0.4532457 -0.5650681\n## 6     -1.5924017  1.327114 -1.74769093 -0.70167093 -0.4532457 -0.5650681\n##    SpecialCH  SpecialMM     LoyalCH SalePriceMM SalePriceCH  PriceDiff Store7No\n## 1 -0.4101973 -0.4461947 -0.24490422  0.08791265  -0.4606397  0.3276049        1\n## 2 -0.4101973  2.2381700  0.08369962 -1.10522569  -0.4606397 -0.7838676        1\n## 3 -0.4101973 -0.4461947  0.34658269  0.48562543  -0.8807610  0.9203903        1\n## 4 -0.4101973 -0.4461947 -0.57350806 -1.10522569  -0.8807610 -0.5615731        1\n## 5 -0.4101973 -0.4461947  1.25528731 -1.10522569  -0.8807610 -0.5615731        0\n## 6 -0.4101973  2.2381700  1.28385285  0.08791265  -0.8807610  0.5498994        0\n##   Store7Yes  PctDiscMM PctDiscCH ListPriceDiff      STORE\n## 1         0 -0.5718037 -0.449862     0.2132873 -0.4419062\n## 2         0  0.9396462 -0.449862     0.2132873 -0.4419062\n## 3         0 -0.5718037  1.016257     0.1218262 -0.4419062\n## 4         0 -0.5718037 -0.449862    -1.9817791 -0.4419062\n## 5         1 -0.5718037 -0.449862    -1.9817791 -1.1401926\n## 6         1 -0.5718037 -0.449862     0.7620539 -1.1401926"},{"path":"caret---a-machine-learning-package-turotial.html","id":"model-training","chapter":"30 caret - a machine learning package turotial","heading":"30.3 Model training","text":"machine learning models defined trained caret train() function. core function masks tedious detail machine learning algorithms provide convenient concise interface. train() function versions different parameters:formula specifies way want construct modelformula specifies way want construct modeldata specifies training data want use construct labeldata specifies training data want use construct labelmethod string specifiying model want apply datamethod string specifiying model want apply dataThe first version train function recommended data single dataframe. another word, column represents feature \\(x_i\\) one column represents \\(y\\). second version train function recommended \\(x\\) \\(y\\) separate dataframes. following code, use first version train() prediction.method parameter can specified conveniently string. far 238 models supported caret can found https://topepo.github.io/caret/available-models.html. tutorial, illustrate apply several common classification algorithms dataset.One amazing feature caret almost methods, automatically finds best hyperparameters user.","code":"\n# first version of train\nmodel <- train(\n  formula,\n  data,\n  method,\n)\n\n# second version of train\nmodel <- train(\n  x,\n  y,\n  method,\n)"},{"path":"caret---a-machine-learning-package-turotial.html","id":"k-nearest-neighbors","chapter":"30 caret - a machine learning package turotial","heading":"30.3.1 K-nearest neighbors","text":"k-nearest neighbor intuitive classification algorithm. Given positive \\(k\\) data \\(x\\), k-neigherest neighbor finds k nearest neighbors training set based euclidian distance\n\\[dist(x,x')=\\sqrt{(x_1-x'_1)^2+(x_2-x'_2)^2+\\cdots + (x_n-x'_n)^2}\\]\nclassifies \\(x\\) class contains nearst neighbors.implement algorithm, specify parameter method=knn","code":"\n# training the knn model\nknn_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"knn\"\n)\n\nknn_model## k-Nearest Neighbors \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results across tuning parameters:\n## \n##   k  Accuracy   Kappa    \n##   5  0.7430519  0.4561180\n##   7  0.7506511  0.4701385\n##   9  0.7522812  0.4717872\n## \n## Accuracy was used to select the optimal model using the largest value.\n## The final value used for the model was k = 9."},{"path":"caret---a-machine-learning-package-turotial.html","id":"logistics-regression","chapter":"30 caret - a machine learning package turotial","heading":"30.3.2 Logistics Regression","text":"logistics regression linear model binary classification, another word, given features \\(x_1,x_2,\\cdots,x_n\\), want predict \\(y=\\{0,1\\}\\). model can described equation\n\\[\\hat{y}=\\sigma(w_1\\cdot x_1+w_2\\cdot x_2+\\cdots+w_n\\cdot x_n)\\]\n\\(\\sigma\\) function defined \n\\[\\sigma(x)=\\frac{1}{1+e^{-x}}\\]hope logistics regression algorithm can find optimal \\(w_1,\\cdots,w_n\\) \\(\\hat{y}\\) closed true label \\(y\\) possible.implement algorithm, specify parameter method=\"glm\":","code":"\n# training the logistics regression model\nlr_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"glm\"\n)\n\nlr_model## Generalized Linear Model \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results:\n## \n##   Accuracy   Kappa    \n##   0.8219706  0.6207552"},{"path":"caret---a-machine-learning-package-turotial.html","id":"support-vector-machine","chapter":"30 caret - a machine learning package turotial","heading":"30.3.3 Support vector machine","text":"support vector machine hyperplane separates input space two subspaces. case \\(x_1\\) \\(x_2\\), want draw line saparates cartesian plane. criteria choosing plane one maximizes distance closest data points classes. gives rise optimization objective\n\\[\\max_{w,b}\\frac{1}{\\vert\\vert w\\vert\\vert_2}\\min_{x_i\\D}\\vert w^Tx_i+b\\vert,\\quad \\text{ s.t. }\\forall ,\\quad y_i(w^Tx_i+b)\\ge 0 \\]\nimplement algorithm, specify parameter method=\"svmRadial\"","code":"\n# training the support vector machine model, the library kernlab is needed to perform kernal transformation\nsvm_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"svmRadial\"\n)\n\nsvm_model## Support Vector Machines with Radial Basis Function Kernel \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results across tuning parameters:\n## \n##   C     Accuracy   Kappa    \n##   0.25  0.8126657  0.5977679\n##   0.50  0.8143211  0.6016239\n##   1.00  0.8123949  0.5975775\n## \n## Tuning parameter 'sigma' was held constant at a value of 0.05934766\n## Accuracy was used to select the optimal model using the largest value.\n## The final values used for the model were sigma = 0.05934766 and C = 0.5."},{"path":"caret---a-machine-learning-package-turotial.html","id":"random-forest","chapter":"30 caret - a machine learning package turotial","heading":"30.3.4 Random Forest","text":"decision tree algorithm learns predict value target variable learning simple decision rules inferred data features. process constructing tree first select root node \\(x_i\\) decision boundry \\(b_i\\). recirsively select children nodes corresponding decision threshold \\(b\\) maximizes information gain. Since different initialization produces different results, random forest aims create multiple trees different initialization collectively ageraging result produced individual decision treesTo implement algorithm, specify parameter method=\"earch\"","code":"\n# training the random forest model, the library earth is needed\nrf_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"earth\"\n)\n\nrf_model## Multivariate Adaptive Regression Spline \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results across tuning parameters:\n## \n##   nprune  Accuracy   Kappa    \n##    2      0.7958702  0.5634097\n##   12      0.8095924  0.5959591\n##   23      0.8081299  0.5933340\n## \n## Tuning parameter 'degree' was held constant at a value of 1\n## Accuracy was used to select the optimal model using the largest value.\n## The final values used for the model were nprune = 12 and degree = 1."},{"path":"caret---a-machine-learning-package-turotial.html","id":"model-prediction-and-evaluation","chapter":"30 caret - a machine learning package turotial","heading":"30.4 Model prediction and evaluation","text":"successfully train models, time evaluate test set validate good model. key function achieve predict() function:evaluate models trained , can call predict() function models:can compute confution matrix model:Confusion matrix knn model:Confusion matrix logistics regression model:Confusion matrix svm model:Confusion matrix random forest model:caret package also provides convenient function resamples() compare metric models.can see random forest best performance dataset.","code":"\npredict(\n  object,\n  newdata\n)\n# inpute the test data first\ntest_df <- predict(knn_inputer, test_df)\nknn_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(knn_model, test_df), mode='everything', positive='MM')\n\nknn_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 169  30\n##         MM  26  95\nggplot(data = data.frame(knn_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for KNN model\")\nlr_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(lr_model, test_df), mode='everything', positive='MM')\n\nlr_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 164  20\n##         MM  31 105\nggplot(data = data.frame(lr_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for Logistics Regression model\")\nsvm_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(svm_model, test_df), mode='everything', positive='MM')\n\nsvm_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 170  27\n##         MM  25  98\nggplot(data = data.frame(svm_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for SVM model\")\nrf_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(svm_model, test_df), mode='everything', positive='MM')\n\nrf_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 170  27\n##         MM  25  98\nggplot(data = data.frame(rf_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for random forest model\")\nmodel_list <- list(KNN = knn_model, LR = lr_model, SVM = svm_model, RF = rf_model)\n\nmodels_compare <- resamples(model_list)\n\nsummary(models_compare)## \n## Call:\n## summary.resamples(object = models_compare)\n## \n## Models: KNN, LR, SVM, RF \n## Number of resamples: 25 \n## \n## Accuracy \n##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## KNN 0.7025090 0.7343173 0.7491166 0.7522812 0.7703180 0.8197880    0\n## LR  0.7909408 0.8118081 0.8201439 0.8219706 0.8297101 0.8763636    0\n## SVM 0.7620818 0.8013937 0.8172043 0.8143211 0.8239700 0.8525180    0\n## RF  0.7750000 0.7906137 0.8058608 0.8095924 0.8230769 0.8625954    0\n## \n## Kappa \n##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## KNN 0.3824306 0.4294194 0.4604863 0.4717872 0.5098018 0.6148838    0\n## LR  0.5476278 0.5914299 0.6196513 0.6207552 0.6368782 0.7459377    0\n## SVM 0.4965787 0.5830797 0.6013610 0.6016239 0.6178862 0.6848073    0\n## RF  0.5198789 0.5579644 0.5906583 0.5959591 0.6214686 0.7077705    0"},{"path":"caret---a-machine-learning-package-turotial.html","id":"reference-3","chapter":"30 caret - a machine learning package turotial","heading":"30.5 Reference","text":"Caret Package – Practical Guide Machine Learning R - Selva Prabhakaran:\nhttps://www.machinelearningplus.com/machine-learning/caret-package/#61howtotrainthemodelandinterprettheresults?basic tutorial caret: machine learning package R - Rebecca Barter:\nhttps://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/caret Package - Max Kuhn:\nhttps://topepo.github.io/caret/index.html","code":""},{"path":"streamlit-for-financial-analysis.html","id":"streamlit-for-financial-analysis","chapter":"31 Streamlit for Financial Analysis","heading":"31 Streamlit for Financial Analysis","text":"Smaranjit Ghose Siddhant Pravin MahurkarMotivation project:previous experiences working Data Science teams personal trading endeavors, observed huge gap literature available easily building useful dashboards analysis price financial instruments like stocks, bonds crypto. internet flooded false promising articles use Reinforcement Learning accurately predict stock prices less material actually code’s customized dashboard analyze various indicators making trade. Furthermore, rise streamlit de facto app deploying end end machine learning applications,saw opportunity bring benefits problem financial data visualization. Moreover, lot people get stuck work done JupyterNotebook RMarkdown fail take production often demand working Data Science team needs present MVP clients. Hence, also shown take dashboard production users can interact real time.Link Project can found hereThe Link Blog can found hereThe Link Repository can found ","code":""},{"path":"interactive-graphs-tutorial.html","id":"interactive-graphs-tutorial","chapter":"32 Interactive graphs tutorial","heading":"32 Interactive graphs tutorial","text":"Chuyang XiaoThe following content brief introductory tutorial create interactive graphs common packages R.","code":"\nlibrary(ggvis)\nlibrary(dplyr)\n\nlibrary(plotly)\n\nlibrary(igraph)\nlibrary(networkD3)"},{"path":"interactive-graphs-tutorial.html","id":"introduction-2","chapter":"32 Interactive graphs tutorial","heading":"32.1 Introduction","text":"rmd file incorporates hand--hand tutorial several useful tools create interactive figures via multiple popular toolboxes. Though pretty attractive well persuasive data presentation, creation interactive diagrams seldom mentioned lecture. Thus, self-learnt couple useful tools decided make detailed tutorial create interactive diagrams.Commonly, compared data selection, critical parameters determining appearances styles diagrams, spline line size dots, even difficult decide. Sometimes, may laborious alter function parameters one--one check output. Hence, interactive graph may pretty helpful condition, enables flexibly modify appearances graphs. Moreover, data presentation, interactive graph also facilitate emphasize specific characteristics well overall patterns graph.following sections, shall run code chunk one--one. description method front code chunk (please read running code) analysis output . lecture, contents fully explained far, hence, pretty confident , finishing reading tutorial, master decent number methods create interactive graphs.way, please read rmd file rather html file check graphs, since dynamic graphs become static .","code":""},{"path":"interactive-graphs-tutorial.html","id":"create-interactive-graphs-via-ggvis","chapter":"32 Interactive graphs tutorial","heading":"32.2 Create Interactive Graphs via ggvis","text":"library ggvis similar ggplot2 extents, ’s expressed little differently, adds new features make plots interactive.graphics produced ggvis mostly web graphics work differently traditional R graphics. presentation graphs becomes various fabulous, yet computationally costly. example, every interactive ggvis plot must connected running R session (static plots need running R session viewed). great exploration, can anything interactive plot can R, ’s great publication. overcome issues time, now aware many existing tools reimplement can everything ggvis can base graphics.Firstly, shall import library ggvis. (Since installation packages installed top file, mind, please uncomment following contents install )Similar ggplot2, ggvis also portray make basic plots\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nHence, observed ggvis well create scatterplot graphs, difference geom_points changed layer_points. hand, also options create similar basic graphs ggplot. try methods useful parameters well:layer_bars(width = , stack = ): create bar chart, width stands width bar, stack represents whether bars stackedlayer_points(size = , fill = ): create scatter plot, size stands size dots, andlayer_lines(): create line chartlayer_smooths(span = ): create smooth conditional mean, span stands extent spanning linelayer_histogram(width = ): create histogram, width stands width barslayer_densities(kernel = ): create density curve, kernel stands kind distribution curve tends obeyIn addition static graphs, library ggvis furthermore known abundant methods creating interactive graphs. instance, via input series method, modify parameters ux interface.following code chunk, input_slider employed, conduct sliders span conditional mean size dots. Please run code , try slide sliders aware changes graph. finish modifications, please click esc key exit interactive interface.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\njust saw graph, two sliders bottom graph. sliding horizontally, determine size dots span smooths optimal state. Likewise, see code, instead particular numeric value, span size replaced new function input_slider. input_slider method requires four parameters set range: first two minimum maximum range, value default value slider, label defines name slider. interactive interface pretty useful determining continuous variables, sizes dots scatter plots width bars bar charts histograms.way, sliders inserted functions parameter values reused. Just like following code:\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nJust like code demonstrate, parameters types ranges, establish one slider object first insert function.addition sliders, ggvis provides multiple methods interact graph. Another common method input_select, provide band including various options. Please run code , try change mode kernel, aware changes graph. finish modifications, please click esc key exit interactive interface.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\ninteractive graph created , seen selection box bottom graph, change kernel via selecting different options . code input_select take list basic parameters, mapping displayed options internal parameters graph. instance, Gaussian appeared selection box stand gaussian option kernel. Compared slider object, selection box appropriate categorical parameters, distribution, color, shape.addition sliders selection box, ggvis also offers number interactive controls, particular applications:input_checkbox(): check-box, feature useful determine boolean valuesinput_checkboxgroup(): group check boxes, feature useful determine number boolean valuesinput_numeric(): spin box, best numeric parametersinput_radiobuttons(): pick one set options, best categorical parametersinput_select(): create drop-text box, best categorical parametersinput_text(): arbitrary text input, best string inputs, x y labels titles.conclude, replacing static parameters functions input series, flexibly modify alterable parameters graph.Moreover, ggvis also provides sorts interactive patterns useful data presentations. presenting bar chart, may need emphasize certain bar explanation. Hence, parameter fill.hover helpful indeed. Please run code , swipe mouse bars, see changes graph.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\nJust like just saw, bar change color cursor landing . function fill fill.hover. parameter fill.hover takes one value, specific color, therefore bar turn color touched cursor. helpful presenting diagram someone else.Similarly, method applied kind graphs. Please run code , swipe mouse line, see changes graph.\nRenderer: \nSVG\n | \nCanvas\n\nDownload\ncan see graph , line turn red cursor lands . function exceptionally useful many convoluted overlapped lines specify one presentation.conclude, package ggvis cover major functions ggplot2, well create interactive graphs helpful experiment presentation.","code":"\n# if(!require(ggvis)) install.packages(\"ggvis\")\n# if(!require(dplyr)) install.packages(\"dplyr\")\n\n# library(ggvis)\n# library(dplyr)\n\n# Most of the following diagrams would be generated by the dataset \"iris\"\ndf <- iris\n# Create an ordinary scatterplot\ndf %>%\n  ggvis(~Sepal.Length,\n        ~Sepal.Width, \n        fill=~Species) %>%\n  layer_points()\ndf %>%\n  ggvis(~Sepal.Length, ~Sepal.Width) %>%\n  layer_smooths(span = input_slider(0.5, 1, value = 1, label = \"Span\")) %>%\n  layer_points(size := input_slider(20, 100, value = 20, label = \"Dot Sizes\"))\nslider <- input_slider(20, 100, value = 20, label = \"Dot Sizes\")\ndf %>% ggvis(~Sepal.Length, ~Sepal.Width) %>%\n  layer_points(size := slider) %>% \n  layer_points(fill := \"red\", opacity := 0.5, size := slider)\ndf %>% ggvis(x = ~Sepal.Length) %>%\n    layer_densities(\n      adjust = input_slider(.1, 2, value = 1, step = .1, label = \"Bandwidth adjustment\"),\n      kernel = input_select(\n        c(\"Gaussian\" = \"gaussian\",\n          \"Epanechnikov\" = \"epanechnikov\",\n          \"Rectangular\" = \"rectangular\",\n          \"Triangular\" = \"triangular\",\n          \"Biweight\" = \"biweight\",\n          \"Cosine\" = \"cosine\",\n          \"Optcosine\" = \"optcosine\"),\n        label = \"Kernel\")\n      )\ndf %>% \n  ggvis(~Sepal.Length, \n        fill := \"#fff8dc\", \n        fill.hover := \"#fcb5a2\") %>%\n  layer_histograms(width = 0.25)\ndf %>% \n  ggvis(~Sepal.Length, \n        ~Sepal.Width,\n        stroke = ~Species,\n        stroke.hover := \"red\",\n        strokeWidth := 2\n        ) %>%\n  layer_lines()"},{"path":"interactive-graphs-tutorial.html","id":"create-interactive-graphs-via-plotly","chapter":"32 Interactive graphs tutorial","heading":"32.3 Create Interactive Graphs via plotly","text":"Similar ggvis, plotly also powerful tool create interactive graphs. Compared conventional ggplot ggvis, create complicated well vivid graphs based data various sources.first, please run code install package. (Since installation packages installed top file, mind, please uncomment following contents install )First foremost, plotly, graph created function plot_ly, style setting set function add_lines. Please run code take look graph created.move cursor along curve line, interact graph clearly see x y coordinates dots, truly helpful estimating maximum minimum graph. Likewise, kind graphs created function plot_ly(data =, x =, type = XXX), XXX replace following methods:‘bar’, ‘barpolar’, ‘box’, ‘candlestick’, ‘carpet’, ‘choropleth’, ‘choroplethmapbox’, ‘cone’, ‘contour’, ‘contourcarpet’, ‘densitymapbox’, ‘funnel’, ‘funnelarea’, ‘heatmap’, ‘heatmapgl’, ‘histogram’, ‘histogram2d’, ‘histogram2dcontour’, ‘icicle’, ‘image’, ‘indicator’, ‘isosurface’, ‘mesh3d’, ‘ohlc’, ‘parcats’, ‘parcoords’, ‘pie’, ‘pointcloud’, ‘sankey’, ‘scatter’, ‘scatter3d’, ‘scattercarpet’, ‘scattergeo’, ‘scattergl’, ‘scattermapbox’, ‘scatterpolar’, ‘scatterpolargl’, ‘scatterternary’, ‘splom’, ‘streamtube’, ‘sunburst’, ‘surface’, ‘table’, ‘treemap’, ‘violin’, ‘volume’, ‘waterfall’However, package plotly far . ggvis simply modify appearances graphs, plotly alter type style graphs. following code, added feature named updatememus, panel one figure allows interact , added figure. Please run code try push two buttons see change graph.According graph, easily see two buttons left change style graph violin plot box plot. Likewise, land cursor box violin, statistic values including median, maximum, minimum values outliers shown, exceedingly convenient. Moreover, focus code, see two list button section corresponds two buttons left graph. parameters list defined :method = “restyle” : modify style graph, parameter always “restyle”\nargs = list(“type”, “box”) : first item type parameter plot_ly desired changed, also color\nlabel = “Box Graph” : name appeared buttonIt indeed easy change style graph clicking buttons, please make sure input data acceptable styles graphs choose.Moreover, besides style graphs, color schema graphs modified well pattern. Please run code click button see changes make graph.observed graph , data structure suitable, easily transformed heatmap, contour lines, surface graphs, well various color schema. Regarding code, buttons stored two lists, color types chart types, incorporated final graph operate separately. Thus, like update certain graph various perspectives, please generate one kind buttons within one list zip together updatemenus final graph. Besides, just like previous graphs, moving cursor graph, also see 3-dimensional coordinate segment, swipe cursors surface graph zoom zoom ., previous shown content just tiny proportion plotly, package still contains numerous powerful toolboxes construct interactive graphs explore.","code":"\n# if(!require(plotly)) install.packages(\"plotly\")\n\n# library(plotly)\nx <- seq(-2*pi, 2*pi, length.out = 1000)\ndf <- data.frame(x, y1 = cos(x))\n\nfig <- plot_ly(df, x = ~x) %>% \n  add_lines(y = ~y1)\n\nfig\nfig = plot_ly(midwest, x = ~percollege, color = ~state, type = \"box\")\n\nfig <- fig %>% layout(\n  xaxis = list(domain = c(0.1, 1)),\n  yaxis = list(title = \"y\"),\n  updatemenus = list(\n    list(\n      type = \"buttons\",\n      y = 0.8,\n      # Establish a button to restyle the graph\n      buttons = list(\n        list(method = \"restyle\",\n             args = list(\"type\", \"box\"),\n             label = \"Box Graph\"),\n        \n        list(method = \"restyle\",\n             args = list(\"type\", \"violin\"),\n             label = \"Violin Graph\")\n        ))\n  ))\n\nfig\nfig <- plot_ly(z = ~as.matrix(mtcars), type = \"heatmap\", colorscale='Rainbow')\n\n# chart option buttons\nchart_types <- list(\n  type = \"buttons\",\n  direction = \"right\",\n  xanchor = 'center',\n  yanchor = \"top\",\n  pad = list('r'= 0, 't'= 10, 'b' = 10),\n  x = 0.5,\n  y = 1.27,\n  \n  # type option buttons\n  buttons = list(\n\n    list(method = \"restyle\",\n         args = list(\"type\", \"heatmap\"),\n         label = \"Heatmap\"),\n\n    list(method = \"restyle\",\n         args = list(\"type\", \"contour\"),\n         label = \"Contour\"),\n\n    list(method = \"restyle\",\n         args = list(\"type\", \"surface\"),\n         label = \"Surface\")\n  ))\n\n# color option buttons  \ncolor_types <- list(\n  type = \"buttons\",\n  direction = \"right\",\n  xanchor = 'center',\n  yanchor = \"top\",\n  pad = list('r'= 0, 't'= 10, 'b' = 10),\n  x = 0.5,\n  y = 1.17,\n  buttons = list(\n\n    list(method = \"restyle\",\n         args = list(\"colorscale\", \"Rainbow\"),\n         label = \"Rainbow\"),\n\n    list(method = \"restyle\",\n         args = list(\"colorscale\", \"Jet\"),\n         label = \"Jet\"),\n\n    list(method = \"restyle\",\n         args = list(\"colorscale\", \"Earth\"),\n         label = \"Earth\"),\n\n    list(method = \"restyle\",\n         args = list(\"colorscale\", \"Electric\"),\n         label = \"Electric\")\n  ))\n\nannot <- list(list(text = \"Chart<br>Type\", x=0.2, y=1.25, xref='paper', yref='paper', showarrow=FALSE),\n              list(text = \"Color<br>Type\", x=0.2, y=1.15, xref='paper', yref='paper', showarrow=FALSE))\n\n# plot\nfig <- fig %>% layout(\n  xaxis = list(domain = c(0.1, 1)),\n  yaxis = list(title = \"y\"),\n  updatemenus = list(chart_types,color_types),\n  annotations = annot)\n\nfig"},{"path":"interactive-graphs-tutorial.html","id":"create-interactive-graphs-via-networkd3","chapter":"32 Interactive graphs tutorial","heading":"32.4 Create Interactive Graphs via networkD3","text":"packages ggplot2 ggvis plotly mostly focus presenting values given data, package networkD3 specializes demonstrating relationship data tree diagrams network diagrams. addition, kinds diagrams relatively structured simple form, interactive pattern package may let flexibly manipulate diaragm highlight certain parts.first, please run code chunk install packages import libraries. (Since installation packages installed top file, mind, please uncomment following contents install )Firstly, simply connected data, create simple network function simpleNetwork. Please run code chunk see output graph. also drag nodes diagrams see happen.see diagram , connective relationship diagram defined two zipped lists, . connection node b, please add list add b list , edge presented graph. Moreover, generated network graph highly flexible, rearrange flexibly expected form simply dragging cursor.hierarchical diagrams, appropriate function dendroNetwork, generate tree diagram displaying hierarchy items horizontally. Please run code swipe cursor along child node right see happens.see, method requires two sections. Firstly, cluster child nodes (model cars original data) hierarchical clusters method hclust. method take two parameters, input dataframe, method cluster . case, data clustered based ave (average). , hclust object generated displayed method dendroNetwork, accepts hclust inputs, conduct tree diagram. diagram, adjacency child nodes represents similarities average. Furthermore, swipe mouse nodes’ names, magnify font immediately, function shall pretty helpful presentations.addition simple networks, still functions package may utilize:\n1. sankeyNetwork: generate flow chart illustrate outflow inflow\n2. forceNetwork: control appearance forced directed network plot complicated networks.\n3. radialNetwork: radar-like appearances, source node locates center graph, spreads numerous child nodes.\n4. diagonalNetwork: similar dendroNetwork, links nodes via smooth curves.","code":"\n# if(!require(igraph)) install.packages(\"igraph\")\n# if(!require(networkD3)) install.packages(\"networkD3\")\n\n# library(igraph)\n# library(networkD3)\n# create a dataset:\ndata <- tibble(\n  from=c(\"A\", \"A\", \"B\", \"D\", \"C\", \"D\", \"E\", \"B\", \"C\", \"D\", \"K\", \"A\", \"M\"),\n  to=c(\"B\", \"E\", \"F\", \"A\", \"C\", \"A\", \"B\", \"Z\", \"A\", \"C\", \"A\", \"B\", \"K\")\n)\n\n# Plot\nsimpleNetwork(data,\n              height=\"100px\", \n              width=\"100px\"\n              )\ndf <- hclust(dist(mtcars), \"ave\")\n\ndendroNetwork(df, fontSize = 10)"},{"path":"interactive-graphs-tutorial.html","id":"conclusion","chapter":"32 Interactive graphs tutorial","heading":"32.5 Conclusion","text":"summarize, still number fantastic tools create interactive graphs, addition ggvis, plotly, networkD3. However, master functions mentioned tutorial, quite convenient produce nice graphs projects.","code":""},{"path":"interactive-graphs-tutorial.html","id":"reference-4","chapter":"32 Interactive graphs tutorial","heading":"32.6 Reference","text":"Plotly R Graphing Library. (2021). R | Plotly. https://plotly.com/r/\nGandrud, Christopher. (2017). christophergandrud. http://christophergandrud.github.io/networkD3/","code":""},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"guideline-on-using-package-rcurl-and-xml-for-web-scraping","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33 Guideline on Using Package Rcurl and XML for Web Scraping","text":"Sitong Qian","code":"\nlibrary(RCurl)\nlibrary(XML)"},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"introduction-3","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.1 Introduction","text":"comes world EDAV, rich dataset inevitably cornerstone success. obtaining dataset always easy job, sometimes, dataset dirty, waiting throughout clean using, might case ’s still born yet, piece-wise information, needed assembled data frame format. Thus, ’s bringing us question construct dataset none., want give approach extarct basic information need dataset construction website, namely web scraping, using package Rcurl XML. ease potential confusion, go concrete example step step, introducing functions go steps, explaining details behind functions simple way showing exactly extracting work.","code":""},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"extracting-information-for-constructing-dataset-rcurl-and-xml","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.2 Extracting Information for Constructing Dataset – Rcurl and XML","text":"","code":""},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"step-1-web-searching","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.2.1 Step 1: Web Searching","text":"first step choosing topic interested , Since ’s job hunting season, let’s say wanna know Data related job posted website CyberCoders. simply copy page browserBy clicking link, expected directed page like .Now, let’s put keyword DataHere, use getForm() function Rcurl Package.According Rcurl package description cran, getForm() provide facilities submitting HTML form using simple GET mechanism(appending name-value pairs parameters URL)straightfoward, getForm() work putting search word box click search buttom. keyword Data myinput, searchterms comes ? Now, go steps searchterms\nfollowing instruction valid macOS system\n\nUsing Chrome browser, site cybercoders, click View -> Developer -> ViewSourceThen redirected page gives HTML source code, page, see HTML page build hierarchy, quickly go page, find term describes search box line 130 name = “searchterms”finish first step, Web Searching. Clearly, different websites hierarchy tree possibly unique ways calling searchterms, Thus, using method, need look specific cases slightly modifications.","code":"\nlink = 'https://www.cybercoders.com/search'\nsearch = getForm(link,searchterms = 'Data' )"},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"step-2-cleanupparsing-search-results","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.2.2 Step 2: Cleanup(Parsing) Search Results","text":"previous step, end putting keywords Data searchterms. , search returns basically expected see enter Data search box directly website, click View Source.input search console, find returns HTML page looking really messy form. ’s getform() function converts page character.cut length, just took screenshot console, otherwise, output occupy several pages actual meaning.want, since HTML built hierarchy, forms can’t obtain information. Thus need return readable form. introduce another powerful function htmlParse XML packageHere, htmlParse simple can directly referred function name, parsing output HTML form.cut length, just took screenshot console, otherwise, output occupy several pages actual meaning.returning nice clean form, move next job, detect pattern!Web scraping really fun, time ’s build elegant neatness doesn’t mean always nice easy, sake time, just assume time.","code":"\ndocument = htmlParse(search)"},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"step-3-detect-the-pattern","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.2.3 Step 3: Detect the Pattern","text":"previous section, return results nice clean form, now need detect pattern. ’s fairly easy see variable document give us first page searching result. directly look website, page returned roughly 21 job positions, want divide 21 jobs can look .introduce function getNodeSet()According XML instruction cran, getNodeSet() find matching nodes internal XML tree. see class(document) find one “XMLInternalDocument”.Exactly want!discussed , HTML built hierarchy. ’s hard find jobs starts div class=“job-listing-item” Thus, now know correct function use know nodes, combine two together, get function. exactly 21 items list, corresponding 21 jobs display websites, showed first jobs later illustration","code":"\nclass(document)## [1] \"HTMLInternalDocument\" \"HTMLInternalDocument\" \"XMLInternalDocument\" \n## [4] \"XMLAbstractDocument\"\nlist = getNodeSet(document,\"//div[@class = 'job-listing-item']\")\nlist[1][[1]]## <div class=\"job-listing-item\">&#13;\n##     <div class=\"job-status\">&#13;\n##         &#13;\n##     <\/div>&#13;\n##     <div class=\"job-details-container\">&#13;\n##         <div class=\"job-title\">&#13;\n##             <a href=\"/data-engineer-job-455427\">Data Engineer<\/a>&#13;\n##         <\/div>&#13;\n##         <div class=\"details\">&#13;\n##             <div class=\"location\">Mountain View, CA <\/div>&#13;\n## &#13;\n## &#13;\n## &#13;\n##                 <div class=\"wage\"><span>Full-time<\/span> $80k - $130k<\/div>&#13;\n##             <div class=\"posted\"><span>Posted<\/span> 10/19/2021<\/div>&#13;\n##         <\/div>&#13;\n##         <div class=\"description\">&#13;\n##             If you are a Data Engineer with experience, please read on! This is a great opportunity for a Data Engineer who is looking to make a shift to Data Science or wanting to develop more skills. This indiv...&#13;\n##         <\/div>&#13;\n##             <div class=\"skills\">&#13;\n##                 <ul class=\"skill-list\"><li class=\"skill-item\">&#13;\n##                             <a href=\"/search/data-engineering-skills/\">&#13;\n##                                 <span class=\"left-off\"/>&#13;\n##                                 <span class=\"skill-name\">Data Engineering<\/span>&#13;\n##                                 <span class=\"right\"/>&#13;\n##                             <\/a>&#13;\n##                         <\/li>&#13;\n##                         <li class=\"skill-item\">&#13;\n##                             <a href=\"/search/data-science-skills/\">&#13;\n##                                 <span class=\"left-off\"/>&#13;\n##                                 <span class=\"skill-name\">Data Science<\/span>&#13;\n##                                 <span class=\"right\"/>&#13;\n##                             <\/a>&#13;\n##                         <\/li>&#13;\n##                         <li class=\"skill-item\">&#13;\n##                             <a href=\"/search/gathering-and-collecting-data-skills/\">&#13;\n##                                 <span class=\"left-off\"/>&#13;\n##                                 <span class=\"skill-name\">Gathering and Collecting Data<\/span>&#13;\n##                                 <span class=\"right\"/>&#13;\n##                             <\/a>&#13;\n##                         <\/li>&#13;\n##                         <li class=\"skill-item\">&#13;\n##                             <a href=\"/search/batch-processing-skills/\">&#13;\n##                                 <span class=\"left-off\"/>&#13;\n##                                 <span class=\"skill-name\">Batch Processing<\/span>&#13;\n##                                 <span class=\"right\"/>&#13;\n##                             <\/a>&#13;\n##                         <\/li>&#13;\n##                         <li class=\"skill-item\">&#13;\n##                             <a href=\"/search/api-skills/\">&#13;\n##                                 <span class=\"left-off\"/>&#13;\n##                                 <span class=\"skill-name\">API<\/span>&#13;\n##                                 <span class=\"right\"/>&#13;\n##                             <\/a>&#13;\n##                         <\/li>&#13;\n##                 <\/ul><\/div>&#13;\n##     <\/div>&#13;\n## <\/div>"},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"step-4-extract-information-on-each-job-post","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.2.4 Step 4: Extract Information on Each Job Post","text":"previous step, get list 21 jobs, now want extract actual information looking construct dataset. Based result, look variables like Job-Title,Wage,Location,Skill,Posted Date,Job Description, etcThen, extract variable name categories? look back first job reference. Let’s say wanna Job-TitleWe introduce function xpathSApplyAccording XML cran, can think xpathSApply two parts, xpath SApply, xpath scan page match result input, SApply returned .like , found job-title starts node div class=‘job-title’, combine function indicatorWhile title 1 located information want, slightly messy since ’s still contains XML format.magic tool, xmlValue, adding xmlValue function, get rid XML locators, just valueStill, irrevalant things, can use trimws fix , get nice pretty formThis xpathSApply function really powerful function extracting value need, ’s return character, can use regular expression clean-jobs. personally think important function XML package.1st job","code":"\ntest = list[1][[1]]\ntitle1 = xpathSApply(test, \".//div[@class = 'job-title']\")\ntitle1## [[1]]\n## <div class=\"job-title\">&#13;\n##             <a href=\"/data-engineer-job-455427\">Data Engineer<\/a>&#13;\n##         <\/div>\ntitle2 = xpathSApply(test, \".//div[@class = 'job-title']\",xmlValue)\ntitle2## [1] \"\\r\\n            Data Engineer\\r\\n        \"\ntitle3 = trimws(xpathSApply(test, \".//div[@class = 'job-title']\",xmlValue))\ntitle3## [1] \"Data Engineer\"\njob_title = trimws(unique(xpathSApply(test, \".//div[@class = 'job-title']\",xmlValue)))\nsalary_jobStatus = trimws(unique(xpathSApply(test, \".//div[@class = 'wage']\",xmlValue)))\nlocation = trimws(unique(xpathSApply(test, \".//div[@class = 'location']\",xmlValue)))\ndate = trimws(xpathSApply(test,\".//div[@class = 'posted']\",xmlValue))\ndescription =  trimws(xpathSApply(test,\".//div[@class = 'description']\",xmlValue))\npreferred_skills = trimws(unique(xpathSApply(test, \".//li[@class = 'skill-item']\",xmlValue)))\njob_title## [1] \"Data Engineer\"\nsalary_jobStatus## [1] \"Full-time $80k - $130k\"\nlocation## [1] \"Mountain View, CA\"\ndate## [1] \"Posted 10/19/2021\"\ndescription## [1] \"If you are a Data Engineer with experience, please read on! This is a great opportunity for a Data Engineer who is looking to make a shift to Data Science or wanting to develop more skills. This indiv...\"\npreferred_skills## [1] \"Data Engineering\"              \"Data Science\"                 \n## [3] \"Gathering and Collecting Data\" \"Batch Processing\"             \n## [5] \"API\""},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"step-5-combine-job-posts","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.2.5 Step 5: Combine Job Posts","text":", come next step, combining results job, well lots XML Rcurl package. However, thought intuitive step sake completeness, help build overall logical way thinking web scraping.Basically, apply one post, now want every post first page. code nothing fancy, just throw individual code “function form”.cut length, just output first three, output posts occupy several pages actual meaning.cut length, just output first three, output posts occupy several pages actual meaning.’s shown, results pretty much cleaned use probably need help regular expression make become actual dataset. since guideline focusing web scraping mainly. call end.","code":"\njob =\n  function(ind)\n  {\n    job_title = trimws(unique(xpathSApply(ind, \".//div[@class = 'job-title']\",xmlValue)))\n    salary_jobStatus = trimws(unique(xpathSApply(ind, \".//div[@class = 'wage']\",xmlValue)))\n    location = trimws(unique(xpathSApply(ind, \".//div[@class = 'location']\",xmlValue)))\n    date = trimws(xpathSApply(ind,\".//div[@class = 'posted']\",xmlValue))\n    description =  trimws(xpathSApply(ind,\".//div[@class = 'description']\",xmlValue))\n    preferred_skills = trimws(unique(xpathSApply(ind, \".//li[@class = 'skill-item']\",xmlValue)))\n    list(title = job_title, job_status =  salary_jobStatus,date = date,location = location,job_description = description,preferred_skills = preferred_skills)\n  }\n\npagecontentjoblist = function(link){\n    searchlinks = getForm(link,searchterms = 'Data' ) \n    documentlinks = htmlParse(searchlinks)\n    all_job_lists = getNodeSet(documentlinks,\"//div[@class = 'job-listing-item']\")\n    return(all_job_lists)\n}\n\nall_job_lists = pagecontentjoblist(link)\npost_all = lapply(all_job_lists, job)\npost_all[1:3]## [[1]]\n## [[1]]$title\n## [1] \"Data Engineer\"\n## \n## [[1]]$job_status\n## [1] \"Full-time $80k - $130k\"\n## \n## [[1]]$date\n## [1] \"Posted 10/19/2021\"\n## \n## [[1]]$location\n## [1] \"Mountain View, CA\"\n## \n## [[1]]$job_description\n## [1] \"If you are a Data Engineer with experience, please read on! This is a great opportunity for a Data Engineer who is looking to make a shift to Data Science or wanting to develop more skills. This indiv...\"\n## \n## [[1]]$preferred_skills\n## [1] \"Data Engineering\"              \"Data Science\"                 \n## [3] \"Gathering and Collecting Data\" \"Batch Processing\"             \n## [5] \"API\"                          \n## \n## \n## [[2]]\n## [[2]]$title\n## [1] \"Data Analyst - Google Tag Manager, A/B Testing\"\n## \n## [[2]]$job_status\n## [1] \"Full-time $90k - $130k\"\n## \n## [[2]]$date\n## [1] \"Posted 09/30/2021\"\n## \n## [[2]]$location\n## [1] \"Los Angeles, CA\"\n## \n## [[2]]$job_description\n## [1] \"If you are a Data Analyst with experience, please read on!    This is a full-time/permanent position, direct-hire with my client.    We are the world leading digital invitation platform that has been...\"\n## \n## [[2]]$preferred_skills\n## [1] \"GTM\"                \"Google Tag Manager\" \"Google Analytics\"  \n## [4] \"SQL\"                \"AB testing\"        \n## \n## \n## [[3]]\n## [[3]]$title\n## [1] \"Data Scientist\"\n## \n## [[3]]$job_status\n## [1] \"Full-time $100k - $150k\"\n## \n## [[3]]$date\n## [1] \"Posted 10/06/2021\"\n## \n## [[3]]$location\n## [1] \"Morrisville, NC\"\n## \n## [[3]]$job_description\n## [1] \"If you are a Data Scientist or Data Enthusiast with experience in bioinformatics, please read on!  We are looking to simplify the complex nature of cancer genomics and bring cutting edge technology an...\"\n## \n## [[3]]$preferred_skills\n## [1] \"Python\" \"Perl\"   \"SQL\"    \"R\"      \"SAS\""},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"conclusion-1","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.3 Conclusion","text":"go like web scraping using packages XML Rcurl, covered just basic approach using frequently used functions two packages. lots useful functions worth reading time. hope people read tutorial clear understanding idea Web Scraping, .","code":""},{"path":"guideline-on-using-package-rcurl-and-xml-for-web-scraping.html","id":"work-cited","chapter":"33 Guideline on Using Package Rcurl and XML for Web Scraping","heading":"33.4 Work Cited","text":"https://cran.r-project.org/web/packages/XML/XML.pdf\nhttps://cran.r-project.org/web/packages/RCurl/RCurl.pdf","code":""},{"path":"twitter-r-package-scraping-via-api-tutorial.html","id":"twitter-r-package-scraping-via-api-tutorial","chapter":"34 TwitteR R Package Scraping via API Tutorial","heading":"34 TwitteR R Package Scraping via API Tutorial","text":"Felix YeungSet :order begin using TwitteR package, first need create Twitter Account App can get API Access Keys & Tokens. exact steps:don’t one, go Twitter.com create account.Go apps.twitter.com create app *Note fill information creating app. easiest way write ’re exploring API.approved, can can finally start create app.completing steps, given opportunity save API Key, API Secret, Access Token, Access Token Secret. need save access API. *Note forgot save can always regenerate .Lastly, ’re done steps, can run install.packages(\"rtweet\") ’ll set!’s link full documentation: https://cran.r-project.org/web/packages/rtweet/rtweet.pdfStarting loading authenticiation token:keys/tokens acquired Set , can store token easy access. going need use token often ’re utilizing functions rtweet.Introduction useful functions:Friends & Followers\n- Friend account user follows\n- Follower account follows userThere’s functions enable pull friends followers account given handle.\n* get_friends() pulls list IDs friends user\n* get_followers() pulls list IDs follow userHere’s exampleWith list User IDs, can now use `lookup_users()’ find relevant information UserID.Tweets & FavoritesNext can look functions can pull information Tweets & Favorites.\n- Tweets character messages users can post Twitter\n- Favorites Tweets users likedHere useful functions:\nsearch_tweets()\nlookup_tweets()\n*get_favorites()Analysis & Interesting Application functions:Graphing # favorited tweets account timeNow basic building block functions, can put together interesting visualiations using Twitter data.first example , can graph number tweets @DataSciColumbia favorited!can see, account really active favorit-ing tweets 2017 2019. Maybe, go check owner account?Finding top hashtagsNext, can use search tweets function pull tweets mention @DataSciColumbia account find top hashtags (frequency) used mentioning account.","code":"\nlibrary(rtweet)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(DT)\nlibrary(ggplot2)\n# load rtweet\nlibrary(rtweet)\n\n# Store API Keys here. Use your own! These are fakes.\napikey <- \"bgBjMmye9UQESChqld2gjkDnm\"\napisecret <- \"P9xUNJ2PCbxlQ3znD6bj8jOM7VPSdqeujVmCjMUxYijBYFGA2c\"\naccestoken <- \"1283967367333191687-bMXBUIqjkbYcaZnMfacl9V0ijcYf9j\"\naccesssecret <- \"HAN0d943eDJFzmDpUrdSCcfWftkWnpJiaiMWCEGLylIHv\"\n\n#Create a token with the create_token function\n\ntoken <- create_token(\n  app = \"RPackageTest\",\n  consumer_key = apikey,\n  consumer_secret = apisecret,\n  access_token = accestoken,\n  access_secret = accesssecret)\n\ntoken\n#Get a list of friends for the @DataSciColumbia account\nfriends <- get_friends(\"DataSciColumbia\")\nmultifriends <- get_friends(c(\"Columbia\", \"DataSciColumbia\")) #Note you can pass multiple handles at the same time\n\n#Get a list of followers of the @DataSciColumbia account\nfollowers <- get_followers(\"DataSciColumbia\")\n\ndatatable(friends)\ndatatable(multifriends)\ndatatable(followers)\nusers <- c(followers$user_id)\nlookup_users(users[0:10]) #get information on 10 users that follow @DataSciColumbia\n# search tweets for a keyword or phrase\nsearch_tweets(q = \"Columbia Data Science\")\n#You can even specify the language or whether you want to include retweets\nsearch_tweets(\"Columbia Data Science\", lang = \"en\", include_rts = FALSE)\n\n#If you have just the status IDs, you can use the lookup_tweets() function to get number of tweets. \nlookup_tweets(c(\"1454214453990416386\", \"1454213087473672194\"))\n\n#If you have just the user handle, you can also look up what tweets a user have favorited.\nfavorites <- get_favorites(\"DataSciColumbia\", n = 3000)\nts_plot(favorites, \"weeks\")\n#Search for tweets that mention @DataSciColumbia\ntweets <- search_tweets(\"@DataSciColumbia\",n=1000,include_rts = FALSE,retryonratelimit=F)\n\n#Show a preview of screen names and tweets\npreview <- tweets %>% dplyr::select(screen_name,text)\ndatatable(preview)\n\n#Pull all the hashtags associated with the tweets\nhash <- tweets %>% dplyr::select(created_at,screen_name,hashtags)\nhash <- unnest(hash,cols=c(hashtags))\n\n#Get a list of all the hashtags and count by frequency\ntophash <- hash %>% filter(is.na(hashtags)==F) %>% \n            group_by(hashtags) %>% \n            summarise(count=n()) %>%\n            arrange(-count) %>%\n            arrange(count) %>% \n            mutate(hashtags=factor(hashtags,levels=hashtags))\n\n#Plot the hashtags in a geom bar chart\nggplot(tophash,aes(x=hashtags,y=count)) + geom_bar(stat='identity') + \n    ggtitle(label=\"Most popular hashtags that menioned @DataSciColumbia\") +\n    coord_flip()"},{"path":"introduction-to-data-manipulation.html","id":"introduction-to-data-manipulation","chapter":"35 Introduction to Data Manipulation","heading":"35 Introduction to Data Manipulation","text":"Yuhe Wang","code":"\nlibrary(tidyverse)\nlibrary(fueleconomy)\nlibrary(tidyverse)\nlibrary(lubridate)"},{"path":"introduction-to-data-manipulation.html","id":"data-wrangling-with-dplyr","chapter":"35 Introduction to Data Manipulation","heading":"35.1 Data Wrangling with dplyr","text":"","code":""},{"path":"introduction-to-data-manipulation.html","id":"pipe","chapter":"35 Introduction to Data Manipulation","heading":"35.1.1 Pipe:","text":"use %>% pipeline. %>% operation(…) == operation(, ….)","code":""},{"path":"introduction-to-data-manipulation.html","id":"tibble-6","chapter":"35 Introduction to Data Manipulation","heading":"35.1.2 Tibble","text":"Tibble type data structure similar data frame base R.Compared normal dataframee, tibble never changes type input, names variables, row names.","code":"\ntibble(x=1:5, y=1,z=x^2+y)## # A tibble: 5 × 3\n##       x     y     z\n##   <int> <dbl> <dbl>\n## 1     1     1     2\n## 2     2     1     5\n## 3     3     1    10\n## 4     4     1    17\n## 5     5     1    26"},{"path":"introduction-to-data-manipulation.html","id":"pick-observations-with-filter","chapter":"35 Introduction to Data Manipulation","heading":"35.1.3 Pick observations with filter()","text":"R provides standard suites: <, >=, <=, !=, ==, %% apply conditions rows","code":"\nvehicles %>% filter(year>1999)## # A tibble: 16,649 × 12\n##       id make  model  year class   trans   drive     cyl displ fuel    hwy   cty\n##    <dbl> <chr> <chr> <dbl> <chr>   <chr>   <chr>   <dbl> <dbl> <chr> <dbl> <dbl>\n##  1 16573 Acura 3.2CL  2001 Compac… Automa… Front-…     6   3.2 Prem…    27    17\n##  2 17489 Acura 3.2CL  2002 Compac… Automa… Front-…     6   3.2 Prem…    27    17\n##  3 18458 Acura 3.2CL  2003 Compac… Manual… Front-…     6   3.2 Prem…    26    17\n##  4 18459 Acura 3.2CL  2003 Compac… Automa… Front-…     6   3.2 Prem…    27    17\n##  5 15871 Acura 3.2TL  2000 Midsiz… Automa… Front-…     6   3.2 Prem…    27    17\n##  6 16734 Acura 3.2TL  2001 Midsiz… Automa… Front-…     6   3.2 Prem…    27    17\n##  7 17664 Acura 3.2TL  2002 Midsiz… Automa… Front-…     6   3.2 Prem…    27    17\n##  8 18629 Acura 3.2TL  2003 Midsiz… Automa… Front-…     6   3.2 Prem…    27    17\n##  9 15872 Acura 3.5RL  2000 Midsiz… Automa… Front-…     6   3.5 Prem…    22    16\n## 10 16735 Acura 3.5RL  2001 Midsiz… Automa… Front-…     6   3.5 Prem…    22    16\n## # … with 16,639 more rows"},{"path":"introduction-to-data-manipulation.html","id":"reorder-rows-with-arrange","chapter":"35 Introduction to Data Manipulation","heading":"35.1.4 Reorder rows with arrange()","text":"","code":"\nvehicles %>% arrange(year,class,trans)## # A tibble: 33,442 × 12\n##       id make      model   year class  trans drive   cyl displ fuel    hwy   cty\n##    <dbl> <chr>     <chr>  <dbl> <chr>  <chr> <chr> <dbl> <dbl> <chr> <dbl> <dbl>\n##  1 27049 Buick     Elect…  1984 Large… Auto… 2-Wh…     6   4.1 Regu…    19    14\n##  2 27050 Buick     Elect…  1984 Large… Auto… 2-Wh…     8   5   Regu…    20    14\n##  3 27051 Buick     Elect…  1984 Large… Auto… 2-Wh…     8   5.7 Dies…    26    18\n##  4 27057 Cadillac  Broug…  1984 Large… Auto… Rear…     8   4.1 Regu…    19    14\n##  5 27058 Cadillac  Broug…  1984 Large… Auto… Rear…     8   5.7 Dies…    26    18\n##  6 28105 Cadillac  Broug…  1984 Large… Auto… Rear…     8   4.1 Regu…    19    14\n##  7 28106 Cadillac  Fleet…  1984 Large… Auto… Rear…     6   4.3 Dies…    31    21\n##  8 28225 Chevrolet S10 P…  1984 Small… Auto… 2-Wh…     4   2   Regu…    24    18\n##  9 27219 Dodge     Ram 5…  1984 Small… Auto… 2-Wh…     4   2   Regu…    21    20\n## 10 27220 Dodge     Ram 5…  1984 Small… Auto… 2-Wh…     4   2   Regu…    20    18\n## # … with 33,432 more rows"},{"path":"introduction-to-data-manipulation.html","id":"create-new-variables-using-mutate","chapter":"35 Introduction to Data Manipulation","heading":"35.1.5 Create new variables using mutate()","text":"","code":"\nvehicles %>% mutate(cyl_2 = cyl*2)## # A tibble: 33,442 × 13\n##       id make  model  year class trans drive   cyl displ fuel    hwy   cty cyl_2\n##    <dbl> <chr> <chr> <dbl> <chr> <chr> <chr> <dbl> <dbl> <chr> <dbl> <dbl> <dbl>\n##  1 13309 Acura 2.2C…  1997 Subc… Auto… Fron…     4   2.2 Regu…    26    20     8\n##  2 13310 Acura 2.2C…  1997 Subc… Manu… Fron…     4   2.2 Regu…    28    22     8\n##  3 13311 Acura 2.2C…  1997 Subc… Auto… Fron…     6   3   Regu…    26    18    12\n##  4 14038 Acura 2.3C…  1998 Subc… Auto… Fron…     4   2.3 Regu…    27    19     8\n##  5 14039 Acura 2.3C…  1998 Subc… Manu… Fron…     4   2.3 Regu…    29    21     8\n##  6 14040 Acura 2.3C…  1998 Subc… Auto… Fron…     6   3   Regu…    26    17    12\n##  7 14834 Acura 2.3C…  1999 Subc… Auto… Fron…     4   2.3 Regu…    27    20     8\n##  8 14835 Acura 2.3C…  1999 Subc… Manu… Fron…     4   2.3 Regu…    29    21     8\n##  9 14836 Acura 2.3C…  1999 Subc… Auto… Fron…     6   3   Regu…    26    17    12\n## 10 11789 Acura 2.5TL  1995 Comp… Auto… Fron…     5   2.5 Prem…    23    18    10\n## # … with 33,432 more rows"},{"path":"introduction-to-data-manipulation.html","id":"create-new-calculations-by-catgories-using-summarize","chapter":"35 Introduction to Data Manipulation","heading":"35.1.6 Create new calculations by catgories using summarize()","text":"can use summarize get statistics different groups. following example, getting average difference air time scheduled air time, grouped different carriers. Note add “na.rm” inside functio remove NA, else see lot NAs.\nCommon operation functions:\nsd(x): standard deviation\nmean(x): mean\nIQR(x): interquartile range\nmad(x): median absolute deviation\nmin(x): min\nquantile(x, 0.5): ith quartile\nmax(x): max\nfirst(x): first row\nnth(x,1): nth row\nlast(): last row\nn(): count\nn_distinct(x): count distinct\nsum(): sum","code":"\nvehicles %>% group_by(class, fuel) %>% summarize(mean_cty =mean(cty, na.rm = TRUE))## # A tibble: 151 × 3\n## # Groups:   class [34]\n##    class        fuel                       mean_cty\n##    <chr>        <chr>                         <dbl>\n##  1 Compact Cars CNG                            24.5\n##  2 Compact Cars Diesel                         29.2\n##  3 Compact Cars Electricity                   110  \n##  4 Compact Cars Gasoline or E85                21.9\n##  5 Compact Cars Midgrade                       16  \n##  6 Compact Cars Premium                        17.6\n##  7 Compact Cars Premium Gas or Electricity     35  \n##  8 Compact Cars Premium or E85                 17.2\n##  9 Compact Cars Regular                        21.1\n## 10 Large Cars   CNG                            13.6\n## # … with 141 more rows"},{"path":"introduction-to-data-manipulation.html","id":"tidy-data-with-dplyr","chapter":"35 Introduction to Data Manipulation","heading":"35.2 Tidy Data with dplyr","text":"","code":""},{"path":"introduction-to-data-manipulation.html","id":"gatherspread","chapter":"35 Introduction to Data Manipulation","heading":"35.2.1 Gather/Spread","text":"can reshape table wide format long format using Gather. can reshape table wide format long format using Sprea vice versa. (following example untrue data)","code":"\nt1 <- tibble(country=c('China', 'US', 'Korea'), `1999` = c(123,323,4245),`2000` = c(12,32,424))\nt1## # A tibble: 3 × 3\n##   country `1999` `2000`\n##   <chr>    <dbl>  <dbl>\n## 1 China      123     12\n## 2 US         323     32\n## 3 Korea     4245    424\nt1 %>% gather(`1999`, `2000`,key='year',value = 'GDP')## # A tibble: 6 × 3\n##   country year    GDP\n##   <chr>   <chr> <dbl>\n## 1 China   1999    123\n## 2 US      1999    323\n## 3 Korea   1999   4245\n## 4 China   2000     12\n## 5 US      2000     32\n## 6 Korea   2000    424"},{"path":"introduction-to-data-manipulation.html","id":"seperateunite","chapter":"35 Introduction to Data Manipulation","heading":"35.2.2 Seperate/Unite","text":"can combine/separate values one column two, two one(using special characters). can add parameter “convert=TRUE” convert chars integer directly.","code":"\nt2 <- tibble(country=c('China', 'US', 'Korea'), rate=c('12/232','123/20384','2328/2301823'))\nt2 %>% separate(rate,into=c(\"numerator\", \"denominator\"),convert=TRUE)## # A tibble: 3 × 3\n##   country numerator denominator\n##   <chr>       <int>       <int>\n## 1 China          12         232\n## 2 US            123       20384\n## 3 Korea        2328     2301823"},{"path":"introduction-to-data-manipulation.html","id":"relational-data-with-dplyr","chapter":"35 Introduction to Data Manipulation","heading":"35.3 Relational Data with dplyr","text":"","code":""},{"path":"introduction-to-data-manipulation.html","id":"prerequisites","chapter":"35 Introduction to Data Manipulation","heading":"35.3.1 prerequisites","text":"KEYS DBMS attribute set attributes helps identify row relation. primary key uniquely identifies observation. foreign key uniquely identify observation another table. Join two table usually takes place two keys within different tables ensure single join.","code":""},{"path":"introduction-to-data-manipulation.html","id":"understanding-different-types-of-joins","chapter":"35 Introduction to Data Manipulation","heading":"35.3.2 Understanding different types of joins","text":"x () y\ninner join: return matching pairs existing tables\nleft join: keep observations x\nright join: keep observations y\nfull join:keep observations x y\nNote one tables can duplicate keys. tables duplicate keys, cause error. Usually, duplicate key one table can produce unexpected result. , best practice investigate tables firstly joining.case two tables different names keys, can use =c(“”=“b”)","code":"\n# These data are frictional \nx <- tibble(Country = c('China','US','Japan','Canada'), population=c(100,200,300,400))\ny <- tibble(Country = c('China','US','Japan','Mexico'), GDP=c(100,23,2142,234))\nleft_join(x, y, by='Country')## # A tibble: 4 × 3\n##   Country population   GDP\n##   <chr>        <dbl> <dbl>\n## 1 China          100   100\n## 2 US             200    23\n## 3 Japan          300  2142\n## 4 Canada         400    NA\nright_join(x, y, by='Country')## # A tibble: 4 × 3\n##   Country population   GDP\n##   <chr>        <dbl> <dbl>\n## 1 China          100   100\n## 2 US             200    23\n## 3 Japan          300  2142\n## 4 Mexico          NA   234\ninner_join(x, y, by='Country')## # A tibble: 3 × 3\n##   Country population   GDP\n##   <chr>        <dbl> <dbl>\n## 1 China          100   100\n## 2 US             200    23\n## 3 Japan          300  2142\nfull_join(x, y, by='Country')## # A tibble: 5 × 3\n##   Country population   GDP\n##   <chr>        <dbl> <dbl>\n## 1 China          100   100\n## 2 US             200    23\n## 3 Japan          300  2142\n## 4 Canada         400    NA\n## 5 Mexico          NA   234"},{"path":"introduction-to-data-manipulation.html","id":"datetime-with-lubridate","chapter":"35 Introduction to Data Manipulation","heading":"35.4 Datetime with Lubridate","text":"","code":"\nlibrary(tidyverse)\nlibrary(lubridate)"},{"path":"introduction-to-data-manipulation.html","id":"datetime-from-strings","chapter":"35 Introduction to Data Manipulation","heading":"35.4.1 Datetime from strings","text":"Lubridate able parse different format datetime stringsGetting components datetime","code":"\nymd('2020-01-01')## [1] \"2020-01-01\"\nmdy('March 1 2021')## [1] \"2021-03-01\"\ndmy('02/01/2020')## [1] \"2020-01-02\"\nt <- ymd_hms('2020-01-01 12:00:00')\nyear(t)## [1] 2020\nmonth(t)## [1] 1\n# day of the month\nmday(t)## [1] 1\n# day of the year\nyday(t)## [1] 1\n# day of the week\nwday(t)## [1] 4\nhour(t)## [1] 12\nminute(t)## [1] 0\nsecond(t)## [1] 0"},{"path":"introduction-to-data-manipulation.html","id":"timespan","chapter":"35 Introduction to Data Manipulation","heading":"35.4.2 Timespan","text":"Duration: exact number seconds\nPeriods: human units (week months)\nIntervals: starting ending pointDurationPeriods","code":"\ndminutes(10)## [1] \"600s (~10 minutes)\"\ndweeks(3)## [1] \"1814400s (~3 weeks)\"\ndyears(1)## [1] \"31557600s (~1 years)\"\nseconds(15)## [1] \"15S\"\ndays(7)## [1] \"7d 0H 0M 0S\"\nmonths(1:6)## [1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n## [5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\nyears(1)/days(1)## [1] 365.25\ntoday() + years(1)## [1] \"2022-11-07\""},{"path":"introduction-to-data-manipulation.html","id":"string-manipulations-with-stringr","chapter":"35 Introduction to Data Manipulation","heading":"35.5 String manipulations with stringr","text":"","code":""},{"path":"introduction-to-data-manipulation.html","id":"matching-patterns","chapter":"35 Introduction to Data Manipulation","heading":"35.5.1 Matching Patterns","text":"can regular expression matching R easily .\nstring_view() useful function showcase string patterns. first input input variable, second input string trying match. following regular expression matches introduced using method.can match substring directly easily.can use ‘.’ wildcard match characterWe can use ‘^’ match string starting “”, “$” find string ending “”. can also use ‘^’ beginning “$” end make sure ’s exact match.can also specify many times character repeats . {n} represents exactly n times repetition,{n,}: n , {,n}: n, {n,m}: n mThere also extra type strings can match characters mentioned. ‘\\d’ matches digit,‘\\s’ matches white space, [xyz] matches x, y z, [^xyz] matches anything x, y z. noticing want match substring starting one backlash, specify two backlashes string matching.introduce couple methods useful conjunction use regular expression.\n1.str_detect(): see detect certain substring, return TRUE FALSE\n2.str_extract(): extract actual substring string\n3.str_subset(): return group strings matches certain pattern\n4.str_count(): count number substring appearances string\n5.str_replace(): replace substrings certain patterns\n6.str_split(): split string according patternsSource: R Data Science","code":"\na <-c('root', 'create','time','death')\nstr_view(a, \"ea\")\na <-c('root', 'create','time','death')\nstr_view(a, \"im.\")\na <-c('root', 'create','time','death','eath')\nstr_view(a, \"^c\")\nstr_view(a, \"^death$\")\nstr_view(a, \"e$\")\na <-'CCccoljenlq'\nstr_view(a, 'C{2}')\na <- 'xeqowhe22'\nstr_view(a, '\\\\d{2}')\nstr_view(a, '[xyz]')\nstr_detect(c('case','happy','sad'), '[ey]$')## [1]  TRUE  TRUE FALSE\nstr_extract_all(c('case','happy','sad'), '[ey]$')## [[1]]\n## [1] \"e\"\n## \n## [[2]]\n## [1] \"y\"\n## \n## [[3]]\n## character(0)\nstr_subset(c('case','happy','sad'), '[ey]$')## [1] \"case\"  \"happy\"\na <- 'laaalk3kr23'\nstr_count(a,'[al]{3}')## [1] 1\na <- 'laaalk3kr23'\nstr_replace(a,'[al]{3}','happy')## [1] \"happyalk3kr23\"\nstr_split('abc def',' ')## [[1]]\n## [1] \"abc\" \"def\""},{"path":"animating-the-plots-in-r.html","id":"animating-the-plots-in-r","chapter":"36 Animating the plots in R","heading":"36 Animating the plots in R","text":"SeokHyun Kim","code":""},{"path":"animating-the-plots-in-r.html","id":"introduction-4","chapter":"36 Animating the plots in R","heading":"36.1 Introduction","text":"gganimate interesting extension ggplot2 package create plots vivid animation R. community contribution project , ’ll show animate plots ggplot2 using strong features gganimate. can also customize graph change time using extension.","code":""},{"path":"animating-the-plots-in-r.html","id":"motivation-1","chapter":"36 Animating the plots in R","heading":"36.2 Motivation","text":"check url . amazing plots using gganimate!Links fancy fireworksCan imagine cool fireworks made using gganimate? Right saw plot website, truly impressed gganimate package can decided share great visualization package others!","code":""},{"path":"animating-the-plots-in-r.html","id":"load-required-packages","chapter":"36 Animating the plots in R","heading":"36.3 Load required packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(gganimate) # main package I'll cover\nlibrary(nord) # color palettes\nlibrary(gifski) # convert image frames to high quality GIF\nlibrary(viridis) # generate color maps\nlibrary(colorspace) # toolbox for selecting colors"},{"path":"animating-the-plots-in-r.html","id":"dataset","chapter":"36 Animating the plots in R","heading":"36.4 Dataset","text":"","code":"\n# I've created dataframe named df which contains speed, strength, win information of each person through year\nhead(df, 10)##    win strength speed year name\n## 1   23       32    17 1995 John\n## 2    2       66    24 1996 John\n## 3    8       57    23 1997 John\n## 4    9       22    25 1998 John\n## 5    6       17    16 1999 John\n## 6   17       25    20 2000 John\n## 7   13       87    21 2001 John\n## 8   21       32    24 2002 John\n## 9    5       65    18 2003 John\n## 10   9       49    21 2004 John\n# Below code is used for creating each features\n# win <- sample(x=1:30, size=27, replace=T)\n# strength  <- sample(x=1:100, size=27, replace=T)\n# speed <- sample(x=15:25, size=27, replace=T)\n# year <- seq(1995, 2021)"},{"path":"animating-the-plots-in-r.html","id":"understanding-transition_states-in-gganimate","chapter":"36 Animating the plots in R","heading":"36.5 Understanding transition_states() in gganimate","text":"transition_states() function gganimate package animates plot based specific variable. (Transition several distinct states data). specifying variable, basis, can obtain GIF image video representing transition time states.","code":""},{"path":"animating-the-plots-in-r.html","id":"transition_states-usage","chapter":"36 Animating the plots in R","heading":"36.5.1 transition_states() Usage","text":"transition_length : relative length transitionstate_length : relative length state","code":"\nggplot(dataframe, aes(x=variable1, y=variable2, ...))+\n  geom_graph(...)+\n  transition_states(variable3,\n                    transition_length=...,\n                    state_length=...)"},{"path":"animating-the-plots-in-r.html","id":"transition_states-examples","chapter":"36 Animating the plots in R","heading":"36.5.2 transition_states() Examples","text":"","code":"\n# Example 1 - barplot\n# Before adding transition_states()\nggplot(df, aes(x=name, y=win, fill=name))+\n  geom_col(show.legend=FALSE)+\n  scale_fill_nord('afternoon_prarie')+\n  theme_minimal()+\n  facet_wrap(~year)\n# Example 1 - barplot\n# After adding transition_states()\nggplot(df, aes(x=name, y=win, fill=name))+\n  geom_col(show.legend=FALSE)+\n  scale_fill_nord('afternoon_prarie')+\n  theme_minimal()+\n  transition_states(year,\n                    transition_length=1.5,\n                    state_length=0.5)\n# Example 2 - scatterplot\n# Before adding transition_states()\nggplot(df, aes(x=win, y=strength, color=name))+\n  geom_point(size=5, alpha=0.5)+\n  scale_color_viridis(option='plasma', discrete=TRUE)+\n  theme_minimal()+\n  theme(legend.position='bottom')\n# Example 2 - scatterplot\n# After adding transition_states()\nggplot(df, aes(x=win, y=strength, color=name))+\n  geom_point(size=5, alpha=0.5)+\n  scale_color_viridis(option='plasma', discrete=TRUE)+\n  theme_minimal()+\n  theme(legend.position='bottom')+\n  transition_states(year,\n                    transition_length=1.2,\n                    state_length=0.2)"},{"path":"animating-the-plots-in-r.html","id":"understanding-transition_reveal-in-gganimate","chapter":"36 Animating the plots in R","heading":"36.6 Understanding transition_reveal() in gganimate","text":"transition_reveal() function gganimate package can create animation data continuously displayed time visualizing given Time Series data plot.","code":""},{"path":"animating-the-plots-in-r.html","id":"transition_reveal-usage","chapter":"36 Animating the plots in R","heading":"36.6.1 transition_reveal() Usage","text":"","code":"\nggplot(dataframe, aes(x=variable1, y=variable2, group=variable3, ...))+\n  geom_line()+\n  geom_point()+\n  ... +\n  transition_reveal(variable1)"},{"path":"animating-the-plots-in-r.html","id":"transition_reveal-examples","chapter":"36 Animating the plots in R","heading":"36.6.2 transition_reveal() Examples","text":"","code":"\n# Before adding transition_reveal()\nggplot(df, aes(x=year, y=win, group=name, color=name))+\n  geom_line()+\n  geom_point()+\n  scale_color_discrete_sequential('Sunset')+\n  theme_minimal()+\n  theme(legend.position='bottom')\n# After adding transition_reveal()\nggplot(df, aes(x=year, y=win, group=name, color=name))+\n  geom_line()+\n  geom_point()+\n  scale_color_discrete_sequential('Sunset')+\n  theme_minimal()+\n  theme(legend.position='bottom')+\n  transition_reveal(year)"},{"path":"animating-the-plots-in-r.html","id":"understanding-transition_time-in-gganimate","chapter":"36 Animating the plots in R","heading":"36.7 Understanding transition_time() in gganimate","text":"transition_time() function gganimate package variant transition_states(), tool visualizing dataframe indicating state specific point animation plot.","code":""},{"path":"animating-the-plots-in-r.html","id":"transition_time-usage","chapter":"36 Animating the plots in R","heading":"36.7.1 transition_time() Usage","text":"length time switched states set proportional interval actual time states. Therefore, one best way visualize data changes time.","code":"\nggplot(dataframe, aes(variable1, variable2, ...), ...)+\n  geom_point(...)+\n  transition_time(variable3)"},{"path":"animating-the-plots-in-r.html","id":"transition_time-examples","chapter":"36 Animating the plots in R","heading":"36.7.2 transition_time() Examples","text":"","code":"\n# Before adding transition_time()\nggplot(df, aes(x=strength, y=win, color=name, size=speed))+\n  geom_point(alpha=0.7)+\n  scale_color_discrete_sequential('Purple-Yellow', rev=FALSE)+\n  scale_y_log10()+\n  scale_size(range=c(3,10))+\n  theme_minimal()+\n  theme(legend.position='bottom')\n# After adding transition_time()\nggplot(df, aes(x=strength, y=win, color=name, size=speed))+\n  geom_point(alpha=0.7)+\n  scale_color_discrete_sequential('Purple-Yellow', rev=FALSE)+\n  scale_y_log10()+\n  scale_size(range=c(3,10))+\n  theme_minimal()+\n  theme(legend.position='bottom')+\n  labs(title='Year: {frame_time}')+\n  transition_time(year)"},{"path":"animating-the-plots-in-r.html","id":"understanding-shadow_wake-in-gganimate","chapter":"36 Animating the plots in R","heading":"36.8 Understanding shadow_wake() in gganimate","text":"shadow_wake() function used transition_time() transition_reveal() shadow place changing data passed. can set gradually reduce size, color, transparency shadow.","code":""},{"path":"animating-the-plots-in-r.html","id":"shadow_wake-usage","chapter":"36 Animating the plots in R","heading":"36.8.1 shadow_wake() Usage","text":"length shadow can set relative ratio total length animation, frame.","code":"\nggplot(dataframe, aes(x=variable1, y=variable2, ...))+\n  geom_point()+\n  ...+\n  transition_함수(variable3)+\n  shadow_wake(wake_length=0.1, alpha=0)"},{"path":"animating-the-plots-in-r.html","id":"shadow_wake-examples","chapter":"36 Animating the plots in R","heading":"36.8.2 shadow_wake() Examples","text":"","code":"\n# Before adding shadow_wake()\nggplot(df)+\n  geom_point(aes(x=strength, y=win, size=speed, color=name))+\n  scale_color_viridis(option='viridis', discrete=TRUE)+\n  scale_x_log10()+\n  scale_size(range=c(1, 3))+\n  theme_minimal()+\n  theme(legend.position='bottom')\n# After adding shadow_wake()\nggplot(df)+\n  geom_point(aes(x=strength, y=win, size=speed, color=name))+\n  scale_color_viridis(option='viridis', discrete=TRUE)+\n  scale_x_log10()+\n  scale_size(range=c(1, 3))+\n  theme_minimal()+\n  theme(legend.position='bottom')+\n  transition_reveal(year)+\n  shadow_wake(wake_length=0.1,\n              alpha=0, \n              size=2)"},{"path":"animating-the-plots-in-r.html","id":"conclusion-2","chapter":"36 Animating the plots in R","heading":"36.9 Conclusion","text":"covered making animated barplot, scatterplot, timeseries also shadow using gganimate, just tip iceberg awesome package can . realized project flexibility gganimate. Even data, can generate lot different types plots also get image video file gganimate. next time, chance work project regarding gganimate , instead using standardized numeric data, ’ll challenge completely new way visualization, example fireworks. encourage dive deep gganimate !","code":""},{"path":"animating-the-plots-in-r.html","id":"citation","chapter":"36 Animating the plots in R","heading":"36.10 Citation","text":"https://www.data-imaginist.com/2019/gganimate--transitioned---state--release/https://www.data-imaginist.com/2019/gganimate--transitioned---state--release/https://gganimate.com/https://gganimate.com/","code":""},{"path":"quick-guide-to-eda.html","id":"quick-guide-to-eda","chapter":"37 Quick guide to eda","heading":"37 Quick guide to eda","text":"Vikram Singh Aniket ShahaneExploratory data analysis involves variety data techniques data used extract underlying information hidden nuances. Just looking data tables normal user understand much, exploratory data analysis techniques enable look data different perspective. look techniques categories.Link Notebook","code":""},{"path":"natural-langauge-processing-common-eda-practices-in-python.html","id":"natural-langauge-processing-common-eda-practices-in-python","chapter":"38 Natural Langauge Processing Common EDA Practices in Python","heading":"38 Natural Langauge Processing Common EDA Practices in Python","text":"Zhucheng Zhan, Yunchen YaoIn Natural Language Processing, exploratory data analysis important essential part training models. obtain adequate information text data visualization, like getting frequently used word quickly understand text , many .community contribution, introduce four major techniques commonly used understand text data give example implementing techniques python. take Twitter text data example illustrate common practices NLP EDA.Check tutorial using following link:https://github.com/skyyce/EDAV_CC/blob/main/EDA_in_NLP.ipynb","code":""},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"beginners-walk-through-of-deep-learning-in-r","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39 Beginner’s Walk-through of Deep Learning in R","text":"Dashansh Prajapati Vijay S Kalmath","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(raster)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"introduction-5","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.1 Introduction","text":"Deep Learning term whenever heard conversation, time accompanies word “Python”. shows high correlation exists Deep Learning Python. Indeed libraries Python like Keras, Tensorflow, Pytorch made possible users create robust deep learning applications. popularity libraries made Python de facto language comes Deep learning.R hand, language preferred statisticians, great capabilities dealing statistics visualizations. Deep Learning R, something unexpected recent years tensorflow introduced R, thereby extending capabilities R Deep Learning domain.tutorial aimed beginners get acquainted tensorflow build models R. Computer Vision Natural Language Processing two major domains Deep Learning. explore building text classification model image classification.Let’s see bulid text classification model first","code":""},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"text-classification---sentiment-analysis-with-r","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2 Text Classification - Sentiment Analysis with R","text":"Sentiment Analysis task finding associated sentiment text. nutshell ’s text classification task whose output sentiment.using IMDB dataset, can download link. contains 50k movie reviews, equal number positive negative reviews.","code":""},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-1-load-the-required-libraries","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2.1 Step 1: Load the required Libraries","text":"Follow steps link install tensorflow system.","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(keras)\nlibrary(tensorflow)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-2-read-and-explore-data","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2.2 Step 2: Read and explore data","text":"Check distribution sentiment columnThere many positive reviews negative ones, .e., equally distributed.","code":"\ndf <- read.csv(\"resources/deep_learning_with_r/imdb_dataset_one_fifth.csv\")\nstr(df)\ndf %>% count(sentiment)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-3-split-the-data-into-training-80-and-testing-20-datasets.","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2.3 Step 3: Split the data into training (80%) and testing (20%) datasets.","text":"Let’s find many words reviews get summary statistics . later useful setting parameters text vectorization functionGiven max 2470 words sentence 3rd quantile 280 words, limit number words 300. sentences less 300 words padded zeros greater 300 truncated contain 300 words.arbitrarily fix vocabulary 10000 words.","code":"\nids <- sample.int(nrow(df), size = nrow(df)*0.8)\ntrain <- df[ids,]\ntest <- df[-ids,]\ndf$review %>% \n  strsplit(\" \") %>% \n  sapply(length) %>% \n  summary()"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-4-data-preparation","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2.4 Step 4: Data Preparation","text":"model works tensors, hence need encode data tensors first. ? using text vectorization layer assign integers 10,000 common words. integers used place words represent input sequence.Furthermore need encode word separately. use embedding layer . embedding layer maps integer fixed size array encoded version integer.Let’s define text vectorization layer.need adapt text vectorization layer input data, built fix sized vocabulary assign integer value words .check vocabulary, uncomment following code run .Following example layer transforms input.","code":"\nnum_words <- 10000\nmax_length <- 300\n\ntext_vectorization <- layer_text_vectorization(\n  max_tokens = num_words, \n  output_sequence_length = max_length, \n)\ntext_vectorization %>% \n  adapt(df$review)\n# get_vocabulary(text_vectorization)\ntext_vectorization(matrix(df$review[1], ncol = 1))"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-5-build-train-a-model","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2.5 Step 5: Build, train a model","text":"Using keras_model function create following model.Define loss functiona optimizer, compile model.Train model.","code":"\ninput <- layer_input(shape = c(1), dtype = \"string\")\n\noutput <- input %>% \n  text_vectorization() %>% \n  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(units = 16, activation = \"relu\") %>%\n  layer_dropout(0.5) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'binary_crossentropy',\n  metrics = list('accuracy')\n)\nhistory <- model %>% fit(\n  train$review,\n  as.numeric(train$sentiment == \"positive\"),\n  epochs = 10,\n  batch_size = 512,\n  validation_split = 0.2,\n  verbose=2\n)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-6-evaluate-the-model","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.2.6 Step 6: Evaluate the model:","text":"Let’s see model performs unseen data?Plot training vs validation accuracy loss.","code":"\nresults <- model %>% evaluate(test$review, as.numeric(test$sentiment == \"pos\"), verbose = 0)\nresults\nplot(history)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"image-classification-using-r-for-cifar10","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3 Image Classification using R for CIFAR10","text":"Contextual image classification pattern recognition problem used field computer vision used classify object context image using machine AI importantly deep learining.Deep learning subset machine learning, essentially neural network three layers. neural networks attempt simulate behavior human brain—albeit far matching ability—allowing “learn” large amounts data. - linkIn walkthrough Deep learning R , working CIFAR10 Dataset. CIFAR10 Dataset consists 60000 rbg images 10 classes.10 Classes present CIFAR10 dataset \n1.airplane\n2.automobile\n3.bird\n4.cat\n5.deer\n6.dog\n7.frog\n8.horse\n9.ship\n10.truckThe CIFAR10 images tiny nature , 32 x 32 pixels wide 3 Channels colorsMore Information CIFAR10 can found link","code":""},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-1-load-the-required-libraries-1","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.1 Step 1: Load the required Libraries","text":"","code":"\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(raster)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-1a-to-make-sure-that-your-tensorflow-is-installed-correctly-run","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.2 Step 1a : To Make sure that your tensorflow is installed correctly run :","text":"","code":"\ntf$constant(\"We are building CNNs in R !!\")"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-2-explore-the-data","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.3 Step 2: Explore the Data","text":"CIFAR10 data split 2 components, training data test data. training data 50,000 Images test data 10,000 Images.CIFAR10 dataset packaged Tensorflow Keras can downloaded directly using tensorflow APIs.?dataset_cifar10 information dataset.","code":"\ncifar10_dataset <- dataset_cifar10()"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-3-split-the-data-into-training-80-and-testing-20-datasets.-1","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.4 Step 3: Split the data into training (80%) and testing (20%) datasets.","text":"standard Deep Learning , two main aspects model creation , one supervised learning part wherein model fed input output model optimized latter prediction evaluation images never seen model .","code":"\nc(train_images, train_labels) %<-% cifar10_dataset$train\nc(test_images, test_labels) %<-% cifar10_dataset$test\n\n# Image Data Preprocessing \n\ntrain_images = train_images/255\ntest_images = test_images/255"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-3a-to-see-the-dimensions-of-the-train_images-and-the-test_images-dataset-use-the-dims-function.","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.5 Step 3a : To see the dimensions of the train_images and the test_images dataset , use the dims function.","text":"","code":"\ndim(train_images)\n\ndim(train_labels)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-4-as-the-dataset-labels-are-just-integers-it-is-crucial-we-define-the-label-array.","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.6 Step 4 : As the dataset Labels are just integers , It is crucial we define the label array.","text":"","code":"\nclass_names = c('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')                                        "},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-5-let-us-plot-these-tiny-images","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.7 Step 5 : Let us plot these tiny images !!","text":"order understand images look like feeding model , let us build function plot RBG image array training images.can see plot_image just 3D array size (32X32x3)Since small images (32 pixels ) , images blurry plotting.","code":"\n# Taking a random image from the training data , the leading comma's are important as we need to the entire 3D matrix.\n\nrandom_image = train_images[10,,,]\n\ndim(random_image)\n#Defining Function PlotImage\n\nplotImage <- function(image){\n  \n  plot(raster::as.raster(image))\n  \n}\n\nplotImage(random_image)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-6-build-a-model","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.8 Step 6 : Build a model","text":"Keras Sequential Model Object instantiated hold layers add CNN. Convolution Layers special layers can directly fed images need image data manipulation aside image processing better results.construction convolutional neural network multi-layered feed-forward neural network, made assembling many unseen layers top particular order.Let us know build Model Convolutional Layers Deep Learning layers image classification.Information layers add model can found . linkWe can see summary Layers build model using command :","code":"\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filters=32,kernel_size = 3, padding=\"same\",input_shape = c(32, 32,3), activation = 'relu') %>% \n  layer_batch_normalization() %>% \n  layer_conv_2d(filters=32,kernel_size = 3, padding=\"same\",activation = 'relu') %>% \n  layer_batch_normalization() %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_dropout(0.2) %>% \n  layer_conv_2d(filters=16,kernel_size = 3, padding=\"same\",activation = 'relu') %>% \n  layer_batch_normalization() %>%\n  layer_conv_2d(filters=16,kernel_size = 3, padding=\"same\",activation = 'relu') %>% \n  layer_batch_normalization() %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_dropout(0.3) %>% \n  layer_flatten() %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  \n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-7-compile-the-model","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.9 Step 7 : Compile the Model","text":"order start training model , need define loss functions , accuracy measurement techniques optimizers.Optimizers Information can found linkLoss functions Information can found link","code":"\nmodel %>% compile(\n  optimizer = 'adam', \n  loss = 'sparse_categorical_crossentropy',\n  metrics = c('accuracy')\n)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-8-time-to-train-the-model-.","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.10 Step 8 : Time to train the model .","text":"Fit function trains model given training testing images , Number epochs define long model trained many images.","code":"\nmodel_run  <- model %>% fit(train_images, train_labels, epochs = 10, verbose = 2)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-8a-let-us-plot-the-model-loss-and-accuracy-functions.","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.11 Step 8a : Let us plot the model Loss and Accuracy Functions.","text":"","code":"\nplot(model_run)"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-9-evaluate-the-model","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.12 Step 9: Evaluate the model:","text":"Let’s see model performs unseen data?","code":"\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\n\nscore"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-10-let-us-predict-some-images","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.13 Step 10: Let us predict some images","text":"","code":"\nimg <- test_images[3, , , , drop = FALSE]\nplotImage(img[1,,,])\nclass_pred <- model %>% predict(img)\nclass_id <- which.max(class_pred)\nclass_names[class_id]\n\n\n\nimg <- test_images[15, , , , drop = FALSE]\nplotImage(img[1,,,])\nclass_pred <- model %>% predict(img)\nclass_id <- which.max(class_pred)\nclass_names[class_id]\n\n\n\n\nimg <- test_images[100, , , , drop = FALSE]\nplotImage(img[1,,,])\nclass_pred <- model %>% predict(img)\nclass_id <- which.max(class_pred)\nclass_names[class_id]\n\n\n\n\nimg <- test_images[500, , , , drop = FALSE]\nplotImage(img[1,,,])\nclass_pred <- model %>% predict(img)\nclass_id <- which.max(class_pred)\nclass_names[class_id]\n\n\n\n\nimg <- test_images[1200, , , , drop = FALSE]\nplotImage(img[1,,,])\nclass_pred <- model %>% predict(img)\nclass_id <- which.max(class_pred)\nclass_names[class_id]"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"step-11-these-models-can-be-stored-in-the-standard-keras-hd5-format-and-are-transferrable-to-any-other-environment-like-python-or-even-java","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.3.14 Step 11: These models can be stored in the standard Keras HD5 Format and are transferrable to any other environment like Python or even Java !!","text":"Loading saved models easy well !!","code":"\nmodel %>% save_model_hdf5(\"CIFAR10-CNN-Model\")\nnew_model <- load_model_hdf5(\"CIFAR10-CNN-Model\")"},{"path":"beginners-walk-through-of-deep-learning-in-r.html","id":"references","chapter":"39 Beginner’s Walk-through of Deep Learning in R","heading":"39.4 References:","text":"Tensorflow R R studioCIFAR10 DataRaster ImageKeras ModelsKeras OptimizersImage Plotting","code":""},{"path":"supervised-learning-tutorial.html","id":"supervised-learning-tutorial","chapter":"40 Supervised Learning Tutorial","heading":"40 Supervised Learning Tutorial","text":"Sung Jun Wonlink caret package guide:\nhttp://topepo.github.io/caret/index.htmlIn tutorial, introducing caret, R package supports various machine learning tools. mainly focus introducing supervised learning.important steps supervised learning.\n1. Data preprocessing\n2. Data splitting\n3. Model selection\n4. Training\n5. EvaluationLet’s walk steps one one.Data preprocessingTo go data preprocessing step, using schedulingData data AppliedPredictiveModeling library.dataset may features one unique value, unique values occur low frequency. Model trained dataset may crash fit well.\nresolve issue, can use nearZeroVar identify features remove features.models can’t take categorical features (linear regression, logistic regression, etc), need convert dummy variables using -called One-Hot Encoding.can see categorical values, column made values 0 1.preprocess features needed, caret supports amazing function called preProcess, decides whatever preprocessings needs perform dataset apply accordingly.\nnumerical features, can apply centering scaling normalize dataset.Data SplittingWe can split data train & test.\ncan use createDataPartition function create balanced split train test.\nuse Sonar data mlbench package example.setting p = 0.8, can split data 8:2 ratio, 8 train 2 test.\nwant matrix output list = FALSE, since splitting two, partition times = 1.Model Selectioncaret provides trainControl function, can change cross validation methods, parameters tuning, number folds k-fold CV, number resampling, etc. Depending classification regression model decide use, can set parameters fit tune model optimal.instance, want use k-fold cross validation, number folds = 10 number resampling = 10, can set parameters :python, hyperparameter tuning find best performing model. R, function train default uses best, basically means chooses parameters shows best accuracy. (lowest RMSE)==> best(testing, “RMSE”, maximize = FALSE)two choices besides best, oneSE tolerance.oneSE aims find simplest model within one standard error ideally optimal model.==> oneSE(testing, “RMSE”, maximize = FALSE, num = 10)tolerance attempts find less complex model falls within percent tolerance ideally optimal model.==> tolerance(testing, “RMSE”, tol = 3, maximize = FALSE)idea behind using oneSE tolerance ordering models simple complex, many cases, models complicated order one next another. important use two methods ordering model complexity deemed right.TrainingWe can use cross validation parameters chosen train model.\nnumerous classification regression models can use fit data.list , \n“treebag” “logicbag” Bagged Trees,\n“rf” Random Forest,\n“adaboost” “gbm” Boosted Trees,\n“lm” Linear Regression,\n“logreg” Logistic Regression,\ncourse, numerous variations models regularization .show example, can see Random Forest model fitting training data, 10-fold cross validation choosing model best performance.EvaluationUsing postResample function, can get performance model test data. Random Forest, can get prediction accuracy Kappa, compares observed accuracy expected accuracy.can also get confusion matrix test data using confusionMatrix function. shows accuracy scores well p-value evaluation metrics. get precision, recall, F1 score, can set mode “prec_recall”.far, seen basics supervised learning.\nlink provided , details step well steps consider feature selection, calibration, etc.hope guys liked tutorial!","code":"\nlibrary(ggplot2)\nlibrary(lattice)\nlibrary(caret)\nlibrary(AppliedPredictiveModeling)\nlibrary(mlbench)\nlibrary(MLmetrics)\ndata(schedulingData)\nstr(schedulingData)## 'data.frame':    4331 obs. of  8 variables:\n##  $ Protocol   : Factor w/ 14 levels \"A\",\"C\",\"D\",\"E\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ Compounds  : num  997 97 101 93 100 100 105 98 101 95 ...\n##  $ InputFields: num  137 103 75 76 82 82 88 95 91 92 ...\n##  $ Iterations : num  20 20 10 20 20 20 20 20 20 20 ...\n##  $ NumPending : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hour       : num  14 13.8 13.8 10.1 10.4 ...\n##  $ Day        : Factor w/ 7 levels \"Mon\",\"Tue\",\"Wed\",..: 2 2 4 5 5 3 5 5 5 3 ...\n##  $ Class      : Factor w/ 4 levels \"VF\",\"F\",\"M\",\"L\": 2 1 1 1 1 1 1 1 1 1 ...\nnzv <- nearZeroVar(schedulingData)\nfilteredDescr <- schedulingData[, -nzv]\nfilteredDescr[1:10,]##    Protocol Compounds InputFields Iterations     Hour Day Class\n## 1         E       997         137         20 14.00000 Tue     F\n## 2         E        97         103         20 13.81667 Tue    VF\n## 3         E       101          75         10 13.85000 Thu    VF\n## 4         E        93          76         20 10.10000 Fri    VF\n## 5         E       100          82         20 10.36667 Fri    VF\n## 6         E       100          82         20 16.53333 Wed    VF\n## 7         E       105          88         20 16.40000 Fri    VF\n## 8         E        98          95         20 16.73333 Fri    VF\n## 9         E       101          91         20 16.18333 Fri    VF\n## 10        E        95          92         20 10.78333 Wed    VF\nstr(filteredDescr)## 'data.frame':    4331 obs. of  7 variables:\n##  $ Protocol   : Factor w/ 14 levels \"A\",\"C\",\"D\",\"E\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ Compounds  : num  997 97 101 93 100 100 105 98 101 95 ...\n##  $ InputFields: num  137 103 75 76 82 82 88 95 91 92 ...\n##  $ Iterations : num  20 20 10 20 20 20 20 20 20 20 ...\n##  $ Hour       : num  14 13.8 13.8 10.1 10.4 ...\n##  $ Day        : Factor w/ 7 levels \"Mon\",\"Tue\",\"Wed\",..: 2 2 4 5 5 3 5 5 5 3 ...\n##  $ Class      : Factor w/ 4 levels \"VF\",\"F\",\"M\",\"L\": 2 1 1 1 1 1 1 1 1 1 ...\ndummies <- dummyVars(\"~.\", data = filteredDescr[,-7])\nencodedData <- data.frame(predict(dummies, newdata = filteredDescr))\nhead(encodedData)##   Protocol.A Protocol.C Protocol.D Protocol.E Protocol.F Protocol.G Protocol.H\n## 1          0          0          0          1          0          0          0\n## 2          0          0          0          1          0          0          0\n## 3          0          0          0          1          0          0          0\n## 4          0          0          0          1          0          0          0\n## 5          0          0          0          1          0          0          0\n## 6          0          0          0          1          0          0          0\n##   Protocol.I Protocol.J Protocol.K Protocol.L Protocol.M Protocol.N Protocol.O\n## 1          0          0          0          0          0          0          0\n## 2          0          0          0          0          0          0          0\n## 3          0          0          0          0          0          0          0\n## 4          0          0          0          0          0          0          0\n## 5          0          0          0          0          0          0          0\n## 6          0          0          0          0          0          0          0\n##   Compounds InputFields Iterations     Hour Day.Mon Day.Tue Day.Wed Day.Thu\n## 1       997         137         20 14.00000       0       1       0       0\n## 2        97         103         20 13.81667       0       1       0       0\n## 3       101          75         10 13.85000       0       0       0       1\n## 4        93          76         20 10.10000       0       0       0       0\n## 5       100          82         20 10.36667       0       0       0       0\n## 6       100          82         20 16.53333       0       0       1       0\n##   Day.Fri Day.Sat Day.Sun\n## 1       0       0       0\n## 2       0       0       0\n## 3       0       0       0\n## 4       1       0       0\n## 5       1       0       0\n## 6       0       0       0\npp_hpc <- preProcess(encodedData, \n                     method = c(\"center\", \"scale\", \"YeoJohnson\"))\ntransformed <- predict(pp_hpc, newdata = encodedData)\nhead(transformed)##   Protocol.A Protocol.C Protocol.D Protocol.E Protocol.F Protocol.G Protocol.H\n## 1 -0.1489308 -0.1958347 -0.1887344   6.641114 -0.2021043 -0.1926351 -0.2828982\n## 2 -0.1489308 -0.1958347 -0.1887344   6.641114 -0.2021043 -0.1926351 -0.2828982\n## 3 -0.1489308 -0.1958347 -0.1887344   6.641114 -0.2021043 -0.1926351 -0.2828982\n## 4 -0.1489308 -0.1958347 -0.1887344   6.641114 -0.2021043 -0.1926351 -0.2828982\n## 5 -0.1489308 -0.1958347 -0.1887344   6.641114 -0.2021043 -0.1926351 -0.2828982\n## 6 -0.1489308 -0.1958347 -0.1887344   6.641114 -0.2021043 -0.1926351 -0.2828982\n##   Protocol.I Protocol.J  Protocol.K Protocol.L Protocol.M Protocol.N Protocol.O\n## 1 -0.3105373 -0.5439322 -0.03724195 -0.2432478 -0.3408963 -0.3757737 -0.3935703\n## 2 -0.3105373 -0.5439322 -0.03724195 -0.2432478 -0.3408963 -0.3757737 -0.3935703\n## 3 -0.3105373 -0.5439322 -0.03724195 -0.2432478 -0.3408963 -0.3757737 -0.3935703\n## 4 -0.3105373 -0.5439322 -0.03724195 -0.2432478 -0.3408963 -0.3757737 -0.3935703\n## 5 -0.3105373 -0.5439322 -0.03724195 -0.2432478 -0.3408963 -0.3757737 -0.3935703\n## 6 -0.3105373 -0.5439322 -0.03724195 -0.2432478 -0.3408963 -0.3757737 -0.3935703\n##    Compounds InputFields Iterations         Hour    Day.Mon   Day.Tue\n## 1  1.2289592  -0.6324580 -0.0615593  0.004586516 -0.4360255  1.952266\n## 2 -0.6065826  -0.8120473 -0.0615593 -0.043733201 -0.4360255  1.952266\n## 3 -0.5719534  -1.0131504 -2.7894869 -0.034967177 -0.4360255 -0.512107\n## 4 -0.6427737  -1.0047277 -0.0615593 -0.964170752 -0.4360255 -0.512107\n## 5 -0.5804713  -0.9564504 -0.0615593 -0.902085020 -0.4360255 -0.512107\n## 6 -0.5804713  -0.9564504 -0.0615593  0.698108782 -0.4360255 -0.512107\n##      Day.Wed    Day.Thu    Day.Fri     Day.Sat    Day.Sun\n## 1 -0.5131843 -0.4464804 -0.5203564 -0.08626629 -0.1964693\n## 2 -0.5131843 -0.4464804 -0.5203564 -0.08626629 -0.1964693\n## 3 -0.5131843  2.2392230 -0.5203564 -0.08626629 -0.1964693\n## 4 -0.5131843 -0.4464804  1.9213160 -0.08626629 -0.1964693\n## 5 -0.5131843 -0.4464804  1.9213160 -0.08626629 -0.1964693\n## 6  1.9481679 -0.4464804 -0.5203564 -0.08626629 -0.1964693\ndata(Sonar)\nstr(Sonar)## 'data.frame':    208 obs. of  61 variables:\n##  $ V1   : num  0.02 0.0453 0.0262 0.01 0.0762 0.0286 0.0317 0.0519 0.0223 0.0164 ...\n##  $ V2   : num  0.0371 0.0523 0.0582 0.0171 0.0666 0.0453 0.0956 0.0548 0.0375 0.0173 ...\n##  $ V3   : num  0.0428 0.0843 0.1099 0.0623 0.0481 ...\n##  $ V4   : num  0.0207 0.0689 0.1083 0.0205 0.0394 ...\n##  $ V5   : num  0.0954 0.1183 0.0974 0.0205 0.059 ...\n##  $ V6   : num  0.0986 0.2583 0.228 0.0368 0.0649 ...\n##  $ V7   : num  0.154 0.216 0.243 0.11 0.121 ...\n##  $ V8   : num  0.16 0.348 0.377 0.128 0.247 ...\n##  $ V9   : num  0.3109 0.3337 0.5598 0.0598 0.3564 ...\n##  $ V10  : num  0.211 0.287 0.619 0.126 0.446 ...\n##  $ V11  : num  0.1609 0.4918 0.6333 0.0881 0.4152 ...\n##  $ V12  : num  0.158 0.655 0.706 0.199 0.395 ...\n##  $ V13  : num  0.2238 0.6919 0.5544 0.0184 0.4256 ...\n##  $ V14  : num  0.0645 0.7797 0.532 0.2261 0.4135 ...\n##  $ V15  : num  0.066 0.746 0.648 0.173 0.453 ...\n##  $ V16  : num  0.227 0.944 0.693 0.213 0.533 ...\n##  $ V17  : num  0.31 1 0.6759 0.0693 0.7306 ...\n##  $ V18  : num  0.3 0.887 0.755 0.228 0.619 ...\n##  $ V19  : num  0.508 0.802 0.893 0.406 0.203 ...\n##  $ V20  : num  0.48 0.782 0.862 0.397 0.464 ...\n##  $ V21  : num  0.578 0.521 0.797 0.274 0.415 ...\n##  $ V22  : num  0.507 0.405 0.674 0.369 0.429 ...\n##  $ V23  : num  0.433 0.396 0.429 0.556 0.573 ...\n##  $ V24  : num  0.555 0.391 0.365 0.485 0.54 ...\n##  $ V25  : num  0.671 0.325 0.533 0.314 0.316 ...\n##  $ V26  : num  0.641 0.32 0.241 0.533 0.229 ...\n##  $ V27  : num  0.71 0.327 0.507 0.526 0.7 ...\n##  $ V28  : num  0.808 0.277 0.853 0.252 1 ...\n##  $ V29  : num  0.679 0.442 0.604 0.209 0.726 ...\n##  $ V30  : num  0.386 0.203 0.851 0.356 0.472 ...\n##  $ V31  : num  0.131 0.379 0.851 0.626 0.51 ...\n##  $ V32  : num  0.26 0.295 0.504 0.734 0.546 ...\n##  $ V33  : num  0.512 0.198 0.186 0.612 0.288 ...\n##  $ V34  : num  0.7547 0.2341 0.2709 0.3497 0.0981 ...\n##  $ V35  : num  0.854 0.131 0.423 0.395 0.195 ...\n##  $ V36  : num  0.851 0.418 0.304 0.301 0.418 ...\n##  $ V37  : num  0.669 0.384 0.612 0.541 0.46 ...\n##  $ V38  : num  0.61 0.106 0.676 0.881 0.322 ...\n##  $ V39  : num  0.494 0.184 0.537 0.986 0.283 ...\n##  $ V40  : num  0.274 0.197 0.472 0.917 0.243 ...\n##  $ V41  : num  0.051 0.167 0.465 0.612 0.198 ...\n##  $ V42  : num  0.2834 0.0583 0.2587 0.5006 0.2444 ...\n##  $ V43  : num  0.282 0.14 0.213 0.321 0.185 ...\n##  $ V44  : num  0.4256 0.1628 0.2222 0.3202 0.0841 ...\n##  $ V45  : num  0.2641 0.0621 0.2111 0.4295 0.0692 ...\n##  $ V46  : num  0.1386 0.0203 0.0176 0.3654 0.0528 ...\n##  $ V47  : num  0.1051 0.053 0.1348 0.2655 0.0357 ...\n##  $ V48  : num  0.1343 0.0742 0.0744 0.1576 0.0085 ...\n##  $ V49  : num  0.0383 0.0409 0.013 0.0681 0.023 0.0264 0.0507 0.0285 0.0777 0.0092 ...\n##  $ V50  : num  0.0324 0.0061 0.0106 0.0294 0.0046 0.0081 0.0159 0.0178 0.0439 0.0198 ...\n##  $ V51  : num  0.0232 0.0125 0.0033 0.0241 0.0156 0.0104 0.0195 0.0052 0.0061 0.0118 ...\n##  $ V52  : num  0.0027 0.0084 0.0232 0.0121 0.0031 0.0045 0.0201 0.0081 0.0145 0.009 ...\n##  $ V53  : num  0.0065 0.0089 0.0166 0.0036 0.0054 0.0014 0.0248 0.012 0.0128 0.0223 ...\n##  $ V54  : num  0.0159 0.0048 0.0095 0.015 0.0105 0.0038 0.0131 0.0045 0.0145 0.0179 ...\n##  $ V55  : num  0.0072 0.0094 0.018 0.0085 0.011 0.0013 0.007 0.0121 0.0058 0.0084 ...\n##  $ V56  : num  0.0167 0.0191 0.0244 0.0073 0.0015 0.0089 0.0138 0.0097 0.0049 0.0068 ...\n##  $ V57  : num  0.018 0.014 0.0316 0.005 0.0072 0.0057 0.0092 0.0085 0.0065 0.0032 ...\n##  $ V58  : num  0.0084 0.0049 0.0164 0.0044 0.0048 0.0027 0.0143 0.0047 0.0093 0.0035 ...\n##  $ V59  : num  0.009 0.0052 0.0095 0.004 0.0107 0.0051 0.0036 0.0048 0.0059 0.0056 ...\n##  $ V60  : num  0.0032 0.0044 0.0078 0.0117 0.0094 0.0062 0.0103 0.0053 0.0022 0.004 ...\n##  $ Class: Factor w/ 2 levels \"M\",\"R\": 2 2 2 2 2 2 2 2 2 2 ...\ninTraining <- createDataPartition(Sonar$Class, p = .8, list = FALSE, times = 1)\ntraining <- Sonar[ inTraining,]\ntesting  <- Sonar[-inTraining,]\nfitControl <- trainControl(## 10-fold CV\n                           method = \"repeatedcv\",\n                           number = 10,\n                           ## repeated ten times\n                           repeats = 10)\nrfFit <- train(Class ~ ., data = training,\n                 method = \"rf\", \n                 trControl = fitControl)\npred <- predict(rfFit, testing)\n\npostResample(pred = pred, obs = testing$Class)##  Accuracy     Kappa \n## 0.8780488 0.7521161\nconfusionMatrix(data = pred, reference = testing$Class, mode = \"prec_recall\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction  M  R\n##          M 21  4\n##          R  1 15\n##                                          \n##                Accuracy : 0.878          \n##                  95% CI : (0.738, 0.9592)\n##     No Information Rate : 0.5366         \n##     P-Value [Acc > NIR] : 3.487e-06      \n##                                          \n##                   Kappa : 0.7521         \n##                                          \n##  Mcnemar's Test P-Value : 0.3711         \n##                                          \n##               Precision : 0.8400         \n##                  Recall : 0.9545         \n##                      F1 : 0.8936         \n##              Prevalence : 0.5366         \n##          Detection Rate : 0.5122         \n##    Detection Prevalence : 0.6098         \n##       Balanced Accuracy : 0.8720         \n##                                          \n##        'Positive' Class : M              \n## "},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"parallel-coordinate-plots-in-ggplot2","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41 Parallel Coordinate Plots in ggplot2","text":"Amrutha Varshini Sundar","code":""},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"introduction-6","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.1 Introduction","text":"parallel coordinate plot used visualizing multivariate numerical data. plots facilitate comparing multiple numeric variables different scales different units measurement. abets finding patterns correlations datasets high dimensional.plot consists X-axis represents different variable corresponding point along Y-axis represents magnitude value variable. datapoint connected series line segments across variables.ubiquitous implementation parallel coordinate plots provided ggparcoord function within GGally package. document gives walkthrough plotting parallel coordinate plots functionalities associated using ggplot2 package.","code":""},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"importing-necessary-packages","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.1.1 Importing necessary packages","text":"","code":"\n# packages we need to implement parallel coordinate plots\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\n\n# Used only for demonstration and comparison purposes\nlibrary(GGally)\nlibrary(ggpubr)"},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"plotting-a-sample-parallel-coordinate-plot-using-ggparcoord","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.2 Plotting a sample parallel coordinate plot using ggparcoord","text":"dataset use plotting parallel coordinate plots Wine Quality Data Set UCI ML Repository. specifically extracting 12 different attributes white wine across 100 different wine samples presenting parallel coordinate plot .","code":"\n# read the dataset\nwinequality <- read.csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep = \";\")[0:100, ]\n\nprint(head(winequality))##   fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n## 1           7.0             0.27        0.36           20.7     0.045\n## 2           6.3             0.30        0.34            1.6     0.049\n## 3           8.1             0.28        0.40            6.9     0.050\n## 4           7.2             0.23        0.32            8.5     0.058\n## 5           7.2             0.23        0.32            8.5     0.058\n## 6           8.1             0.28        0.40            6.9     0.050\n##   free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n## 1                  45                  170  1.0010 3.00      0.45     8.8\n## 2                  14                  132  0.9940 3.30      0.49     9.5\n## 3                  30                   97  0.9951 3.26      0.44    10.1\n## 4                  47                  186  0.9956 3.19      0.40     9.9\n## 5                  47                  186  0.9956 3.19      0.40     9.9\n## 6                  30                   97  0.9951 3.26      0.44    10.1\n##   quality\n## 1       6\n## 2       6\n## 3       6\n## 4       6\n## 5       6\n## 6       6\n# creating a dummy wine type category as the first column \nwinequality = winequality %>% add_column(WineType = c(sub(\"^\",\"Class \", 1:100)), .before=\"fixed.acidity\")\n\n# plotting with basic ggparcoord from GGally\nggally_par_coord <- ggparcoord(winequality, columns = 2:13, scale = 'globalminmax') +\n                      xlab(\"Wine Attributes\") +\n                      ylab(\"Value\") \n\nggally_par_coord +\n  ggtitle(\"Parallel coordinates plot showing the wine attributes for different wines\") +\n    theme(plot.title = element_text(size = 15, face = \"bold\"))"},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"custom-function-to-implement-a-similar-functionality-for-presenting-parallel-coordinate-plots-using-ggplot2","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.3 Custom function to implement a similar functionality for presenting parallel coordinate plots using ggplot2","text":"function geom_parcoord implementation present parallel coordinate plots using ggplot2. function accepts following parameters,df_wide - unpivoted dataframe multidimensional variables present separate columns dataframedf_wide - unpivoted dataframe multidimensional variables present separate columns dataframecolumns - Indicates columns variables dataframe.\nDefault behavior - NULL. Assumes first column group column represents series line segment single entity.columns - Indicates columns variables dataframe.Default behavior - NULL. Assumes first column group column represents series line segment single entity.spline_factor - Controls level smoothness sequential line segments.\nDefault behavior - 1. indicates smoothing. Higher value, higher smoothing.spline_factor - Controls level smoothness sequential line segments.Default behavior - 1. indicates smoothing. Higher value, higher smoothing.standardize - Boolean indicate whether variables scaled plotting . recommended helps better analysis patterns plotted comparable scales. uses scale function provided base R.\nDefault behavior - Tstandardize - Boolean indicate whether variables scaled plotting . recommended helps better analysis patterns plotted comparable scales. uses scale function provided base R.Default behavior - T","code":"\nsmooth_df_group <- function(dt, spline_factor, groupvars){\n  data.frame(spline(as.factor(dt$category), dt$values, n=12*groupvars))\n}\n\n# Custom function to plot \ngeom_parcoord <- function(df_wide, columns=NULL, spline_factor = 1, standardize = T) {\n  \n  # Assuming the first column is going to be the column to be grouped\n  group_by_col = 1\n  columns = NULL\n  \n  # normalize data\n  if(standardize) {\n    df = df_wide[(group_by_col+1):length(colnames(df_wide))] %>% mutate_all(scale)\n    df = df %>% add_column(WineType = c(sub(\"^\",\"Class \", 1:100)), .before=\"fixed.acidity\")\n    df_wide = df\n  }\n  \n  # If columns are not provided, then set defaults\n  if(is.null(columns)) {\n    columns = (group_by_col+1):length(colnames(df_wide))\n  }\n  \n  # pivot the dataframe to convert to the long format\n  df_long <- pivot_longer(df_wide, cols = columns, names_to = \"category\", values_to = \"values\")\n  \n  # group by using the group_by column and apply appropriate level of smoothing\n  final_df <- df_long %>%\n   group_by_at(colnames(df_wide)[group_by_col]) %>%\n   group_modify(smooth_df_group, spline_factor)\n  \n  # Plot using geom_line\n  ggplot(filter(final_df)) + \n    geom_line(aes_string(x = \"x\", y = \"y\", group=colnames(df_wide)[group_by_col])) +\n    ylab('Values') +\n    scale_x_continuous(name=\"Category\", limits=c(1, length(columns)), labels=sort(colnames(df_wide)[columns]), breaks=c(1:length(columns))) \n}"},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"plotting-parallel-coordinate-plots-using-custom-function---geom_parcoord","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.4 Plotting parallel coordinate plots using custom function - geom_parcoord","text":"plot simple parcoord plot using default behavior geom_parcoord. level smoothing values scaled.","code":"\ncustom_parcoord = geom_parcoord(winequality, standardize = F)\ncustom_parcoord +\n  ggtitle(\"Parallel coordinate plot of White Wine Quality Attributes using geom_parcoord\") + \n    theme(plot.title = element_text(size = 15, face = \"bold\"))"},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"playing-around-with-some-parameters-of-geom_parcoord","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.4.1 Playing around with some parameters of geom_parcoord","text":"Tweaking parameters smoothing factor scaling plot appropriately","code":"\ngeom_parcoord(winequality, spline_factor = 20, standardize = T) + \n  ggtitle(\"Parallel coordinate plot of White Wine Quality Attributes using geom_parcoord(spline_factor=20, standardize=T)\") + \n    theme(plot.title = element_text(size = 15, face = \"bold\"))"},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"summary-and-comparisons","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.5 Summary and Comparisons","text":"shows side side comparison plots obtained ggparcoord custom geom_parcoord functions.","code":"\nggarrange(ggally_par_coord, custom_parcoord, nrow=2, labels = c(\"ggparcoord\", \"geom_parcoord\")) "},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"observations","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.5.1 Observations","text":"outset plots may look similar, differences respect visuals implementation,One main observation ordering dimensions along X-axis. can see ggparcoord retains order variables dataset, custom geom_parcoord variables plotted alphabetical order.One main observation ordering dimensions along X-axis. can see ggparcoord retains order variables dataset, custom geom_parcoord variables plotted alphabetical order.ggparcoord several functionalities different methods scaling like ‘uniminmax’, ‘globalminmax’ custom function implementations extent base R scale function implementation.ggparcoord several functionalities different methods scaling like ‘uniminmax’, ‘globalminmax’ custom function implementations extent base R scale function implementation.spline functionality might differ two depending method used splining ggparcoord.spline functionality might differ two depending method used splining ggparcoord.","code":""},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"improvements-and-future-work","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.6 Improvements and future work","text":"Adding variation different methods scaling support much normal base R scaling.Adding variation different methods scaling support much normal base R scaling.Giving option retain order variables given dataframe instead ordering alphabetically.Giving option retain order variables given dataframe instead ordering alphabetically.Adding ggplot2 geom parallel coordinate plots - geom_parcoord. avoid installing another package just plotting parallel coordinate plots. main motivation idea behind project.Adding ggplot2 geom parallel coordinate plots - geom_parcoord. avoid installing another package just plotting parallel coordinate plots. main motivation idea behind project.","code":""},{"path":"parallel-coordinate-plots-in-ggplot2.html","id":"sources","chapter":"41 Parallel Coordinate Plots in ggplot2","heading":"41.6.1 Sources","text":"Dataset - https://archive.ics.uci.edu/ml/datasets/Wine+QualityLearning parallel coordinate plots - https://towardsdatascience.com/parallel-coordinates-plots-6fcfa066dcb3Learning ggparcoord - https://www.r-graph-gallery.com/parallel-plot-ggally.html","code":""},{"path":"using-ggplot-in-python.html","id":"using-ggplot-in-python","chapter":"42 Using ggplot in python","heading":"42 Using ggplot in python","text":"Junyi YaoI new R, like high quality plots made ggplot2. used use python packages(numpy, pandas) process data use seaborn make plots. personal opinion, numpy pandas easier use tidyverse. However, ggplot2 makes better plots.think use numpy pandas process data use ggplot make plots, convinient. tried find ways use ggplot python.\npackage found “plotnine”, essentially python version ggplot. want make tutorial use numpy pandas process data use ggplot2 make plots Rstudio.link tutorial: Using ggplot python","code":""},{"path":"icons-and-symbols-in-r.html","id":"icons-and-symbols-in-r","chapter":"43 Icons and symbols in R","heading":"43 Icons and symbols in R","text":"Chen Jin","code":"\nlibrary(readr)\nlibrary(ggpubr)\nlibrary(showtext)\nlibrary(plyr)\nlibrary(ggplot2)\nlibrary(ggimage)\nlibrary(emojifont)\nlibrary(personograph)"},{"path":"icons-and-symbols-in-r.html","id":"why-use-symbols","chapter":"43 Icons and symbols in R","heading":"43.1 Why use Symbols?","text":"class, ’ve covering variety graph types, fundamental plotting techniques, proper treatments different data get. Equipped skills, ’d able come quite neat informative visualizations. However, simply taking care basic requirements generate ubiquitous standard-following graph. graphs may suitable scientific research paper presentation formal meeting, cases need lighthearted graph arouse viewer’s interest. instance, internet blog tweet, like graph entertaining eye-catching achieve views. one way add easily understandable symbols graph.","code":""},{"path":"icons-and-symbols-in-r.html","id":"pch-in-base-r","chapter":"43 Icons and symbols in R","heading":"43.2 ‘pch’ in Base R","text":"us ’ve experience base R plotting , probably quite familiar pch parameter. pch stands ‘plotting character’, namely symbol use. standard argument set character plotted number R functions. ‘pch’ can either single character integer code one set graphic symbols.using library ggpubr, can see corresponding pch characters 1 25 :","code":"\n# x <- c(sapply(seq(5, 25, 5), function(i) rep(i, 5)))\n# y <- rep(seq(25, 5, -5), 5)\n# \n# plot(x, y, pch = 1, cex = 3, yaxt = \"n\", xaxt = \"n\",\n#      ann = FALSE, xlim = c(1, 29), ylim = c(0, 30), lwd = 1)\n# text(x - 1.5, y, 1:25)\n\nggpubr::show_point_shapes()"},{"path":"icons-and-symbols-in-r.html","id":"pch-values","chapter":"43 Icons and symbols in R","heading":"43.2.0.1 pch values","text":"Values pch stored internally integers. interpretation isNA_integer_ : symbol.0 : 18: S-compatible vector symbols.19 : 25: R vector symbols.26 : 31: unused (ignored).32 : 127: ASCII characters.128 : 255: native characters single-byte locale symbol font. (128:159 used Windows.)-32 … : Unicode code point (supported).","code":""},{"path":"icons-and-symbols-in-r.html","id":"an-example-of-using-pch-iris-data","chapter":"43 Icons and symbols in R","heading":"43.2.0.2 An example of using pch (Iris Data)","text":"using different symbols different species, can clearly see variations among three types.","code":"\n# Define color for each of the 3 iris species\ncolors <- c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\ncolors <- colors[as.numeric(iris$Species)]\n\n# Define shapes\nshapes = c(16, 17, 18) \nshapes <- shapes[as.numeric(iris$Species)]\n\n# Plot\nplot(x = iris$Sepal.Length, y = iris$Sepal.Width,\n     xlab = \"Sepal Length\", ylab = \"Sepal Width\", main = \"Sepal Width vs. Length for 3 Species of Iris\",\n     col = colors, pch = shapes)\nlegend(\"topright\", legend = levels(iris$Species),\n      col =  c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n      pch = c(16, 17, 18) )"},{"path":"icons-and-symbols-in-r.html","id":"limitations-of-pch","chapter":"43 Icons and symbols in R","heading":"43.2.0.3 Limitations of pch:","text":"can see list, supported symbols quite limited R (basically pch = 1-25 ASCII symbols). pch numbers goes , might encounter issue locale support particular character. Besides, ’s difficult try memorize number represents cumbersome list, always look time need particular symbol check availability. Therefore, introduce appealing techniques follows.","code":""},{"path":"icons-and-symbols-in-r.html","id":"showtext","chapter":"43 Icons and symbols in R","heading":"43.3 showtext","text":"Showtext package originally designed deal font issues R. many cases, using non-standard fonts R graphs easy task. instance, using chinese characters plotting oftentimes result unreadable results.’s trick can apply build icons help showtext. many fonts look way symbols, 'wmpeople1' demonstrated just one . examples symbol fonts include Wingdings, Dingbat, webdings etc. adding fonts, manage show symbols text graph. now geom_text() something familiar can easily handle proceed.","code":""},{"path":"icons-and-symbols-in-r.html","id":"an-example-of-showtext-demographics-data","chapter":"43 Icons and symbols in R","heading":"43.3.0.1 An example of showtext (Demographics Data)","text":"example shows demographics splitting population subcategories education level. use symbol clearly indicates ’s demographic plot nature can clearly distinguish men women corresponding token.Every symbol graph actually character p u. ’s just appear differently 'wmpeople1' font. applying concept, now access great number text symbols simply adding font resources code.","code":"\nfont_add(\"wmpeople1\", \"resources/icons_and_symbols_in_r/wmpeople1.TTF\")\nshowtext_auto()\n\ndat = read.csv(textConnection('\n      edu,educode,gender,population\n      No School,1,m,17464\n      No School,1,f,41268\n      Primary School,2,m,139378\n      Primary School,2,f,154854\n      Middle School,3,m,236369\n      Middle School,3,f,205537\n      High School,4,m,94528\n      High School,4,f,70521\n      Bacherlor or above,5,m,57013\n      Bacherlor or above,5,f,50334\n'))\n\ndat$int = round(dat$population / 10000)\ngdat = ddply(dat, \"educode\", function(d) {\n    male = d$int[d$gender == \"m\"]\n    female = d$int[d$gender == \"f\"]\n    data.frame(gender = c(rep(\"m\", male), rep(\"f\", female)),\n               x = 1:(male + female))\n})\n\ngdat$char = ifelse(gdat$gender == \"m\", \"p\", \"u\")\nggplot(gdat, aes(x = x, y = factor(educode))) +\n    geom_text(aes(label = char, colour = gender),\n              family = \"wmpeople1\", size = 8) +\n    scale_x_continuous(\"Population（10 million）\") +\n    scale_y_discrete(\"Education Level\",\n        labels = unique(dat$edu[order(dat$educode)])) +\n    scale_colour_hue(guide = FALSE) +\n    ggtitle(\"2012 Demographics Data\")"},{"path":"icons-and-symbols-in-r.html","id":"ggimage","chapter":"43 Icons and symbols in R","heading":"43.4 ggimage","text":"ggimage supports image files graphic objects visualized ggplot2 graphic system. includes variety interesting useful functions introduce .","code":""},{"path":"icons-and-symbols-in-r.html","id":"geom_image","chapter":"43 Icons and symbols in R","heading":"43.4.1 geom_image()","text":"access greater range symbol choices, sometimes want use image external sources icon plotting. geom_image() layer ggplot2 can amazingly achieve . can simply use url file directory import image graphs.example, plot lines several Rstudio icons “R” shape:","code":"\nx <- c(2, 2, 2, 2, 2, 3, 3, 3.5, 3.5, 4)\ny <- c(2, 3, 4, 5, 6, 4, 6, 3, 5, 2)\nd <- data.frame(x = x, y = y)\n\nimg <- (\"https://www.r-project.org/logo/Rlogo.png\")\nggplot(d, aes(x, y)) + geom_image(image = img, size = .1) +\n  xlim(0, 6) + ylim(0, 7) +\n  ggtitle(\"Plot R with Rstudio Symbols\")"},{"path":"icons-and-symbols-in-r.html","id":"geom_emoji","chapter":"43 Icons and symbols in R","heading":"43.4.2 geom_emoji()","text":"discovered previously, symbols pch provides mostly simple shapes plain texts ASCII characters. doesn’t offer much support want use colorful emojis. special layer geom_emoji() facilitate easy incoporation emojis graphs, simply specifying unicode.","code":""},{"path":"icons-and-symbols-in-r.html","id":"emoji-characters","chapter":"43 Icons and symbols in R","heading":"43.4.2.1 Emoji Characters","text":"emoji corresponding unicode. can take look examples Emoji unicode https://apps.timwhitlock.info/emoji/tables/unicode.search_emoji() function can also help us find related emojis looking . return emoji aliases can converted unicode emoji function.","code":"\nsearch_emoji('smile')## [1] \"smiley\"      \"smile\"       \"sweat_smile\" \"smiley_cat\"  \"smile_cat\"\nemoji(search_emoji('smile'))## [1] \"😃\" \"😄\" \"😅\" \"😺\" \"😸\""},{"path":"icons-and-symbols-in-r.html","id":"an-example-of-geom_emoji-iris-data","chapter":"43 Icons and symbols in R","heading":"43.4.2.2 An example of geom_emoji() (Iris Data)","text":"can visualize distinguish points fitted well linear regression fitted relatively poorly help intuitive emojis.","code":"\nset.seed(0)\niris2 <- iris[sample(1:nrow(iris), 30), ]\nmodel <- lm(Petal.Length ~ Sepal.Length, data = iris2)\niris2$fitted <- predict(model)\n\np <- ggplot(iris2, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_linerange(aes(ymin = fitted, ymax = Petal.Length),\n                 colour = \"red\") +\n  geom_abline(intercept = model$coefficients[1],\n              slope = model$coefficients[2]) +\n  ggtitle(\"Regression on Petal Length and Sepal Length\")\n\np + ggimage::geom_emoji(aes(image = ifelse(abs(Petal.Length-fitted) > 0.5, '1f645', '1f600')), cex=0.04)"},{"path":"icons-and-symbols-in-r.html","id":"geom_icon","chapter":"43 Icons and symbols in R","heading":"43.4.3 geom_icon()","text":"geom_icon() provides support icons https://ionic.io/ionicons. website provides open source icons web use. Figures black white well designed. icon three formats - ‘Outline’, ‘Filled’, ‘Sharp’. can browse using key words search icons conveniently.","code":""},{"path":"icons-and-symbols-in-r.html","id":"an-example-of-geom_icon","chapter":"43 Icons and symbols in R","heading":"43.4.3.1 An example of geom_icon()","text":"’s example can list daily routines using wide variety symbols ionic website.","code":"\nimg <- list.files(system.file(\"extdata\", package=\"ggimage\"),\n                  pattern=\"png\", full.names=TRUE)\nd <- data.frame(x = rep(1:5, 3),\n                y = (rep(3:1, each = 5)))\nd$icon <- c('bed', 'fast-food', 'bus', 'business', 'book', 'call', 'ice-cream', 'mail-unread', 'musical-notes', 'flask', 'language', 'pizza', 'beer', 'walk' ,'bed')\nggplot(d, aes(x,y)) + geom_icon(aes(image=icon)) + \n  xlim(0, 10) + ylim(0, 4) + \n  geom_text(aes(label = icon), size=2, vjust = -4) + \n  ggtitle(\"Daily Tasks in Icons\") +\n  theme_void()"},{"path":"icons-and-symbols-in-r.html","id":"geom_pokemon","chapter":"43 Icons and symbols in R","heading":"43.4.4 geom_pokemon()","text":"special layer provides support pokemon characters. can easily create cute graphs target kids anime fans. also layer geom_flags matches abbreviated country code corresponding national flags. one can particularly helpful plotting worldwide data making comparisons among different nations.","code":""},{"path":"icons-and-symbols-in-r.html","id":"an-example-of-geom_pokemon","chapter":"43 Icons and symbols in R","heading":"43.4.4.1 An example of geom_pokemon()","text":"Let’s find pikachu located graph. ’s Attack Defense ability compare pokemons shown selected group?","code":"\npokemon <- readr::read_csv(\"https://gist.githubusercontent.com/armgilles/194bcff35001e7eb53a2a8b441e8b2c6/raw/92200bc0a673d5ce2110aaad4544ed6c4010f687/pokemon.csv\")\npkm <- pokemon[21:51, ]\nggplot(pkm, aes(Attack, Defense)) + \n  geom_pokemon(aes(image=tolower(Name)), size=.05) + \n  geom_text(aes(label = Name), size=1.2, vjust = -3) + \n  ggtitle(\"Defense vs. Attack for Selected Pokemon\")"},{"path":"icons-and-symbols-in-r.html","id":"special-type-personographs","chapter":"43 Icons and symbols in R","heading":"43.5 Special Type: Personographs","text":"personograph (Kuiper-Marshall plot) pictographic representation relative harm benefit intervention. icon grid colored indicate whether percentage people harmed intervention, benefit intervention, good outcome regardless intervention, bad outcome regardless intervention.","code":""},{"path":"icons-and-symbols-in-r.html","id":"arguments-of-personograph","chapter":"43 Icons and symbols in R","heading":"43.5.0.1 Arguments of personograph :","text":"data: list names percentages (0 1)data: list names percentages (0 1)fig.title: Figure titlefig.title: Figure titledraw.legend: Logical TRUE (default) draw legenddraw.legend: Logical TRUE (default) draw legendicon: grImport Picture icon, overwrites icon.styleicon: grImport Picture icon, overwrites icon.styleicon.dim: dimensions icon vector c(width, height) numerical. Calculated dimensions suppliedicon.dim: dimensions icon vector c(width, height) numerical. Calculated dimensions suppliedicon.style: numeric 1-11 indicating included icons use, mostly variations themeicon.style: numeric 1-11 indicating included icons use, mostly variations theme","code":""},{"path":"icons-and-symbols-in-r.html","id":"adjust-grid-size-and-color-each-portions","chapter":"43 Icons and symbols in R","heading":"43.5.0.2 Adjust grid size and Color each portions","text":"can specify number icons dimensions personograph. default 10 10 grid.","code":"\ndata <- list(first=0.89, second=0.06, third=0.05)\n# With colors\npersonograph(data, n.icons=64, dimensions=c(8, 8), colors=list(first=\"grey\", second=\"blue\", third=\"red\"))"},{"path":"icons-and-symbols-in-r.html","id":"different-icon-style","chapter":"43 Icons and symbols in R","heading":"43.5.0.3 Different icon style","text":"can change icons choosing preset icon styles use grImport picture overwrite .type graph particularly relevant useful want showcase impact pandemic data. can vividly depicts transmission pattern virus influential groups people. personograph() function implemented way ’s easy just pass named list percentages, colors, icon. Since can also overwrite symbol, can easily adapt personograph suitable sanarios well.","code":"\n# With different icon.style\ndata <- list(first=0.2, second=0.7, third=0.1)\npersonograph(data, n.icons=9, icon.style=6) # numeric from 1-11"},{"path":"icons-and-symbols-in-r.html","id":"references-1","chapter":"43 Icons and symbols in R","heading":"43.6 References:","text":"R DocumentationR Documentationhttps://cran.r-project.org/web/packages/showtext/index.htmlhttps://cran.r-project.org/web/packages/showtext/index.htmlhttps://github.com/GuangchuangYu/ggimagehttps://github.com/GuangchuangYu/ggimagehttps://ionic.io/ioniconshttps://ionic.io/ioniconshttps://apps.timwhitlock.info/emoji/tables/unicodehttps://apps.timwhitlock.info/emoji/tables/unicodehttps://warwick.ac.uk/fac/sci/wdsi/events/wrug/resources/emoji_plots.pdfhttps://warwick.ac.uk/fac/sci/wdsi/events/wrug/resources/emoji_plots.pdfhttps://cran.r-project.org/web/packages/personograph/README.htmlhttps://cran.r-project.org/web/packages/personograph/README.html","code":""},{"path":"exploratory-data-analysis-and-preprocessing-with-r-for-data-challenges.html","id":"exploratory-data-analysis-and-preprocessing-with-r-for-data-challenges","chapter":"44 Exploratory data analysis and preprocessing with R for data challenges","heading":"44 Exploratory data analysis and preprocessing with R for data challenges","text":"Tianhao Wu","code":"\n# loading libraries used in solving the problems\nlibrary(tidyverse)\nlibrary(gridExtra)"},{"path":"exploratory-data-analysis-and-preprocessing-with-r-for-data-challenges.html","id":"introduction-7","chapter":"44 Exploratory data analysis and preprocessing with R for data challenges","heading":"44.1 Introduction","text":"data challenge usually first step get data science job internship, among EDA & Data Preprocessing important part determine potential model performance usually give meaningful insights. companies requirement building fancy models, instead, like see explore analyze data, preprocess data well enough modeling. cases, good EDA & Preprocessing together easy models get test.many people sharing solutions different projects internet. However, projects done Python, usually lack instructions handle data challenge general. html file, share perform EDA & Data preprocessing typical data challenge R, using data “Iowa House Price Prediction Competition” https://www.kaggle.com/c/iowa-house-price-prediction","code":""},{"path":"exploratory-data-analysis-and-preprocessing-with-r-for-data-challenges.html","id":"exploratory-data-analysis-preprocessing","chapter":"44 Exploratory data analysis and preprocessing with R for data challenges","heading":"44.2 Exploratory Data Analysis & Preprocessing","text":"first thing always brief overview data questions mind:\nmany features ? many numeric? many categorical? many categorical features nominal? information affect strategies choosing models. example, lot categorical features, linear models might work well handle categorical features nature, encoded categorical features unlikely follow linear relationship.","code":"\n# read in data and have a brief look\ndf <- read_csv(\"resources/r_for_data_challenges/train.csv\", show_col_types = FALSE)\nnum_fea <- df %>% select_if(names(df) != 'SalePrice') %>% \n  select_if(is.numeric) %>%\n  names()\ncat_fea <- df %>% select_if(is.character) %>%\n  names()\nlabel <- 'SalePrice'\n# explore numeric columns\nstr(df[,num_fea])## tibble [1,460 × 37] (S3: tbl_df/tbl/data.frame)\n##  $ Id           : num [1:1460] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ MSSubClass   : num [1:1460] 60 20 60 70 60 50 20 60 50 190 ...\n##  $ LotFrontage  : num [1:1460] 65 80 68 60 84 85 75 NA 51 50 ...\n##  $ LotArea      : num [1:1460] 8450 9600 11250 9550 14260 ...\n##  $ OverallQual  : num [1:1460] 7 6 7 7 8 5 8 7 7 5 ...\n##  $ OverallCond  : num [1:1460] 5 8 5 5 5 5 5 6 5 6 ...\n##  $ YearBuilt    : num [1:1460] 2003 1976 2001 1915 2000 ...\n##  $ YearRemodAdd : num [1:1460] 2003 1976 2002 1970 2000 ...\n##  $ MasVnrArea   : num [1:1460] 196 0 162 0 350 0 186 240 0 0 ...\n##  $ BsmtFinSF1   : num [1:1460] 706 978 486 216 655 ...\n##  $ BsmtFinSF2   : num [1:1460] 0 0 0 0 0 0 0 32 0 0 ...\n##  $ BsmtUnfSF    : num [1:1460] 150 284 434 540 490 64 317 216 952 140 ...\n##  $ TotalBsmtSF  : num [1:1460] 856 1262 920 756 1145 ...\n##  $ 1stFlrSF     : num [1:1460] 856 1262 920 961 1145 ...\n##  $ 2ndFlrSF     : num [1:1460] 854 0 866 756 1053 ...\n##  $ LowQualFinSF : num [1:1460] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ GrLivArea    : num [1:1460] 1710 1262 1786 1717 2198 ...\n##  $ BsmtFullBath : num [1:1460] 1 0 1 1 1 1 1 1 0 1 ...\n##  $ BsmtHalfBath : num [1:1460] 0 1 0 0 0 0 0 0 0 0 ...\n##  $ FullBath     : num [1:1460] 2 2 2 1 2 1 2 2 2 1 ...\n##  $ HalfBath     : num [1:1460] 1 0 1 0 1 1 0 1 0 0 ...\n##  $ BedroomAbvGr : num [1:1460] 3 3 3 3 4 1 3 3 2 2 ...\n##  $ KitchenAbvGr : num [1:1460] 1 1 1 1 1 1 1 1 2 2 ...\n##  $ TotRmsAbvGrd : num [1:1460] 8 6 6 7 9 5 7 7 8 5 ...\n##  $ Fireplaces   : num [1:1460] 0 1 1 1 1 0 1 2 2 2 ...\n##  $ GarageYrBlt  : num [1:1460] 2003 1976 2001 1998 2000 ...\n##  $ GarageCars   : num [1:1460] 2 2 2 3 3 2 2 2 2 1 ...\n##  $ GarageArea   : num [1:1460] 548 460 608 642 836 480 636 484 468 205 ...\n##  $ WoodDeckSF   : num [1:1460] 0 298 0 0 192 40 255 235 90 0 ...\n##  $ OpenPorchSF  : num [1:1460] 61 0 42 35 84 30 57 204 0 4 ...\n##  $ EnclosedPorch: num [1:1460] 0 0 0 272 0 0 0 228 205 0 ...\n##  $ 3SsnPorch    : num [1:1460] 0 0 0 0 0 320 0 0 0 0 ...\n##  $ ScreenPorch  : num [1:1460] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ PoolArea     : num [1:1460] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ MiscVal      : num [1:1460] 0 0 0 0 0 700 0 350 0 0 ...\n##  $ MoSold       : num [1:1460] 2 5 9 2 12 10 8 11 4 1 ...\n##  $ YrSold       : num [1:1460] 2008 2007 2008 2006 2008 ...\n# explore categorical columns\nstr(df[,cat_fea])## tibble [1,460 × 43] (S3: tbl_df/tbl/data.frame)\n##  $ MSZoning     : chr [1:1460] \"RL\" \"RL\" \"RL\" \"RL\" ...\n##  $ Street       : chr [1:1460] \"Pave\" \"Pave\" \"Pave\" \"Pave\" ...\n##  $ Alley        : chr [1:1460] NA NA NA NA ...\n##  $ LotShape     : chr [1:1460] \"Reg\" \"Reg\" \"IR1\" \"IR1\" ...\n##  $ LandContour  : chr [1:1460] \"Lvl\" \"Lvl\" \"Lvl\" \"Lvl\" ...\n##  $ Utilities    : chr [1:1460] \"AllPub\" \"AllPub\" \"AllPub\" \"AllPub\" ...\n##  $ LotConfig    : chr [1:1460] \"Inside\" \"FR2\" \"Inside\" \"Corner\" ...\n##  $ LandSlope    : chr [1:1460] \"Gtl\" \"Gtl\" \"Gtl\" \"Gtl\" ...\n##  $ Neighborhood : chr [1:1460] \"CollgCr\" \"Veenker\" \"CollgCr\" \"Crawfor\" ...\n##  $ Condition1   : chr [1:1460] \"Norm\" \"Feedr\" \"Norm\" \"Norm\" ...\n##  $ Condition2   : chr [1:1460] \"Norm\" \"Norm\" \"Norm\" \"Norm\" ...\n##  $ BldgType     : chr [1:1460] \"1Fam\" \"1Fam\" \"1Fam\" \"1Fam\" ...\n##  $ HouseStyle   : chr [1:1460] \"2Story\" \"1Story\" \"2Story\" \"2Story\" ...\n##  $ RoofStyle    : chr [1:1460] \"Gable\" \"Gable\" \"Gable\" \"Gable\" ...\n##  $ RoofMatl     : chr [1:1460] \"CompShg\" \"CompShg\" \"CompShg\" \"CompShg\" ...\n##  $ Exterior1st  : chr [1:1460] \"VinylSd\" \"MetalSd\" \"VinylSd\" \"Wd Sdng\" ...\n##  $ Exterior2nd  : chr [1:1460] \"VinylSd\" \"MetalSd\" \"VinylSd\" \"Wd Shng\" ...\n##  $ MasVnrType   : chr [1:1460] \"BrkFace\" \"None\" \"BrkFace\" \"None\" ...\n##  $ ExterQual    : chr [1:1460] \"Gd\" \"TA\" \"Gd\" \"TA\" ...\n##  $ ExterCond    : chr [1:1460] \"TA\" \"TA\" \"TA\" \"TA\" ...\n##  $ Foundation   : chr [1:1460] \"PConc\" \"CBlock\" \"PConc\" \"BrkTil\" ...\n##  $ BsmtQual     : chr [1:1460] \"Gd\" \"Gd\" \"Gd\" \"TA\" ...\n##  $ BsmtCond     : chr [1:1460] \"TA\" \"TA\" \"TA\" \"Gd\" ...\n##  $ BsmtExposure : chr [1:1460] \"No\" \"Gd\" \"Mn\" \"No\" ...\n##  $ BsmtFinType1 : chr [1:1460] \"GLQ\" \"ALQ\" \"GLQ\" \"ALQ\" ...\n##  $ BsmtFinType2 : chr [1:1460] \"Unf\" \"Unf\" \"Unf\" \"Unf\" ...\n##  $ Heating      : chr [1:1460] \"GasA\" \"GasA\" \"GasA\" \"GasA\" ...\n##  $ HeatingQC    : chr [1:1460] \"Ex\" \"Ex\" \"Ex\" \"Gd\" ...\n##  $ CentralAir   : chr [1:1460] \"Y\" \"Y\" \"Y\" \"Y\" ...\n##  $ Electrical   : chr [1:1460] \"SBrkr\" \"SBrkr\" \"SBrkr\" \"SBrkr\" ...\n##  $ KitchenQual  : chr [1:1460] \"Gd\" \"TA\" \"Gd\" \"Gd\" ...\n##  $ Functional   : chr [1:1460] \"Typ\" \"Typ\" \"Typ\" \"Typ\" ...\n##  $ FireplaceQu  : chr [1:1460] NA \"TA\" \"TA\" \"Gd\" ...\n##  $ GarageType   : chr [1:1460] \"Attchd\" \"Attchd\" \"Attchd\" \"Detchd\" ...\n##  $ GarageFinish : chr [1:1460] \"RFn\" \"RFn\" \"RFn\" \"Unf\" ...\n##  $ GarageQual   : chr [1:1460] \"TA\" \"TA\" \"TA\" \"TA\" ...\n##  $ GarageCond   : chr [1:1460] \"TA\" \"TA\" \"TA\" \"TA\" ...\n##  $ PavedDrive   : chr [1:1460] \"Y\" \"Y\" \"Y\" \"Y\" ...\n##  $ PoolQC       : chr [1:1460] NA NA NA NA ...\n##  $ Fence        : chr [1:1460] NA NA NA NA ...\n##  $ MiscFeature  : chr [1:1460] NA NA NA NA ...\n##  $ SaleType     : chr [1:1460] \"WD\" \"WD\" \"WD\" \"WD\" ...\n##  $ SaleCondition: chr [1:1460] \"Normal\" \"Normal\" \"Normal\" \"Abnorml\" ..."},{"path":"exploratory-data-analysis-and-preprocessing-with-r-for-data-challenges.html","id":"explore-handle-numeric-features","chapter":"44 Exploratory data analysis and preprocessing with R for data challenges","heading":"44.2.1 Explore & Handle Numeric Features","text":"Drop useless features like “Id” known influence target label. don’t drop columns, might add noise lead overfitting. example, IDs assigned house prices sorted, “strong correlation” “ID” “House Price”. result, model give “ID” large weight predicting house prices, desired .compute show proportions missing values feature.Drop “LotFrontage” many missing values (proportion > 15%)Fill NAs medians rest numeric features (median robust outliers mean)Always check whether NAs filled(using loop, important use “local({})” give iteration separate local space previous results overwritten recent one!!)Use multiple scatter plots explore relationships important features label. choose features based correlation coefficients label. reason want visualize relationships follows:outliers unexpected patterns?features assume highly correlated label displayed (low correlation coefficients (<0.4))?train models, come back check whether feature importance returned models aligned observation . interesting differences, explore reasons behind.","code":"\n# drop useless features to avoid overfitting & noise\nnum_fea = num_fea[num_fea != 'Id']\ndf <- df[,c(num_fea, cat_fea, label)]\n# show proportions of missing values\nmissing_col <- colSums(is.na(df[,num_fea]))/dim(df)[1] \nmissing_col[missing_col > 0] %>% sort(decreasing = TRUE)## LotFrontage GarageYrBlt  MasVnrArea \n## 0.177397260 0.055479452 0.005479452\nnum_fea <- num_fea[num_fea != 'LotFrontage']\ndf <- df[,c(num_fea, cat_fea, label)]\nfor (i in c('GarageYrBlt','MasVnrArea')) {\n  df[is.na(df[i]), i] <- local({\n    i <- i\n    median(unlist(df[i]), na.rm=TRUE)\n  })\n}\nmissing_col <- colSums(is.na(df[,num_fea]))/dim(df)[1] \nmissing_col[missing_col > 0] %>% sort(decreasing = TRUE)## named numeric(0)\ncorr <- cor(df[,append(num_fea, label)])[,label]\ncorr <- corr[order(abs(corr), decreasing = TRUE)]\nimp_num_fea <- names(corr[abs(corr)>0.4]) # only take the features with absolute value of coeff > 0.4\nimp_num_fea <- imp_num_fea[imp_num_fea != label]\n\np <- list()\nfor (i in 1:length(imp_num_fea)) {\n  p[[i]] <- local({ \n    i <- i\n    ggplot() + \n    geom_point(aes(x=unlist(df[,imp_num_fea[i]]), y=unlist(df[label]))) +\n    xlab(imp_num_fea[i]) +\n    ylab(\"House Prices\") \n  })\n}\ngrid.arrange(grobs = p, ncol = 3)"},{"path":"exploratory-data-analysis-and-preprocessing-with-r-for-data-challenges.html","id":"explore-handle-categorical-columns","chapter":"44 Exploratory data analysis and preprocessing with R for data challenges","heading":"44.2.2 Explore & Handle Categorical Columns","text":"Compute show proportions missing values feature.Categorical features usually tricky NA might mean “unknown”. According data description, “MasVnrType” “Electrical” truly “NA” missing values, NAs features just mean “None”. Another thing note , even though categorical features large portions missing values, just delete columns like handle numeric data. Sometimes, missing values super informative (example, mean house prices corresponding missing values feature significantly higher rest classes feature). Therefore, visualize missing values explicit class, decide whether drop column make missing values indicator column. features missing values, can just fill NAs mode.models handle categorical features , therefore need transform categorical data numeric data meaningful manner. three frequently used strategies:One-hot Encoding: transform class categorical feature binary feature. method works well nominal features low cardinality. One-hot might work well linear models causes multi-collinearity breaks assumption independent features.Ordinal Encoding: transform class ordinal categorical feature integer. order assigning integers follow internal order. example, assign 1 poor, 2 average, 3 good. lot ordinal features different sets levels, method tedious need specify internal order feature.Target Encoding: replace class categorical feature mean labels corresponding class. method applies types categorical features, especially good categorical features high cardinality.House price dataset typical dataset various categorical features. use one-hot, dramatically increase dimension dataset. Since many ordinal features different set levels, ordinal encoding also good idea. Therefore, can use target encoding , gives meaningful orders without causing multi-colinearity increasing dimension. Moreover, since many categorical features, hard choose features visualize. use target encoding transform features, choose visualize categorical features just like handle numeric data.Since categorical features transformed numeric features, visualize way plotting numeric features.Finally missing values filled, categorical features transformed. Now ready modeling!Note:remove outliers reasonable small dataset. Moreover, even though remove “outliers”, usually little impact model performance. right way just keep data run model first. certain predictions dramatically higher errors, go data try see outliers remove.remove outliers reasonable small dataset. Moreover, even though remove “outliers”, usually little impact model performance. right way just keep data run model first. certain predictions dramatically higher errors, go data try see outliers remove.scale data . Firstly, scaling needed every model. tree models, scaling just matter . However, distance-based models, scaling might reduce training time, boost interpretability feature importance, improve accuracy/reduce error.scale data . Firstly, scaling needed every model. tree models, scaling just matter . However, distance-based models, scaling might reduce training time, boost interpretability feature importance, improve accuracy/reduce error.","code":"\n# show proportions of missing values\nmissing_col <- colSums(is.na(df[,cat_fea]))/dim(df)[1] \nmissing_col <- missing_col[missing_col > 0] %>% sort(decreasing = TRUE)\nmissing_col##       PoolQC  MiscFeature        Alley        Fence  FireplaceQu   GarageType \n## 0.9952054795 0.9630136986 0.9376712329 0.8075342466 0.4726027397 0.0554794521 \n## GarageFinish   GarageQual   GarageCond BsmtExposure BsmtFinType2     BsmtQual \n## 0.0554794521 0.0554794521 0.0554794521 0.0260273973 0.0260273973 0.0253424658 \n##     BsmtCond BsmtFinType1   MasVnrType   Electrical \n## 0.0253424658 0.0253424658 0.0054794521 0.0006849315\n# Find the mode for each feature\ndf %>% group_by_at('MasVnrType') %>% count()## # A tibble: 5 × 2\n## # Groups:   MasVnrType [5]\n##   MasVnrType     n\n##   <chr>      <int>\n## 1 BrkCmn        15\n## 2 BrkFace      445\n## 3 None         864\n## 4 Stone        128\n## 5 <NA>           8\ndf %>% group_by_at('Electrical') %>% count()## # A tibble: 6 × 2\n## # Groups:   Electrical [6]\n##   Electrical     n\n##   <chr>      <int>\n## 1 FuseA         94\n## 2 FuseF         27\n## 3 FuseP          3\n## 4 Mix            1\n## 5 SBrkr       1334\n## 6 <NA>           1\ndf[is.na(df['MasVnrType']), 'MasVnrType'] <- 'None'\ndf[is.na(df['Electrical']), 'Electrical'] <- 'SBrkr'\ndf[is.na(df)] <- 'None'\nmissing_col <- colSums(is.na(df[,cat_fea]))/dim(df)[1] \nmissing_col[missing_col > 0] %>% sort(decreasing = TRUE) # always check whether all NAs have been filled## named numeric(0)\ntarget_encoder <- function(df, feature) { # feature input in string format\n  fea = paste(feature, 'target', sep = '_')\n  df1 <- df %>% group_by_at(feature) %>% \n    summarise(n = mean(SalePrice)) %>%\n    rename_with(.fn = function(x) fea, .cols = n) # rename encoded features\n  df2 <- left_join(df, df1)\n  df2 <- select_if(df2, names(df2) != feature) # drop original features\n  return(df2)\n}\n\nfor (i in cat_fea) {\n  suppressMessages(df <- target_encoder(df, i)) # to avoid message of \"joining by..\"\n}\n\nstr(df)## tibble [1,460 × 79] (S3: tbl_df/tbl/data.frame)\n##  $ MSSubClass          : num [1:1460] 60 20 60 70 60 50 20 60 50 190 ...\n##  $ LotArea             : num [1:1460] 8450 9600 11250 9550 14260 ...\n##  $ OverallQual         : num [1:1460] 7 6 7 7 8 5 8 7 7 5 ...\n##  $ OverallCond         : num [1:1460] 5 8 5 5 5 5 5 6 5 6 ...\n##  $ YearBuilt           : num [1:1460] 2003 1976 2001 1915 2000 ...\n##  $ YearRemodAdd        : num [1:1460] 2003 1976 2002 1970 2000 ...\n##  $ MasVnrArea          : num [1:1460] 196 0 162 0 350 0 186 240 0 0 ...\n##  $ BsmtFinSF1          : num [1:1460] 706 978 486 216 655 ...\n##  $ BsmtFinSF2          : num [1:1460] 0 0 0 0 0 0 0 32 0 0 ...\n##  $ BsmtUnfSF           : num [1:1460] 150 284 434 540 490 64 317 216 952 140 ...\n##  $ TotalBsmtSF         : num [1:1460] 856 1262 920 756 1145 ...\n##  $ 1stFlrSF            : num [1:1460] 856 1262 920 961 1145 ...\n##  $ 2ndFlrSF            : num [1:1460] 854 0 866 756 1053 ...\n##  $ LowQualFinSF        : num [1:1460] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ GrLivArea           : num [1:1460] 1710 1262 1786 1717 2198 ...\n##  $ BsmtFullBath        : num [1:1460] 1 0 1 1 1 1 1 1 0 1 ...\n##  $ BsmtHalfBath        : num [1:1460] 0 1 0 0 0 0 0 0 0 0 ...\n##  $ FullBath            : num [1:1460] 2 2 2 1 2 1 2 2 2 1 ...\n##  $ HalfBath            : num [1:1460] 1 0 1 0 1 1 0 1 0 0 ...\n##  $ BedroomAbvGr        : num [1:1460] 3 3 3 3 4 1 3 3 2 2 ...\n##  $ KitchenAbvGr        : num [1:1460] 1 1 1 1 1 1 1 1 2 2 ...\n##  $ TotRmsAbvGrd        : num [1:1460] 8 6 6 7 9 5 7 7 8 5 ...\n##  $ Fireplaces          : num [1:1460] 0 1 1 1 1 0 1 2 2 2 ...\n##  $ GarageYrBlt         : num [1:1460] 2003 1976 2001 1998 2000 ...\n##  $ GarageCars          : num [1:1460] 2 2 2 3 3 2 2 2 2 1 ...\n##  $ GarageArea          : num [1:1460] 548 460 608 642 836 480 636 484 468 205 ...\n##  $ WoodDeckSF          : num [1:1460] 0 298 0 0 192 40 255 235 90 0 ...\n##  $ OpenPorchSF         : num [1:1460] 61 0 42 35 84 30 57 204 0 4 ...\n##  $ EnclosedPorch       : num [1:1460] 0 0 0 272 0 0 0 228 205 0 ...\n##  $ 3SsnPorch           : num [1:1460] 0 0 0 0 0 320 0 0 0 0 ...\n##  $ ScreenPorch         : num [1:1460] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ PoolArea            : num [1:1460] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ MiscVal             : num [1:1460] 0 0 0 0 0 700 0 350 0 0 ...\n##  $ MoSold              : num [1:1460] 2 5 9 2 12 10 8 11 4 1 ...\n##  $ YrSold              : num [1:1460] 2008 2007 2008 2006 2008 ...\n##  $ SalePrice           : num [1:1460] 208500 181500 223500 140000 250000 ...\n##  $ MSZoning_target     : num [1:1460] 191005 191005 191005 191005 191005 ...\n##  $ Street_target       : num [1:1460] 181131 181131 181131 181131 181131 ...\n##  $ Alley_target        : num [1:1460] 183452 183452 183452 183452 183452 ...\n##  $ LotShape_target     : num [1:1460] 164755 164755 206102 206102 206102 ...\n##  $ LandContour_target  : num [1:1460] 180184 180184 180184 180184 180184 ...\n##  $ Utilities_target    : num [1:1460] 180951 180951 180951 180951 180951 ...\n##  $ LotConfig_target    : num [1:1460] 176938 177935 176938 181623 177935 ...\n##  $ LandSlope_target    : num [1:1460] 179957 179957 179957 179957 179957 ...\n##  $ Neighborhood_target : num [1:1460] 197966 238773 197966 210625 335295 ...\n##  $ Condition1_target   : num [1:1460] 184495 142475 184495 184495 184495 ...\n##  $ Condition2_target   : num [1:1460] 181169 181169 181169 181169 181169 ...\n##  $ BldgType_target     : num [1:1460] 185764 185764 185764 185764 185764 ...\n##  $ HouseStyle_target   : num [1:1460] 210052 175985 210052 210052 210052 ...\n##  $ RoofStyle_target    : num [1:1460] 171484 171484 171484 171484 171484 ...\n##  $ RoofMatl_target     : num [1:1460] 179804 179804 179804 179804 179804 ...\n##  $ Exterior1st_target  : num [1:1460] 213733 149422 213733 149842 213733 ...\n##  $ Exterior2nd_target  : num [1:1460] 214432 149803 214432 161329 214432 ...\n##  $ MasVnrType_target   : num [1:1460] 204692 156958 204692 156958 204692 ...\n##  $ ExterQual_target    : num [1:1460] 231634 144341 231634 144341 231634 ...\n##  $ ExterCond_target    : num [1:1460] 184035 184035 184035 184035 184035 ...\n##  $ Foundation_target   : num [1:1460] 225230 149806 225230 132291 225230 ...\n##  $ BsmtQual_target     : num [1:1460] 202688 202688 202688 140760 202688 ...\n##  $ BsmtCond_target     : num [1:1460] 183633 183633 183633 213600 183633 ...\n##  $ BsmtExposure_target : num [1:1460] 165652 257690 192790 165652 206643 ...\n##  $ BsmtFinType1_target : num [1:1460] 235414 161573 235414 161573 235414 ...\n##  $ BsmtFinType2_target : num [1:1460] 184695 184695 184695 184695 184695 ...\n##  $ Heating_target      : num [1:1460] 182021 182021 182021 182021 182021 ...\n##  $ HeatingQC_target    : num [1:1460] 214914 214914 214914 156859 214914 ...\n##  $ CentralAir_target   : num [1:1460] 186187 186187 186187 186187 186187 ...\n##  $ Electrical_target   : num [1:1460] 186811 186811 186811 186811 186811 ...\n##  $ KitchenQual_target  : num [1:1460] 212116 139963 212116 212116 212116 ...\n##  $ Functional_target   : num [1:1460] 183429 183429 183429 183429 183429 ...\n##  $ FireplaceQu_target  : num [1:1460] 141331 205723 205723 226351 205723 ...\n##  $ GarageType_target   : num [1:1460] 202893 202893 202893 134091 202893 ...\n##  $ GarageFinish_target : num [1:1460] 202069 202069 202069 142156 202069 ...\n##  $ GarageQual_target   : num [1:1460] 187490 187490 187490 187490 187490 ...\n##  $ GarageCond_target   : num [1:1460] 187886 187886 187886 187886 187886 ...\n##  $ PavedDrive_target   : num [1:1460] 186434 186434 186434 186434 186434 ...\n##  $ PoolQC_target       : num [1:1460] 180405 180405 180405 180405 180405 ...\n##  $ Fence_target        : num [1:1460] 187597 187597 187597 187597 187597 ...\n##  $ MiscFeature_target  : num [1:1460] 182046 182046 182046 182046 182046 ...\n##  $ SaleType_target     : num [1:1460] 173402 173402 173402 173402 173402 ...\n##  $ SaleCondition_target: num [1:1460] 175202 175202 175202 146527 175202 ...\nfor (i in 1:length(cat_fea)) {\n  cat_fea[i] <- local({\n    i <- i\n    paste(cat_fea[i],'target',sep='_') # update categorical feature list\n  })\n}\n\ncorr <- cor(df[,append(cat_fea, label)])[,label]\ncorr <- corr[order(abs(corr), decreasing = TRUE)]\nimp_num_fea <- names(corr[abs(corr)>0.4])\nimp_num_fea <- imp_num_fea[imp_num_fea != label]\n\np <- list()\nfor (i in 1:length(imp_num_fea)) {\n  p[[i]] <- local({ \n    i <- i\n    ggplot() + \n    geom_point(aes(x=unlist(df[,imp_num_fea[i]]), y=unlist(df[label]))) +\n    xlab(imp_num_fea[i]) +\n    ylab(\"House Prices\") \n  })\n}\ngrid.arrange(grobs = p, ncol = 3)"},{"path":"linear-association-r-studio-plugin.html","id":"linear-association-r-studio-plugin","chapter":"45 Linear Association R Studio Plugin","heading":"45 Linear Association R Studio Plugin","text":"Ruilin Liu Zhifeng Zhang","code":""},{"path":"linear-association-r-studio-plugin.html","id":"introduction-8","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.1 Introduction","text":"community contribution project, write simple R addin project helps us find linear association feature columns target column data frame. idea actually comes problem sets 2 3 need find correlation two variables. indeed several ways like pair scatter plots mosaic plots.However, want simpler efficient way . learnt statistic class, p-value linear regression tests null hypothesis coefficient equal zero. lower p-value, stronger contribution feature column target column.Thus, develop idea can write simple R addin fits linear model onto target column return top n feature columns lowest p-values, highest possibilities correlation target column.","code":""},{"path":"linear-association-r-studio-plugin.html","id":"installation-1","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.2 Installation","text":"Link: https://github.com/gzmason/linear_association_rplugin/tree/mainDownload files repo, download zip file LinearAssociation.zip unzip .Open project file LinearAssociation.Rproj Rstudio.top right panel Build sub panel, click Install Restart.Check Addins menu addin called Find Linear Association LINEARASSOCIATION. , done installing addin.","code":""},{"path":"linear-association-r-studio-plugin.html","id":"usage","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.3 Usage","text":"","code":""},{"path":"linear-association-r-studio-plugin.html","id":"note-this-addin-runs-on-library-tidyverse.-please-run-the-following-command-first-before-running-into-next-step","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.3.0.1 Note: This addin runs on library ‘tidyverse’. Please run the following command first before running into next step!","text":"library(tidyverse)addin take three arguments line, separated commas:name data framename target columnnumber columns, referred nand returns n number feature columns highest probability linear correlation target column.Tips:addin tolerates white-space arguments.first two arguments required, whereas third argument n optional. provided, n 5 default.data frame stored environment first, addin reads data frame current environment variables.n reset 5 non-integer negative value passed.n capped maximum number feature columns greater number passed.Currently, target column can contain continuous numeric data, since using linear regression. Categorical target column supported!","code":""},{"path":"linear-association-r-studio-plugin.html","id":"example","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.3.0.2 Example","text":"using SleepStudy example PSet3 example.console output :","code":""},{"path":"linear-association-r-studio-plugin.html","id":"troubleshooting","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.4 Troubleshooting","text":"“str_split” foundSolution: load tidyverse package\nlibrary(tidyverse)showing one row “NA”Solution: change target column continuous one. addin support categorical target column logistic regression .","code":""},{"path":"linear-association-r-studio-plugin.html","id":"evaluation-and-improvement","chapter":"45 Linear Association R Studio Plugin","heading":"45.0.5 Evaluation and Improvement","text":"learnt lot process creating small R addin project. first might seem bit scary write addin since us clues going . reading tutorials write R addin, find rather simple enjoying. needless say creating R package huge sense accomplishment, though lots trials errors along way.number ways little project can improved. example, adding logistic regression feature categorical target column. Evaluating metrics regarding linear association rather simple p-value achieve comprehensive results.","code":""},{"path":"customized-sorting-in-r.html","id":"customized-sorting-in-r","chapter":"46 Customized Sorting in R","heading":"46 Customized Sorting in R","text":"Xuchen WangThe idea package comes lack sorting function R, uses order function sort really caused confusion . addition, user defined functions applicable, like implement sort function .using idea merge sort sorting functions runs O(nlogn).file contains two functions, aiming solve following questionsort list user-defined comparison functioneg:lst=[[2,2,8],[3,6,7],[9,5,2]],compare<-function(x,y) {\nx[[2]]<y[[2]]\n}lst<-sort(lst,compare)lst [[2,2,8],[9,5,2],[3,6,7]] nowexample:sort list vector just change compare function:sort dataframe user-defined functionThe logic , thing notice comparison function takes two rows compare ","code":"\nlstmerge<-function(l1,l2,comp) {\n  m<-length(l1)\n  n<-length(l2)\n  i<-1\n  j<-1\n  output<-vector(\"list\",m+n)\n  while(i<=m&&j<=n) {\n    if(comp(l1[[i]],l2[[j]])) {\n      output[[i+j-1]]<-l1[[i]]\n      i<-i+1\n    } else {\n      output[[i+j-1]]<-l2[[j]]\n      j<-j+1\n    }\n  }\n  while(i<=m) {\n     output[[i+j-1]]<-l1[[i]]\n    i<-i+1\n  }\n   while(j<=n) {\n     output[[i+j-1]]<-l2[[j]]\n    j<-j+1\n   }\n  output\n}\n\nlstsort<-function(lst,comp) {\n  n<-length(lst)\n  if(n<=1) {\n    return(lst)\n  }\n  mid<-ceiling(n/2)\n  lft<-lstsort(lst[(1:mid)],comp)\n  rt<-lstsort(lst[((mid+1):n)],comp)\n  return(lstmerge(lft,rt,comp))\n  \n}\nlst= lapply(1:100, function(x) as.list(sample(1:100,size=2)))\nsapply(lst,function(x) x[[2]])##   [1]  62  78  55   5  87  84  54  81  70  69  25  60 100  50  61   1  23  29\n##  [19]  78  28  56  84  39  92  36  56  68  38  63  35  68   7 100  87  78  61\n##  [37]  78   3   9  59  95  28  78  94  14  48  21  23  50  90  79  53  33  76\n##  [55]  91  88  98  22  98  34  45  33  98  85   9  73   7  44  58  41  85  30\n##  [73]  19  24  19  90  74  35  50  45 100  94  36  21  35  34  14  15  40  19\n##  [91]   4  84   8  26  34   9  37  36  26  34\ncompare<-function(x,y) {\n  x[[2]]<y[[2]]\n}\n\nlst<-lstsort(lst,compare)\nsapply(lst,function(x) x[[2]])##   [1]   1   3   4   5   7   7   8   9   9   9  14  14  15  19  19  19  21  21\n##  [19]  22  23  23  24  25  26  26  28  28  29  30  33  33  34  34  34  34  35\n##  [37]  35  35  36  36  36  37  38  39  40  41  44  45  45  48  50  50  50  53\n##  [55]  54  55  56  56  58  59  60  61  61  62  63  68  68  69  70  73  74  76\n##  [73]  78  78  78  78  78  79  81  84  84  84  85  85  87  87  88  90  90  91\n##  [91]  92  94  94  95  98  98  98 100 100 100\nlst= lapply(1:100, function(x) sample(1:100,size=2))\nsapply(lst,function(x) x[2])##   [1]  43  38  49  99   7  47  45  75  49  73   9   9  13  45  15  93   9  78\n##  [19]  92  88   4  93  31  73  41  71  47  23  11  19  12   2  98  11  75  17\n##  [37]  94  37  94  42  94  34  90  73  19  81  39  90  38  63   7  97   5  57\n##  [55]  36  76  23  54  22  23  74  71  21  21  64  46  93  27  87  45 100  23\n##  [73]  80  55  72   2  33  79  88  33  30  61  59  35  91  56  76  23  45  44\n##  [91]  98   7  15  53  31   6  96  89  56  11\ncompare2<-function(x,y) {\n  x[2]<y[2]\n}\nlst<-lstsort(lst,compare2)\nsapply(lst,function(x) x[2])##   [1]   2   2   4   5   6   7   7   7   9   9   9  11  11  11  12  13  15  15\n##  [19]  17  19  19  21  21  22  23  23  23  23  23  27  30  31  31  33  33  34\n##  [37]  35  36  37  38  38  39  41  42  43  44  45  45  45  45  46  47  47  49\n##  [55]  49  53  54  55  56  56  57  59  61  63  64  71  71  72  73  73  73  74\n##  [73]  75  75  76  76  78  79  80  81  87  88  88  89  90  90  91  92  93  93\n##  [91]  93  94  94  94  96  97  98  98  99 100\ndfmerge<-function(d1,d2,comp) {\n  m<-nrow(d1)\n  n<-nrow(d2)\n  i<-1\n  j<-1\n  output<-data.frame(matrix(NA,nrow=m+n,ncol=length(colnames(d1))))\n  colnames(output)<-colnames(d1)\n while(i<=m&&j<=n) {\n    if(comp(d1[i,],d2[j,])) {\n      output[(i+j-1),]<-d1[i,]\n      i<-i+1\n    } else {\n      output[(i+j-1),]<-d2[j,]\n      j<-j+1\n    }\n  }\n  while(i<=m) {\n     output[(i+j-1),]<-d1[i,]\n      i<-i+1\n  }\n   while(j<=n) {\n    output[(i+j-1),]<-d2[j,]\n      j<-j+1\n   }\n  output\n  \n}\n\ndfsort<-function(df,comp) {\n  n<-nrow(df)\n  if(n<=1) {\n    return(df)\n  }\n  mid<-ceiling(n/2)\n  lft<-dfsort(df[1:mid,],comp)\n  rt<-dfsort(df[(mid+1):n,],comp)\n  return(dfmerge(lft,rt,comp))\n  \n}\nM<-matrix(sample(1:100,300,replace=TRUE),ncol=3)\ndf<-data.frame(M)\ncolnames(df)<-c(\"a\",\"b\",\"c\")\ndf##      a  b   c\n## 1   82 46  69\n## 2   49 94  55\n## 3   20 17 100\n## 4   16 75  69\n## 5   68 27  73\n## 6   57 13  26\n## 7   97  7  71\n## 8   21 15  63\n## 9   56 86  94\n## 10  94 82  18\n## 11   6 92   2\n## 12   7 28  64\n## 13   9 90  94\n## 14  86 78  73\n## 15  69 52  60\n## 16  94 83  78\n## 17  81 99  90\n## 18  91 57  35\n## 19  46 53  67\n## 20  45 65  58\n## 21  44 63  25\n## 22  53 33  48\n## 23  25 76   6\n## 24  95 34  94\n## 25  53 32   3\n## 26  14 25  72\n## 27  48 28  57\n## 28  20 26  17\n## 29  88 56  39\n## 30   7  9  50\n## 31  15 20  23\n## 32  13 66  98\n## 33  97 77  50\n## 34  76 48  82\n## 35  37 84   4\n## 36  53 10  15\n## 37  30 90  32\n## 38  85 19  44\n## 39  10 82  17\n## 40   3  5   3\n## 41  41 37  42\n## 42  31 68  25\n## 43  58 83  43\n## 44  62 88   7\n## 45   1 47  46\n## 46  35 75  27\n## 47  19 78  86\n## 48  11 36  89\n## 49  18  8  32\n## 50  10 69  40\n## 51  49 69  18\n## 52  92 17  14\n## 53   4 59  56\n## 54  90 68  49\n## 55   9 69  46\n## 56  13 93  92\n## 57  80 69  11\n## 58  32  6  70\n## 59  49  2  36\n## 60  31 91  14\n## 61  25 98  70\n## 62  66 97  52\n## 63  21 90  82\n## 64  82 41  39\n## 65  54 62  55\n## 66  99 20  79\n## 67  59 15  66\n## 68  55 63  90\n## 69  28 63  92\n## 70  91 24  74\n## 71  84 45  90\n## 72  39 87  22\n## 73  15 86  38\n## 74   5 39   8\n## 75  91 61  17\n## 76  12 63  71\n## 77  55 70  83\n## 78  63 64  29\n## 79  52 67  80\n## 80  79 43  99\n## 81  98 81  53\n## 82  27 42  66\n## 83  81 56  81\n## 84  50 52  80\n## 85  53 27  47\n## 86  64 87  65\n## 87  52 37  71\n## 88   8 92  34\n## 89  40 46  10\n## 90  80  6  82\n## 91  47 59  51\n## 92  33 84   6\n## 93  33 29  77\n## 94  84 66  35\n## 95  52 92  30\n## 96  49 79  44\n## 97  17 50  28\n## 98  26 62  72\n## 99  56 24  82\n## 100 96 26  31\ndfcomp<-function(r1,r2) {\n  r1[\"b\"][1]<r2[\"b\"][1]\n}\n\n\ndf<-dfsort(df,dfcomp)\ndf##      a  b   c\n## 1   49  2  36\n## 2    3  5   3\n## 3   80  6  82\n## 4   32  6  70\n## 5   97  7  71\n## 6   18  8  32\n## 7    7  9  50\n## 8   53 10  15\n## 9   57 13  26\n## 10  59 15  66\n## 11  21 15  63\n## 12  92 17  14\n## 13  20 17 100\n## 14  85 19  44\n## 15  99 20  79\n## 16  15 20  23\n## 17  56 24  82\n## 18  91 24  74\n## 19  14 25  72\n## 20  96 26  31\n## 21  20 26  17\n## 22  53 27  47\n## 23  68 27  73\n## 24  48 28  57\n## 25   7 28  64\n## 26  33 29  77\n## 27  53 32   3\n## 28  53 33  48\n## 29  95 34  94\n## 30  11 36  89\n## 31  52 37  71\n## 32  41 37  42\n## 33   5 39   8\n## 34  82 41  39\n## 35  27 42  66\n## 36  79 43  99\n## 37  84 45  90\n## 38  40 46  10\n## 39  82 46  69\n## 40   1 47  46\n## 41  76 48  82\n## 42  17 50  28\n## 43  50 52  80\n## 44  69 52  60\n## 45  46 53  67\n## 46  81 56  81\n## 47  88 56  39\n## 48  91 57  35\n## 49  47 59  51\n## 50   4 59  56\n## 51  91 61  17\n## 52  26 62  72\n## 53  54 62  55\n## 54  12 63  71\n## 55  28 63  92\n## 56  55 63  90\n## 57  44 63  25\n## 58  63 64  29\n## 59  45 65  58\n## 60  84 66  35\n## 61  13 66  98\n## 62  52 67  80\n## 63  90 68  49\n## 64  31 68  25\n## 65  80 69  11\n## 66   9 69  46\n## 67  49 69  18\n## 68  10 69  40\n## 69  55 70  83\n## 70  35 75  27\n## 71  16 75  69\n## 72  25 76   6\n## 73  97 77  50\n## 74  19 78  86\n## 75  86 78  73\n## 76  49 79  44\n## 77  98 81  53\n## 78  10 82  17\n## 79  94 82  18\n## 80  58 83  43\n## 81  94 83  78\n## 82  33 84   6\n## 83  37 84   4\n## 84  15 86  38\n## 85  56 86  94\n## 86  64 87  65\n## 87  39 87  22\n## 88  62 88   7\n## 89  21 90  82\n## 90  30 90  32\n## 91   9 90  94\n## 92  31 91  14\n## 93  52 92  30\n## 94   8 92  34\n## 95   6 92   2\n## 96  13 93  92\n## 97  49 94  55\n## 98  66 97  52\n## 99  25 98  70\n## 100 81 99  90"},{"path":"data-science-product-cycles-in-enterprises.html","id":"data-science-product-cycles-in-enterprises","chapter":"47 Data Science Product Cycles in Enterprises","heading":"47 Data Science Product Cycles in Enterprises","text":"Leo Du Hao LiAs Data Science students, typically work carefully prepared data problem sets professors designed lots effort guide students achieve learning objectives can mostly clearly measured within reasonable timeline. However, reality, almost never seamless achieve results. delivered Zoom talk Nov. 1st share unique perspectives different stages typical development life cycle Data Science powered products large enterprises, different teams involved, technologies often used, visualization tools techniques leveraged different stages.Due policies, share materials publicly. slide deck, well recording session available Courseworks submission, students taking EDAV Fall 2021 upon request.","code":""},{"path":"github-initial-setup.html","id":"github-initial-setup","chapter":"48 Github initial setup","heading":"48 Github initial setup","text":"Joyce Robbins","code":""},{"path":"github-initial-setup.html","id":"create-new-repo","chapter":"48 Github initial setup","heading":"48.1 Create new repo","text":"Create new repository copying template: http://www.github.com/jtr13/cctemplate following instructions README.","code":""},{"path":"github-initial-setup.html","id":"pages-in-repo-settings","chapter":"48 Github initial setup","heading":"48.2 Pages in repo settings","text":"Change source gh-pagesMay trigger GHA get work","code":""},{"path":"github-initial-setup.html","id":"add-packages-to-description-file","chapter":"48 Github initial setup","heading":"48.3 Add packages to DESCRIPTION file","text":"Need better process…Downloaded submissions CourseWorksCreate DESCRIPTION file. Add add dependencies projthis::proj_update_deps()https://twitter.com/ijlyttle/status/1370776366585614342Add Imports real DESCRIPTION file.Found problematic packages looking reverse dependencies packages failed install:devtools::revdep()Also used pak::pkg_deps_tree()Problems:magickrJava dependency qdap","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"tutorial-for-pull-request-mergers","chapter":"49 Tutorial for pull request mergers","heading":"49 Tutorial for pull request mergers","text":"","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"general","chapter":"49 Tutorial for pull request mergers","heading":"49.1 General","text":"following checklist steps perform merging pull request. point, ’re sure , request review one PR leaders.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-branch","chapter":"49 Tutorial for pull request mergers","heading":"49.2 Check branch","text":"PR submitted non-main branch.PR submitted main branch, provide instructions fix problem:Close PR.Close PR.Follow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesFollow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesIf trouble 2., delete local folder project, delete fork GitHub, start .trouble 2., delete local folder project, delete fork GitHub, start .Open new PR.Open new PR.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"examine-files-that-were-added-or-modified","chapter":"49 Tutorial for pull request mergers","heading":"49.3 Examine files that were added or modified","text":"ONE .Rmd file.ONE .Rmd file.additional resources resources/<project_name>/ folder.additional resources resources/<project_name>/ folder.files root directory besides .Rmd file.files root directory besides .Rmd file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-filename","chapter":"49 Tutorial for pull request mergers","heading":"49.4 Check .Rmd filename","text":".Rmd filename words joined underscores, white space. (Update: need branch name.).Rmd filename can contain lowercase letters. (Otherwise filenames sort nicely repo home page.)","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-file-contents","chapter":"49 Tutorial for pull request mergers","heading":"49.5 Check .Rmd file contents","text":"file contain YAML header --- line.first line start single hashtag #, followed single whitespace, title.second line blank, followed author name(s).additional single hashtag headers chapter. (, new chapters created.)hashtag headers followed numbers since hashtags create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading.file contains setup chunk .Rmd file, contain setup label. (bookdown render fail duplicate chunk labels.)\n.e. use {r, include=FALSE} instead {r setup, include=FALSE}.\nSee sample .RmdLinks internal files must contain resources/<project_name>/ path, : ![Test Photo](resources/sample_project/election.jpg)file contain install.packages(), write functions, setwd(), getwd().’s anything else looks odd ’re sure, assign jtr13 review explain issue.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"request-changes","chapter":"49 Tutorial for pull request mergers","heading":"49.6 Request changes","text":"problems checks listed , explain pull request merged request changes following steps:, add changes requested label pull request.job pull request done now. contributors fix requests, review either move forward merge explain changes still need made.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-the-pull-request","chapter":"49 Tutorial for pull request mergers","heading":"49.7 Merge the pull request","text":"good go, ’s time merge pull request. several steps.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"steps-to-merge-the-pr","chapter":"49 Tutorial for pull request mergers","heading":"49.7.1 Steps to Merge the PR","text":"Go main branch project (jtr13/cc21fall1) open _bookdown.yml fileGo main branch project (jtr13/cc21fall1) open _bookdown.yml fileCopy entire rmd_files section. look something like \nrmd_files: [ 'index.Rmd', # must first chapter 'assignment.Rmd', ...., ...., ]Copy entire rmd_files section. look something like \nrmd_files: [ 'index.Rmd', # must first chapter 'assignment.Rmd', ...., ...., ]Open branch submitted PR following steps:\naccess PR branch:\n\nMake sure PR branch checking PR branch name shown (main):\nOpen branch submitted PR following steps:access PR branch:Make sure PR branch checking PR branch name shown (main):Remove rmd_files: [] section paste one copied main branch project.Remove rmd_files: [] section paste one copied main branch project.Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Save edited version.Save edited version.Come back PR.Come back PR.Merge PR.Merge PR.Click Actions tabs check whether build successful (successful build green dot front actions). PLEASE NOTE actions take complete (approximately 5-6 mins depending number files rendered), might need wait time finally check whether build successful .Click Actions tabs check whether build successful (successful build green dot front actions). PLEASE NOTE actions take complete (approximately 5-6 mins depending number files rendered), might need wait time finally check whether build successful .case build fail able understand rectify please tag one PR Assigners can review . PLEASE revert merge create new branches workflow.case build fail able understand rectify please tag one PR Assigners can review . PLEASE revert merge create new branches workflow.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"pr-leaders-only-add-part-names-to-.rmd-for-every-first-article-in-part","chapter":"49 Tutorial for pull request mergers","heading":"49.7.2 PR Leaders only: Add part names to .Rmd for every first article in part","text":"adding first chapter PART.every first article part, add chapter name top .Rmd file, propose changes. example like .\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-pr-and-leave-a-comment","chapter":"49 Tutorial for pull request mergers","heading":"49.7.3 Merge PR and leave a comment","text":"Now comes final step.develop experience confident things correctly, assign another PR merger review work:@aye21874 (Ayush), @clarissarjtai (Clarissa), @ejosied (Josie), @hwelinkim (Hyo Won), @ivanye2509 (Xin)\n@kfijan (Katharina), @ktsht (Alex), @mtz2110 (Maxwell), @s10singh97 (Shashwat), @ShiyuWang88 (Shiyu), @Shruti-Kaushal (Shruti), @verlocks (Zheyu)(Please fix names mistakes aren’t names go … don’t need submit pull request, just edit file commit main branch.)Go back conversation tab pull requests page, example:https://github.com/jtr13/cc20/pull/23#issuecomment-728506101Leave comments congratulations 🎉 (type :tada:) click green button merge.\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-updated-version","chapter":"49 Tutorial for pull request mergers","heading":"49.7.4 Check updated version","text":"successful merge means addition file files added project merge conflicts. mean book render deploy GitHub pages without issues. merge, take 5-10 minutes GitHub Actions render book deploy updated version. ’s problem notified email address . words, job done. However ’re interested, can check progress clicking Actions top repo.","code":""}]
